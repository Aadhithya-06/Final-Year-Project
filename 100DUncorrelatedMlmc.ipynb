{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QP33ZZJQka3l"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(\"Pytorch/\"))\n",
        "sys.path.append(os.path.abspath(\"models/\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vhh1gnwvi1ZB"
      },
      "outputs": [],
      "source": [
        "from FBSNNs import *\n",
        "from BlackScholesBarenblatt100D import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "M = 100  # number of trajectories (batch size)\n",
        "N = 50  # number of time snapshots\n",
        "D = 100 # number of dimensions\n",
        "Mm = N ** (1/5)\n",
        "\n",
        "layers = [D + 1] + 4 * [256] + [1]\n",
        "\n",
        "Xi = np.array([100.0, 50.0] * int(D / 2))[None, :]\n",
        "T = 1.0\n",
        "\n",
        "\"Available architectures\"\n",
        "mode = \"FC\"  # FC, Resnet and NAIS-Net are available\n",
        "activation = \"ReLU\"  # Sine and ReLU are available\n",
        "model = BlackScholesBarenblatt(Xi, T, M, N, D, Mm, layers, mode, activation)\n",
        "\n",
        "n_iter = 2 * 10**4\n",
        "lr = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Gradients at iteration 0:\n",
            " - layer1.weight grad norm: 0.5133111476898193\n",
            " - layer1.bias grad norm: 0.00059798255097121\n",
            " - layer2.weight grad norm: 0.0019682368729263544\n",
            " - layer2.bias grad norm: 0.0005891931941732764\n",
            " - layer2_input.weight grad norm: 0.5077073574066162\n",
            " - layer2_input.bias grad norm: 0.0005891931941732764\n",
            " - layer3.weight grad norm: 0.002669684123247862\n",
            " - layer3.bias grad norm: 0.0005741642671637237\n",
            " - layer3_input.weight grad norm: 0.494527667760849\n",
            " - layer3_input.bias grad norm: 0.0005741642671637237\n",
            " - layer4.weight grad norm: 0.0032641850411891937\n",
            " - layer4.bias grad norm: 0.0005633259424939752\n",
            " - layer4_input.weight grad norm: 0.4837028980255127\n",
            " - layer4_input.bias grad norm: 0.0005633259424939752\n",
            " - layer5.weight grad norm: 0.012796120718121529\n",
            " - layer5.bias grad norm: 0.005568892229348421\n",
            "It: 0, Loss: 5.280e+13, Y0: 1.412, Time: 0.42, Learning Rate: 1.000e-03\n",
            "Gradients at iteration 1:\n",
            " - layer1.weight grad norm: 0.527459442615509\n",
            " - layer1.bias grad norm: 0.000610418850556016\n",
            " - layer2.weight grad norm: 0.0019394642440602183\n",
            " - layer2.bias grad norm: 0.0006044756155461073\n",
            " - layer2_input.weight grad norm: 0.5246224999427795\n",
            " - layer2_input.bias grad norm: 0.0006044756155461073\n",
            " - layer3.weight grad norm: 0.0026936735957860947\n",
            " - layer3.bias grad norm: 0.0005755485035479069\n",
            " - layer3_input.weight grad norm: 0.4990808963775635\n",
            " - layer3_input.bias grad norm: 0.0005755485035479069\n",
            " - layer4.weight grad norm: 0.003204044885933399\n",
            " - layer4.bias grad norm: 0.0005029738531447947\n",
            " - layer4_input.weight grad norm: 0.44413480162620544\n",
            " - layer4_input.bias grad norm: 0.0005029738531447947\n",
            " - layer5.weight grad norm: 0.012915990315377712\n",
            " - layer5.bias grad norm: 0.00548413535580039\n",
            "Gradients at iteration 2:\n",
            " - layer1.weight grad norm: 0.4918047785758972\n",
            " - layer1.bias grad norm: 0.0005635342095047235\n",
            " - layer2.weight grad norm: 0.0020182004664093256\n",
            " - layer2.bias grad norm: 0.000633122050203383\n",
            " - layer2_input.weight grad norm: 0.5428344011306763\n",
            " - layer2_input.bias grad norm: 0.000633122050203383\n",
            " - layer3.weight grad norm: 0.002789855934679508\n",
            " - layer3.bias grad norm: 0.0005630285013467073\n",
            " - layer3_input.weight grad norm: 0.4882863461971283\n",
            " - layer3_input.bias grad norm: 0.0005630285013467073\n",
            " - layer4.weight grad norm: 0.0033471875358372927\n",
            " - layer4.bias grad norm: 0.0005481074331328273\n",
            " - layer4_input.weight grad norm: 0.474134236574173\n",
            " - layer4_input.bias grad norm: 0.0005481074331328273\n",
            " - layer5.weight grad norm: 0.013209346681833267\n",
            " - layer5.bias grad norm: 0.00567846791818738\n",
            "Gradients at iteration 3:\n",
            " - layer1.weight grad norm: 0.5001265406608582\n",
            " - layer1.bias grad norm: 0.0005747514078393579\n",
            " - layer2.weight grad norm: 0.0019500810885801911\n",
            " - layer2.bias grad norm: 0.0005629247170872986\n",
            " - layer2_input.weight grad norm: 0.4900076389312744\n",
            " - layer2_input.bias grad norm: 0.0005629247170872986\n",
            " - layer3.weight grad norm: 0.0026847089175134897\n",
            " - layer3.bias grad norm: 0.0005986117757856846\n",
            " - layer3_input.weight grad norm: 0.51320481300354\n",
            " - layer3_input.bias grad norm: 0.0005986117757856846\n",
            " - layer4.weight grad norm: 0.0032101734541356564\n",
            " - layer4.bias grad norm: 0.0005767468828707933\n",
            " - layer4_input.weight grad norm: 0.4961678385734558\n",
            " - layer4_input.bias grad norm: 0.0005767468828707933\n",
            " - layer5.weight grad norm: 0.012290370650589466\n",
            " - layer5.bias grad norm: 0.00544307567179203\n",
            "Gradients at iteration 4:\n",
            " - layer1.weight grad norm: 0.5433334112167358\n",
            " - layer1.bias grad norm: 0.0006421113503165543\n",
            " - layer2.weight grad norm: 0.0019723742734640837\n",
            " - layer2.bias grad norm: 0.0005201107123866677\n",
            " - layer2_input.weight grad norm: 0.4550305902957916\n",
            " - layer2_input.bias grad norm: 0.0005201107123866677\n",
            " - layer3.weight grad norm: 0.0027214917354285717\n",
            " - layer3.bias grad norm: 0.0005710452678613365\n",
            " - layer3_input.weight grad norm: 0.4918346107006073\n",
            " - layer3_input.bias grad norm: 0.0005710452678613365\n",
            " - layer4.weight grad norm: 0.003239501267671585\n",
            " - layer4.bias grad norm: 0.0005926756421104074\n",
            " - layer4_input.weight grad norm: 0.5055945515632629\n",
            " - layer4_input.bias grad norm: 0.0005926756421104074\n",
            " - layer5.weight grad norm: 0.012422647327184677\n",
            " - layer5.bias grad norm: 0.0055065881460905075\n",
            "Gradients at iteration 5:\n",
            " - layer1.weight grad norm: 0.5018185973167419\n",
            " - layer1.bias grad norm: 0.0005744147929362953\n",
            " - layer2.weight grad norm: 0.002008881652727723\n",
            " - layer2.bias grad norm: 0.000607242516707629\n",
            " - layer2_input.weight grad norm: 0.5274310111999512\n",
            " - layer2_input.bias grad norm: 0.000607242516707629\n",
            " - layer3.weight grad norm: 0.0027716760523617268\n",
            " - layer3.bias grad norm: 0.0005934719229117036\n",
            " - layer3_input.weight grad norm: 0.5130733251571655\n",
            " - layer3_input.bias grad norm: 0.0005934719229117036\n",
            " - layer4.weight grad norm: 0.003323307726532221\n",
            " - layer4.bias grad norm: 0.0005165761103853583\n",
            " - layer4_input.weight grad norm: 0.45444947481155396\n",
            " - layer4_input.bias grad norm: 0.0005165761103853583\n",
            " - layer5.weight grad norm: 0.013005182147026062\n",
            " - layer5.bias grad norm: 0.005640307907015085\n",
            "Gradients at iteration 6:\n",
            " - layer1.weight grad norm: 0.5596315860748291\n",
            " - layer1.bias grad norm: 0.0006592553108930588\n",
            " - layer2.weight grad norm: 0.0020358068868517876\n",
            " - layer2.bias grad norm: 0.0005813545431010425\n",
            " - layer2_input.weight grad norm: 0.5014045834541321\n",
            " - layer2_input.bias grad norm: 0.0005813545431010425\n",
            " - layer3.weight grad norm: 0.0028244799468666315\n",
            " - layer3.bias grad norm: 0.0005491413758136332\n",
            " - layer3_input.weight grad norm: 0.47667768597602844\n",
            " - layer3_input.bias grad norm: 0.0005491413758136332\n",
            " - layer4.weight grad norm: 0.0032866003457456827\n",
            " - layer4.bias grad norm: 0.0005252218106761575\n",
            " - layer4_input.weight grad norm: 0.4560279846191406\n",
            " - layer4_input.bias grad norm: 0.0005252218106761575\n",
            " - layer5.weight grad norm: 0.0128600699827075\n",
            " - layer5.bias grad norm: 0.00567584577947855\n",
            "Gradients at iteration 7:\n",
            " - layer1.weight grad norm: 0.5256966948509216\n",
            " - layer1.bias grad norm: 0.0006087110959924757\n",
            " - layer2.weight grad norm: 0.0019650741014629602\n",
            " - layer2.bias grad norm: 0.0005920152761973441\n",
            " - layer2_input.weight grad norm: 0.5111011266708374\n",
            " - layer2_input.bias grad norm: 0.0005920152761973441\n",
            " - layer3.weight grad norm: 0.002682543359696865\n",
            " - layer3.bias grad norm: 0.0005509632173925638\n",
            " - layer3_input.weight grad norm: 0.4814736247062683\n",
            " - layer3_input.bias grad norm: 0.0005509632173925638\n",
            " - layer4.weight grad norm: 0.0032511549070477486\n",
            " - layer4.bias grad norm: 0.0005509791662916541\n",
            " - layer4_input.weight grad norm: 0.4799850881099701\n",
            " - layer4_input.bias grad norm: 0.0005509791662916541\n",
            " - layer5.weight grad norm: 0.012718531303107738\n",
            " - layer5.bias grad norm: 0.005503227934241295\n",
            "Gradients at iteration 8:\n",
            " - layer1.weight grad norm: 0.5504374504089355\n",
            " - layer1.bias grad norm: 0.0006507575744763017\n",
            " - layer2.weight grad norm: 0.0019226938020437956\n",
            " - layer2.bias grad norm: 0.0005466020666062832\n",
            " - layer2_input.weight grad norm: 0.4729566276073456\n",
            " - layer2_input.bias grad norm: 0.0005466020666062832\n",
            " - layer3.weight grad norm: 0.002671200316399336\n",
            " - layer3.bias grad norm: 0.000575865269638598\n",
            " - layer3_input.weight grad norm: 0.4950626492500305\n",
            " - layer3_input.bias grad norm: 0.000575865269638598\n",
            " - layer4.weight grad norm: 0.0032400989439338446\n",
            " - layer4.bias grad norm: 0.0005599404103122652\n",
            " - layer4_input.weight grad norm: 0.47753265500068665\n",
            " - layer4_input.bias grad norm: 0.0005599404103122652\n",
            " - layer5.weight grad norm: 0.012380119413137436\n",
            " - layer5.bias grad norm: 0.005422106012701988\n",
            "Gradients at iteration 9:\n",
            " - layer1.weight grad norm: 0.5494371652603149\n",
            " - layer1.bias grad norm: 0.0006395819364115596\n",
            " - layer2.weight grad norm: 0.001945864176377654\n",
            " - layer2.bias grad norm: 0.0005808621644973755\n",
            " - layer2_input.weight grad norm: 0.5044786930084229\n",
            " - layer2_input.bias grad norm: 0.0005808621644973755\n",
            " - layer3.weight grad norm: 0.002721920143812895\n",
            " - layer3.bias grad norm: 0.0005392823950387537\n",
            " - layer3_input.weight grad norm: 0.4744933545589447\n",
            " - layer3_input.bias grad norm: 0.0005392823950387537\n",
            " - layer4.weight grad norm: 0.0032322732731699944\n",
            " - layer4.bias grad norm: 0.0005368716665543616\n",
            " - layer4_input.weight grad norm: 0.46719345450401306\n",
            " - layer4_input.bias grad norm: 0.0005368716665543616\n",
            " - layer5.weight grad norm: 0.012345601804554462\n",
            " - layer5.bias grad norm: 0.005479978397488594\n",
            "Gradients at iteration 10:\n",
            " - layer1.weight grad norm: 0.5383142232894897\n",
            " - layer1.bias grad norm: 0.0006342335254885256\n",
            " - layer2.weight grad norm: 0.0019028993556275964\n",
            " - layer2.bias grad norm: 0.0005485499859787524\n",
            " - layer2_input.weight grad norm: 0.47436216473579407\n",
            " - layer2_input.bias grad norm: 0.0005485499859787524\n",
            " - layer3.weight grad norm: 0.0026413751766085625\n",
            " - layer3.bias grad norm: 0.000583222194109112\n",
            " - layer3_input.weight grad norm: 0.4957367777824402\n",
            " - layer3_input.bias grad norm: 0.000583222194109112\n",
            " - layer4.weight grad norm: 0.0031745308078825474\n",
            " - layer4.bias grad norm: 0.0005769234267063439\n",
            " - layer4_input.weight grad norm: 0.4891335964202881\n",
            " - layer4_input.bias grad norm: 0.0005769234267063439\n",
            " - layer5.weight grad norm: 0.01183300744742155\n",
            " - layer5.bias grad norm: 0.005349674262106419\n",
            "Gradients at iteration 11:\n",
            " - layer1.weight grad norm: 0.5080003142356873\n",
            " - layer1.bias grad norm: 0.0005874696653336287\n",
            " - layer2.weight grad norm: 0.0019824833143502474\n",
            " - layer2.bias grad norm: 0.0005301720811985433\n",
            " - layer2_input.weight grad norm: 0.4629947543144226\n",
            " - layer2_input.bias grad norm: 0.0005301720811985433\n",
            " - layer3.weight grad norm: 0.0027870144695043564\n",
            " - layer3.bias grad norm: 0.0006232604500837624\n",
            " - layer3_input.weight grad norm: 0.5298413038253784\n",
            " - layer3_input.bias grad norm: 0.0006232604500837624\n",
            " - layer4.weight grad norm: 0.0033198916353285313\n",
            " - layer4.bias grad norm: 0.0005769843701273203\n",
            " - layer4_input.weight grad norm: 0.49661552906036377\n",
            " - layer4_input.bias grad norm: 0.0005769843701273203\n",
            " - layer5.weight grad norm: 0.012492994777858257\n",
            " - layer5.bias grad norm: 0.005620464216917753\n",
            "Gradients at iteration 12:\n",
            " - layer1.weight grad norm: 0.5410656929016113\n",
            " - layer1.bias grad norm: 0.0006238984642550349\n",
            " - layer2.weight grad norm: 0.0020154290832579136\n",
            " - layer2.bias grad norm: 0.0006039232248440385\n",
            " - layer2_input.weight grad norm: 0.5222155451774597\n",
            " - layer2_input.bias grad norm: 0.0006039232248440385\n",
            " - layer3.weight grad norm: 0.0027641754131764174\n",
            " - layer3.bias grad norm: 0.0005474732024595141\n",
            " - layer3_input.weight grad norm: 0.4799565374851227\n",
            " - layer3_input.bias grad norm: 0.0005474732024595141\n",
            " - layer4.weight grad norm: 0.0033630153629928827\n",
            " - layer4.bias grad norm: 0.0005119018023833632\n",
            " - layer4_input.weight grad norm: 0.45163434743881226\n",
            " - layer4_input.bias grad norm: 0.0005119018023833632\n",
            " - layer5.weight grad norm: 0.012226001359522343\n",
            " - layer5.bias grad norm: 0.0056620570831000805\n",
            "Gradients at iteration 13:\n",
            " - layer1.weight grad norm: 0.5337114334106445\n",
            " - layer1.bias grad norm: 0.0006171520217321813\n",
            " - layer2.weight grad norm: 0.0018903054296970367\n",
            " - layer2.bias grad norm: 0.0005721831112168729\n",
            " - layer2_input.weight grad norm: 0.495915025472641\n",
            " - layer2_input.bias grad norm: 0.0005721831112168729\n",
            " - layer3.weight grad norm: 0.0026600579731166363\n",
            " - layer3.bias grad norm: 0.0005698695313185453\n",
            " - layer3_input.weight grad norm: 0.4930512607097626\n",
            " - layer3_input.bias grad norm: 0.0005698695313185453\n",
            " - layer4.weight grad norm: 0.0032130335457623005\n",
            " - layer4.bias grad norm: 0.0005465443828143179\n",
            " - layer4_input.weight grad norm: 0.4753032922744751\n",
            " - layer4_input.bias grad norm: 0.0005465443828143179\n",
            " - layer5.weight grad norm: 0.01245458610355854\n",
            " - layer5.bias grad norm: 0.0054155574180185795\n",
            "Gradients at iteration 14:\n",
            " - layer1.weight grad norm: 0.4981515109539032\n",
            " - layer1.bias grad norm: 0.0005717575550079346\n",
            " - layer2.weight grad norm: 0.0019665630534291267\n",
            " - layer2.bias grad norm: 0.0005526757449842989\n",
            " - layer2_input.weight grad norm: 0.4819071888923645\n",
            " - layer2_input.bias grad norm: 0.0005526757449842989\n",
            " - layer3.weight grad norm: 0.0027358841616660357\n",
            " - layer3.bias grad norm: 0.0005955082015134394\n",
            " - layer3_input.weight grad norm: 0.5117098689079285\n",
            " - layer3_input.bias grad norm: 0.0005955082015134394\n",
            " - layer4.weight grad norm: 0.0032326234504580498\n",
            " - layer4.bias grad norm: 0.0005950212944298983\n",
            " - layer4_input.weight grad norm: 0.507499635219574\n",
            " - layer4_input.bias grad norm: 0.0005950212944298983\n",
            " - layer5.weight grad norm: 0.012358342297375202\n",
            " - layer5.bias grad norm: 0.005531996488571167\n",
            "Gradients at iteration 15:\n",
            " - layer1.weight grad norm: 0.5187014937400818\n",
            " - layer1.bias grad norm: 0.0005982733564451337\n",
            " - layer2.weight grad norm: 0.0019068235997110605\n",
            " - layer2.bias grad norm: 0.0005726395756937563\n",
            " - layer2_input.weight grad norm: 0.4960196614265442\n",
            " - layer2_input.bias grad norm: 0.0005726395756937563\n",
            " - layer3.weight grad norm: 0.002645455999299884\n",
            " - layer3.bias grad norm: 0.0005799706559628248\n",
            " - layer3_input.weight grad norm: 0.5023081302642822\n",
            " - layer3_input.bias grad norm: 0.0005799706559628248\n",
            " - layer4.weight grad norm: 0.003167363815009594\n",
            " - layer4.bias grad norm: 0.0005569675122387707\n",
            " - layer4_input.weight grad norm: 0.4820738434791565\n",
            " - layer4_input.bias grad norm: 0.0005569675122387707\n",
            " - layer5.weight grad norm: 0.012361029163002968\n",
            " - layer5.bias grad norm: 0.005367039702832699\n",
            "Gradients at iteration 16:\n",
            " - layer1.weight grad norm: 0.5211309194564819\n",
            " - layer1.bias grad norm: 0.0006034186226315796\n",
            " - layer2.weight grad norm: 0.001970570767298341\n",
            " - layer2.bias grad norm: 0.0005721255438402295\n",
            " - layer2_input.weight grad norm: 0.4980780780315399\n",
            " - layer2_input.bias grad norm: 0.0005721255438402295\n",
            " - layer3.weight grad norm: 0.0026977329980582\n",
            " - layer3.bias grad norm: 0.0005441763205453753\n",
            " - layer3_input.weight grad norm: 0.47752436995506287\n",
            " - layer3_input.bias grad norm: 0.0005441763205453753\n",
            " - layer4.weight grad norm: 0.0032613868825137615\n",
            " - layer4.bias grad norm: 0.0005797742051072419\n",
            " - layer4_input.weight grad norm: 0.5020840764045715\n",
            " - layer4_input.bias grad norm: 0.0005797742051072419\n",
            " - layer5.weight grad norm: 0.012989410199224949\n",
            " - layer5.bias grad norm: 0.005474204197525978\n",
            "Gradients at iteration 17:\n",
            " - layer1.weight grad norm: 0.49643176794052124\n",
            " - layer1.bias grad norm: 0.000581191445235163\n",
            " - layer2.weight grad norm: 0.0019422558834776282\n",
            " - layer2.bias grad norm: 0.0005934199434705079\n",
            " - layer2_input.weight grad norm: 0.5104449391365051\n",
            " - layer2_input.bias grad norm: 0.0005934199434705079\n",
            " - layer3.weight grad norm: 0.0027235224843025208\n",
            " - layer3.bias grad norm: 0.0006016346160322428\n",
            " - layer3_input.weight grad norm: 0.5127364993095398\n",
            " - layer3_input.bias grad norm: 0.0006016346160322428\n",
            " - layer4.weight grad norm: 0.003238439792767167\n",
            " - layer4.bias grad norm: 0.0005571204819716513\n",
            " - layer4_input.weight grad norm: 0.47949540615081787\n",
            " - layer4_input.bias grad norm: 0.0005571204819716513\n",
            " - layer5.weight grad norm: 0.011535150930285454\n",
            " - layer5.bias grad norm: 0.00545157678425312\n",
            "Gradients at iteration 18:\n",
            " - layer1.weight grad norm: 0.5123959183692932\n",
            " - layer1.bias grad norm: 0.0005899651441723108\n",
            " - layer2.weight grad norm: 0.0018993538105860353\n",
            " - layer2.bias grad norm: 0.000566859554965049\n",
            " - layer2_input.weight grad norm: 0.4926638901233673\n",
            " - layer2_input.bias grad norm: 0.000566859554965049\n",
            " - layer3.weight grad norm: 0.002638102974742651\n",
            " - layer3.bias grad norm: 0.0006099783349782228\n",
            " - layer3_input.weight grad norm: 0.5229515433311462\n",
            " - layer3_input.bias grad norm: 0.0006099783349782228\n",
            " - layer4.weight grad norm: 0.003138892585411668\n",
            " - layer4.bias grad norm: 0.0005415974301286042\n",
            " - layer4_input.weight grad norm: 0.4701317250728607\n",
            " - layer4_input.bias grad norm: 0.0005415974301286042\n",
            " - layer5.weight grad norm: 0.013385722413659096\n",
            " - layer5.bias grad norm: 0.005346584599465132\n",
            "Gradients at iteration 19:\n",
            " - layer1.weight grad norm: 0.5333002805709839\n",
            " - layer1.bias grad norm: 0.0006178736803121865\n",
            " - layer2.weight grad norm: 0.0018806670559570193\n",
            " - layer2.bias grad norm: 0.0005772982840426266\n",
            " - layer2_input.weight grad norm: 0.49473801255226135\n",
            " - layer2_input.bias grad norm: 0.0005772982840426266\n",
            " - layer3.weight grad norm: 0.002643145387992263\n",
            " - layer3.bias grad norm: 0.0005679294117726386\n",
            " - layer3_input.weight grad norm: 0.48830917477607727\n",
            " - layer3_input.bias grad norm: 0.0005679294117726386\n",
            " - layer4.weight grad norm: 0.003111955476924777\n",
            " - layer4.bias grad norm: 0.0005576227558776736\n",
            " - layer4_input.weight grad norm: 0.48184463381767273\n",
            " - layer4_input.bias grad norm: 0.0005576227558776736\n",
            " - layer5.weight grad norm: 0.012426053173840046\n",
            " - layer5.bias grad norm: 0.005299787037074566\n",
            "Gradients at iteration 20:\n",
            " - layer1.weight grad norm: 0.5205913782119751\n",
            " - layer1.bias grad norm: 0.0005995491519570351\n",
            " - layer2.weight grad norm: 0.0020149198826402426\n",
            " - layer2.bias grad norm: 0.0005766996182501316\n",
            " - layer2_input.weight grad norm: 0.5019292831420898\n",
            " - layer2_input.bias grad norm: 0.0005766996182501316\n",
            " - layer3.weight grad norm: 0.0027952706441283226\n",
            " - layer3.bias grad norm: 0.0005767703987658024\n",
            " - layer3_input.weight grad norm: 0.4947158098220825\n",
            " - layer3_input.bias grad norm: 0.0005767703987658024\n",
            " - layer4.weight grad norm: 0.003339278744533658\n",
            " - layer4.bias grad norm: 0.0005528008914552629\n",
            " - layer4_input.weight grad norm: 0.4817551076412201\n",
            " - layer4_input.bias grad norm: 0.0005528008914552629\n",
            " - layer5.weight grad norm: 0.012757370248436928\n",
            " - layer5.bias grad norm: 0.00565310986712575\n",
            "Gradients at iteration 21:\n",
            " - layer1.weight grad norm: 0.5072150826454163\n",
            " - layer1.bias grad norm: 0.0005780540523119271\n",
            " - layer2.weight grad norm: 0.0019914829172194004\n",
            " - layer2.bias grad norm: 0.0006013504462316632\n",
            " - layer2_input.weight grad norm: 0.5226678848266602\n",
            " - layer2_input.bias grad norm: 0.0006013504462316632\n",
            " - layer3.weight grad norm: 0.0027536156121641397\n",
            " - layer3.bias grad norm: 0.0005770759307779372\n",
            " - layer3_input.weight grad norm: 0.5016964077949524\n",
            " - layer3_input.bias grad norm: 0.0005770759307779372\n",
            " - layer4.weight grad norm: 0.0032862622756510973\n",
            " - layer4.bias grad norm: 0.0005329805426299572\n",
            " - layer4_input.weight grad norm: 0.4665305018424988\n",
            " - layer4_input.bias grad norm: 0.0005329805426299572\n",
            " - layer5.weight grad norm: 0.012048961594700813\n",
            " - layer5.bias grad norm: 0.005599125754088163\n",
            "Gradients at iteration 22:\n",
            " - layer1.weight grad norm: 0.4933980107307434\n",
            " - layer1.bias grad norm: 0.0005593452951870859\n",
            " - layer2.weight grad norm: 0.001980566419661045\n",
            " - layer2.bias grad norm: 0.0005890554748475552\n",
            " - layer2_input.weight grad norm: 0.5135079622268677\n",
            " - layer2_input.bias grad norm: 0.0005890554748475552\n",
            " - layer3.weight grad norm: 0.002723845886066556\n",
            " - layer3.bias grad norm: 0.0005826079868711531\n",
            " - layer3_input.weight grad norm: 0.5070416331291199\n",
            " - layer3_input.bias grad norm: 0.0005826079868711531\n",
            " - layer4.weight grad norm: 0.003298636293038726\n",
            " - layer4.bias grad norm: 0.0005531134083867073\n",
            " - layer4_input.weight grad norm: 0.4853785037994385\n",
            " - layer4_input.bias grad norm: 0.0005531134083867073\n",
            " - layer5.weight grad norm: 0.011379428207874298\n",
            " - layer5.bias grad norm: 0.005520371720194817\n",
            "Gradients at iteration 23:\n",
            " - layer1.weight grad norm: 0.5214276909828186\n",
            " - layer1.bias grad norm: 0.0006060497835278511\n",
            " - layer2.weight grad norm: 0.0019211821490898728\n",
            " - layer2.bias grad norm: 0.0005367941339500248\n",
            " - layer2_input.weight grad norm: 0.4670945107936859\n",
            " - layer2_input.bias grad norm: 0.0005367941339500248\n",
            " - layer3.weight grad norm: 0.0026088375598192215\n",
            " - layer3.bias grad norm: 0.000595604709815234\n",
            " - layer3_input.weight grad norm: 0.5101555585861206\n",
            " - layer3_input.bias grad norm: 0.000595604709815234\n",
            " - layer4.weight grad norm: 0.003193107433617115\n",
            " - layer4.bias grad norm: 0.000588199298363179\n",
            " - layer4_input.weight grad norm: 0.4994783103466034\n",
            " - layer4_input.bias grad norm: 0.000588199298363179\n",
            " - layer5.weight grad norm: 0.012090567499399185\n",
            " - layer5.bias grad norm: 0.005413000006228685\n",
            "Gradients at iteration 24:\n",
            " - layer1.weight grad norm: 0.5279626250267029\n",
            " - layer1.bias grad norm: 0.0006164885126054287\n",
            " - layer2.weight grad norm: 0.0019221280235797167\n",
            " - layer2.bias grad norm: 0.000592460623010993\n",
            " - layer2_input.weight grad norm: 0.5099567770957947\n",
            " - layer2_input.bias grad norm: 0.000592460623010993\n",
            " - layer3.weight grad norm: 0.002691988367587328\n",
            " - layer3.bias grad norm: 0.000581600412260741\n",
            " - layer3_input.weight grad norm: 0.4968188405036926\n",
            " - layer3_input.bias grad norm: 0.000581600412260741\n",
            " - layer4.weight grad norm: 0.003143154550343752\n",
            " - layer4.bias grad norm: 0.0005343967350199819\n",
            " - layer4_input.weight grad norm: 0.4627635180950165\n",
            " - layer4_input.bias grad norm: 0.0005343967350199819\n",
            " - layer5.weight grad norm: 0.012988877482712269\n",
            " - layer5.bias grad norm: 0.005349465645849705\n",
            "Gradients at iteration 25:\n",
            " - layer1.weight grad norm: 0.5181629061698914\n",
            " - layer1.bias grad norm: 0.0005941292038187385\n",
            " - layer2.weight grad norm: 0.002023728098720312\n",
            " - layer2.bias grad norm: 0.000595362507738173\n",
            " - layer2_input.weight grad norm: 0.5147044658660889\n",
            " - layer2_input.bias grad norm: 0.000595362507738173\n",
            " - layer3.weight grad norm: 0.0027661307249218225\n",
            " - layer3.bias grad norm: 0.0005915569490753114\n",
            " - layer3_input.weight grad norm: 0.5080305337905884\n",
            " - layer3_input.bias grad norm: 0.0005915569490753114\n",
            " - layer4.weight grad norm: 0.0032913703471422195\n",
            " - layer4.bias grad norm: 0.0005213819677010179\n",
            " - layer4_input.weight grad norm: 0.45637768507003784\n",
            " - layer4_input.bias grad norm: 0.0005213819677010179\n",
            " - layer5.weight grad norm: 0.012441003695130348\n",
            " - layer5.bias grad norm: 0.005590966437011957\n",
            "Gradients at iteration 26:\n",
            " - layer1.weight grad norm: 0.49019187688827515\n",
            " - layer1.bias grad norm: 0.000565173162613064\n",
            " - layer2.weight grad norm: 0.001995340222492814\n",
            " - layer2.bias grad norm: 0.0006022205343469977\n",
            " - layer2_input.weight grad norm: 0.5152900815010071\n",
            " - layer2_input.bias grad norm: 0.0006022205343469977\n",
            " - layer3.weight grad norm: 0.0027793622575700283\n",
            " - layer3.bias grad norm: 0.000578560633584857\n",
            " - layer3_input.weight grad norm: 0.4973977506160736\n",
            " - layer3_input.bias grad norm: 0.000578560633584857\n",
            " - layer4.weight grad norm: 0.0033718750346451998\n",
            " - layer4.bias grad norm: 0.0005792572046630085\n",
            " - layer4_input.weight grad norm: 0.49652495980262756\n",
            " - layer4_input.bias grad norm: 0.0005792572046630085\n",
            " - layer5.weight grad norm: 0.013777107931673527\n",
            " - layer5.bias grad norm: 0.005585427395999432\n",
            "Gradients at iteration 27:\n",
            " - layer1.weight grad norm: 0.5219532251358032\n",
            " - layer1.bias grad norm: 0.0006116021540947258\n",
            " - layer2.weight grad norm: 0.001916484092362225\n",
            " - layer2.bias grad norm: 0.0005497488309629261\n",
            " - layer2_input.weight grad norm: 0.4748288094997406\n",
            " - layer2_input.bias grad norm: 0.0005497488309629261\n",
            " - layer3.weight grad norm: 0.002605899702757597\n",
            " - layer3.bias grad norm: 0.0005826653796248138\n",
            " - layer3_input.weight grad norm: 0.5011433959007263\n",
            " - layer3_input.bias grad norm: 0.0005826653796248138\n",
            " - layer4.weight grad norm: 0.00308964098803699\n",
            " - layer4.bias grad norm: 0.000586582173127681\n",
            " - layer4_input.weight grad norm: 0.5007713437080383\n",
            " - layer4_input.bias grad norm: 0.000586582173127681\n",
            " - layer5.weight grad norm: 0.011639900505542755\n",
            " - layer5.bias grad norm: 0.005289956461638212\n",
            "Gradients at iteration 28:\n",
            " - layer1.weight grad norm: 0.5108284950256348\n",
            " - layer1.bias grad norm: 0.0005964556476101279\n",
            " - layer2.weight grad norm: 0.0019181971438229084\n",
            " - layer2.bias grad norm: 0.0006161390920169652\n",
            " - layer2_input.weight grad norm: 0.5230360627174377\n",
            " - layer2_input.bias grad norm: 0.0006161390920169652\n",
            " - layer3.weight grad norm: 0.0026623450685292482\n",
            " - layer3.bias grad norm: 0.0005745996022596955\n",
            " - layer3_input.weight grad norm: 0.4934040904045105\n",
            " - layer3_input.bias grad norm: 0.0005745996022596955\n",
            " - layer4.weight grad norm: 0.003176308237016201\n",
            " - layer4.bias grad norm: 0.0005470875184983015\n",
            " - layer4_input.weight grad norm: 0.4710083305835724\n",
            " - layer4_input.bias grad norm: 0.0005470875184983015\n",
            " - layer5.weight grad norm: 0.011797898449003696\n",
            " - layer5.bias grad norm: 0.005359499715268612\n",
            "Gradients at iteration 29:\n",
            " - layer1.weight grad norm: 0.5084525346755981\n",
            " - layer1.bias grad norm: 0.0005869739106856287\n",
            " - layer2.weight grad norm: 0.0018839029362425208\n",
            " - layer2.bias grad norm: 0.0005949502228759229\n",
            " - layer2_input.weight grad norm: 0.5109316110610962\n",
            " - layer2_input.bias grad norm: 0.0005949502228759229\n",
            " - layer3.weight grad norm: 0.002631454961374402\n",
            " - layer3.bias grad norm: 0.0005882377154193819\n",
            " - layer3_input.weight grad norm: 0.5057303309440613\n",
            " - layer3_input.bias grad norm: 0.0005882377154193819\n",
            " - layer4.weight grad norm: 0.0031075289007276297\n",
            " - layer4.bias grad norm: 0.0005468616145662963\n",
            " - layer4_input.weight grad norm: 0.4737868010997772\n",
            " - layer4_input.bias grad norm: 0.0005468616145662963\n",
            " - layer5.weight grad norm: 0.011734230443835258\n",
            " - layer5.bias grad norm: 0.005266256630420685\n",
            "Gradients at iteration 30:\n",
            " - layer1.weight grad norm: 0.512101948261261\n",
            " - layer1.bias grad norm: 0.0005946531891822815\n",
            " - layer2.weight grad norm: 0.0019498165929690003\n",
            " - layer2.bias grad norm: 0.0005270647816359997\n",
            " - layer2_input.weight grad norm: 0.458126425743103\n",
            " - layer2_input.bias grad norm: 0.0005270647816359997\n",
            " - layer3.weight grad norm: 0.0027046590112149715\n",
            " - layer3.bias grad norm: 0.0006493331748060882\n",
            " - layer3_input.weight grad norm: 0.5534985661506653\n",
            " - layer3_input.bias grad norm: 0.0006493331748060882\n",
            " - layer4.weight grad norm: 0.0031906634103506804\n",
            " - layer4.bias grad norm: 0.000545739138033241\n",
            " - layer4_input.weight grad norm: 0.47042903304100037\n",
            " - layer4_input.bias grad norm: 0.000545739138033241\n",
            " - layer5.weight grad norm: 0.012433569878339767\n",
            " - layer5.bias grad norm: 0.0054217493161559105\n",
            "Gradients at iteration 31:\n",
            " - layer1.weight grad norm: 0.49997419118881226\n",
            " - layer1.bias grad norm: 0.0005708140088245273\n",
            " - layer2.weight grad norm: 0.00200846279039979\n",
            " - layer2.bias grad norm: 0.0005987201002426445\n",
            " - layer2_input.weight grad norm: 0.5169040560722351\n",
            " - layer2_input.bias grad norm: 0.0005987201002426445\n",
            " - layer3.weight grad norm: 0.0027247124817222357\n",
            " - layer3.bias grad norm: 0.0005527096218429506\n",
            " - layer3_input.weight grad norm: 0.4840810000896454\n",
            " - layer3_input.bias grad norm: 0.0005527096218429506\n",
            " - layer4.weight grad norm: 0.0032651410438120365\n",
            " - layer4.bias grad norm: 0.0005752922734245658\n",
            " - layer4_input.weight grad norm: 0.4982761740684509\n",
            " - layer4_input.bias grad norm: 0.0005752922734245658\n",
            " - layer5.weight grad norm: 0.012939225882291794\n",
            " - layer5.bias grad norm: 0.005533562041819096\n",
            "Gradients at iteration 32:\n",
            " - layer1.weight grad norm: 0.5314497947692871\n",
            " - layer1.bias grad norm: 0.000616163422819227\n",
            " - layer2.weight grad norm: 0.0019280706765130162\n",
            " - layer2.bias grad norm: 0.0005697269807569683\n",
            " - layer2_input.weight grad norm: 0.4948256015777588\n",
            " - layer2_input.bias grad norm: 0.0005697269807569683\n",
            " - layer3.weight grad norm: 0.0026512572076171637\n",
            " - layer3.bias grad norm: 0.00055988336680457\n",
            " - layer3_input.weight grad norm: 0.4830910265445709\n",
            " - layer3_input.bias grad norm: 0.00055988336680457\n",
            " - layer4.weight grad norm: 0.0031763354782015085\n",
            " - layer4.bias grad norm: 0.0005723918438889086\n",
            " - layer4_input.weight grad norm: 0.48899850249290466\n",
            " - layer4_input.bias grad norm: 0.0005723918438889086\n",
            " - layer5.weight grad norm: 0.012638691812753677\n",
            " - layer5.bias grad norm: 0.005412560887634754\n",
            "Gradients at iteration 33:\n",
            " - layer1.weight grad norm: 0.5113600492477417\n",
            " - layer1.bias grad norm: 0.0005824131658300757\n",
            " - layer2.weight grad norm: 0.0020219553261995316\n",
            " - layer2.bias grad norm: 0.0006106275250203907\n",
            " - layer2_input.weight grad norm: 0.5277051329612732\n",
            " - layer2_input.bias grad norm: 0.0006106275250203907\n",
            " - layer3.weight grad norm: 0.002779243979603052\n",
            " - layer3.bias grad norm: 0.0005236192955635488\n",
            " - layer3_input.weight grad norm: 0.4631478190422058\n",
            " - layer3_input.bias grad norm: 0.0005236192955635488\n",
            " - layer4.weight grad norm: 0.0033119835425168276\n",
            " - layer4.bias grad norm: 0.0005696290172636509\n",
            " - layer4_input.weight grad norm: 0.49530065059661865\n",
            " - layer4_input.bias grad norm: 0.0005696290172636509\n",
            " - layer5.weight grad norm: 0.012377817183732986\n",
            " - layer5.bias grad norm: 0.005601143930107355\n",
            "Gradients at iteration 34:\n",
            " - layer1.weight grad norm: 0.5214636921882629\n",
            " - layer1.bias grad norm: 0.0006105338106863201\n",
            " - layer2.weight grad norm: 0.0019056207966059446\n",
            " - layer2.bias grad norm: 0.0005938897375017405\n",
            " - layer2_input.weight grad norm: 0.5092591643333435\n",
            " - layer2_input.bias grad norm: 0.0005938897375017405\n",
            " - layer3.weight grad norm: 0.0026381011120975018\n",
            " - layer3.bias grad norm: 0.0005499633844010532\n",
            " - layer3_input.weight grad norm: 0.4753724932670593\n",
            " - layer3_input.bias grad norm: 0.0005499633844010532\n",
            " - layer4.weight grad norm: 0.0030888556502759457\n",
            " - layer4.bias grad norm: 0.0005730721750296652\n",
            " - layer4_input.weight grad norm: 0.4925030469894409\n",
            " - layer4_input.bias grad norm: 0.0005730721750296652\n",
            " - layer5.weight grad norm: 0.011934660375118256\n",
            " - layer5.bias grad norm: 0.005254550836980343\n",
            "Gradients at iteration 35:\n",
            " - layer1.weight grad norm: 0.47969403862953186\n",
            " - layer1.bias grad norm: 0.0005507102468982339\n",
            " - layer2.weight grad norm: 0.0020162134896963835\n",
            " - layer2.bias grad norm: 0.000544730806723237\n",
            " - layer2_input.weight grad norm: 0.47363120317459106\n",
            " - layer2_input.bias grad norm: 0.000544730806723237\n",
            " - layer3.weight grad norm: 0.0027714078314602375\n",
            " - layer3.bias grad norm: 0.000583818880841136\n",
            " - layer3_input.weight grad norm: 0.5010288953781128\n",
            " - layer3_input.bias grad norm: 0.000583818880841136\n",
            " - layer4.weight grad norm: 0.0033244735095649958\n",
            " - layer4.bias grad norm: 0.0006350363837555051\n",
            " - layer4_input.weight grad norm: 0.5425019264221191\n",
            " - layer4_input.bias grad norm: 0.0006350363837555051\n",
            " - layer5.weight grad norm: 0.013099515810608864\n",
            " - layer5.bias grad norm: 0.00564950006082654\n",
            "Gradients at iteration 36:\n",
            " - layer1.weight grad norm: 0.5463499426841736\n",
            " - layer1.bias grad norm: 0.0006341937114484608\n",
            " - layer2.weight grad norm: 0.0019636256620287895\n",
            " - layer2.bias grad norm: 0.0005786135443486273\n",
            " - layer2_input.weight grad norm: 0.5012236833572388\n",
            " - layer2_input.bias grad norm: 0.0005786135443486273\n",
            " - layer3.weight grad norm: 0.0027192668057978153\n",
            " - layer3.bias grad norm: 0.0005658228183165193\n",
            " - layer3_input.weight grad norm: 0.490720272064209\n",
            " - layer3_input.bias grad norm: 0.0005658228183165193\n",
            " - layer4.weight grad norm: 0.003297245828434825\n",
            " - layer4.bias grad norm: 0.0005211035022512078\n",
            " - layer4_input.weight grad norm: 0.4574580192565918\n",
            " - layer4_input.bias grad norm: 0.0005211035022512078\n",
            " - layer5.weight grad norm: 0.012122459709644318\n",
            " - layer5.bias grad norm: 0.005548435263335705\n",
            "Gradients at iteration 37:\n",
            " - layer1.weight grad norm: 0.5148549675941467\n",
            " - layer1.bias grad norm: 0.0005974475061520934\n",
            " - layer2.weight grad norm: 0.0018759637605398893\n",
            " - layer2.bias grad norm: 0.0006046234047971666\n",
            " - layer2_input.weight grad norm: 0.5190166234970093\n",
            " - layer2_input.bias grad norm: 0.0006046234047971666\n",
            " - layer3.weight grad norm: 0.002592245815321803\n",
            " - layer3.bias grad norm: 0.0005864102859050035\n",
            " - layer3_input.weight grad norm: 0.5020483732223511\n",
            " - layer3_input.bias grad norm: 0.0005864102859050035\n",
            " - layer4.weight grad norm: 0.003062343457713723\n",
            " - layer4.bias grad norm: 0.0005340309580788016\n",
            " - layer4_input.weight grad norm: 0.46184036135673523\n",
            " - layer4_input.bias grad norm: 0.0005340309580788016\n",
            " - layer5.weight grad norm: 0.012150682508945465\n",
            " - layer5.bias grad norm: 0.005235318560153246\n",
            "Gradients at iteration 38:\n",
            " - layer1.weight grad norm: 0.5268417596817017\n",
            " - layer1.bias grad norm: 0.0006124970386736095\n",
            " - layer2.weight grad norm: 0.0019386355997994542\n",
            " - layer2.bias grad norm: 0.0006115810247138143\n",
            " - layer2_input.weight grad norm: 0.5234425067901611\n",
            " - layer2_input.bias grad norm: 0.0006115810247138143\n",
            " - layer3.weight grad norm: 0.0026617019902914762\n",
            " - layer3.bias grad norm: 0.0005982447764836252\n",
            " - layer3_input.weight grad norm: 0.5132801532745361\n",
            " - layer3_input.bias grad norm: 0.0005982447764836252\n",
            " - layer4.weight grad norm: 0.0031927356030792\n",
            " - layer4.bias grad norm: 0.00048607552889734507\n",
            " - layer4_input.weight grad norm: 0.42987319827079773\n",
            " - layer4_input.bias grad norm: 0.00048607552889734507\n",
            " - layer5.weight grad norm: 0.012060673907399178\n",
            " - layer5.bias grad norm: 0.005435111932456493\n",
            "Gradients at iteration 39:\n",
            " - layer1.weight grad norm: 0.5185428261756897\n",
            " - layer1.bias grad norm: 0.0005930378683842719\n",
            " - layer2.weight grad norm: 0.001985369250178337\n",
            " - layer2.bias grad norm: 0.0005756946629844606\n",
            " - layer2_input.weight grad norm: 0.5033062100410461\n",
            " - layer2_input.bias grad norm: 0.0005756946629844606\n",
            " - layer3.weight grad norm: 0.0027599993627518415\n",
            " - layer3.bias grad norm: 0.0005566352047026157\n",
            " - layer3_input.weight grad norm: 0.48486238718032837\n",
            " - layer3_input.bias grad norm: 0.0005566352047026157\n",
            " - layer4.weight grad norm: 0.003291261149570346\n",
            " - layer4.bias grad norm: 0.0005683162598870695\n",
            " - layer4_input.weight grad norm: 0.49245157837867737\n",
            " - layer4_input.bias grad norm: 0.0005683162598870695\n",
            " - layer5.weight grad norm: 0.011870160698890686\n",
            " - layer5.bias grad norm: 0.005525766871869564\n",
            "Gradients at iteration 40:\n",
            " - layer1.weight grad norm: 0.48079100251197815\n",
            " - layer1.bias grad norm: 0.0005482531269080937\n",
            " - layer2.weight grad norm: 0.0020465131383389235\n",
            " - layer2.bias grad norm: 0.0006041835877113044\n",
            " - layer2_input.weight grad norm: 0.5214825868606567\n",
            " - layer2_input.bias grad norm: 0.0006041835877113044\n",
            " - layer3.weight grad norm: 0.0028584771789610386\n",
            " - layer3.bias grad norm: 0.0006193324225023389\n",
            " - layer3_input.weight grad norm: 0.532667338848114\n",
            " - layer3_input.bias grad norm: 0.0006193324225023389\n",
            " - layer4.weight grad norm: 0.003417407860979438\n",
            " - layer4.bias grad norm: 0.000527729804161936\n",
            " - layer4_input.weight grad norm: 0.4614800214767456\n",
            " - layer4_input.bias grad norm: 0.000527729804161936\n",
            " - layer5.weight grad norm: 0.011775121092796326\n",
            " - layer5.bias grad norm: 0.00570197869092226\n",
            "Gradients at iteration 41:\n",
            " - layer1.weight grad norm: 0.569452166557312\n",
            " - layer1.bias grad norm: 0.0006676047923974693\n",
            " - layer2.weight grad norm: 0.0019497732864692807\n",
            " - layer2.bias grad norm: 0.0005675580468960106\n",
            " - layer2_input.weight grad norm: 0.4902821183204651\n",
            " - layer2_input.bias grad norm: 0.0005675580468960106\n",
            " - layer3.weight grad norm: 0.0027184877544641495\n",
            " - layer3.bias grad norm: 0.0005433017504401505\n",
            " - layer3_input.weight grad norm: 0.4743711054325104\n",
            " - layer3_input.bias grad norm: 0.0005433017504401505\n",
            " - layer4.weight grad norm: 0.0032609854824841022\n",
            " - layer4.bias grad norm: 0.0005235025892034173\n",
            " - layer4_input.weight grad norm: 0.4583900272846222\n",
            " - layer4_input.bias grad norm: 0.0005235025892034173\n",
            " - layer5.weight grad norm: 0.01200481690466404\n",
            " - layer5.bias grad norm: 0.005475741811096668\n",
            "Gradients at iteration 42:\n",
            " - layer1.weight grad norm: 0.5252838134765625\n",
            " - layer1.bias grad norm: 0.0006029322394169867\n",
            " - layer2.weight grad norm: 0.001953709637746215\n",
            " - layer2.bias grad norm: 0.0005483236163854599\n",
            " - layer2_input.weight grad norm: 0.4804025888442993\n",
            " - layer2_input.bias grad norm: 0.0005483236163854599\n",
            " - layer3.weight grad norm: 0.0027468700427562\n",
            " - layer3.bias grad norm: 0.0005971775390207767\n",
            " - layer3_input.weight grad norm: 0.51546710729599\n",
            " - layer3_input.bias grad norm: 0.0005971775390207767\n",
            " - layer4.weight grad norm: 0.003305562073364854\n",
            " - layer4.bias grad norm: 0.0005470559117384255\n",
            " - layer4_input.weight grad norm: 0.4768614172935486\n",
            " - layer4_input.bias grad norm: 0.0005470559117384255\n",
            " - layer5.weight grad norm: 0.01148101594299078\n",
            " - layer5.bias grad norm: 0.005540771409869194\n",
            "Gradients at iteration 43:\n",
            " - layer1.weight grad norm: 0.5155127048492432\n",
            " - layer1.bias grad norm: 0.0006079895538277924\n",
            " - layer2.weight grad norm: 0.0018799444660544395\n",
            " - layer2.bias grad norm: 0.0006081372848711908\n",
            " - layer2_input.weight grad norm: 0.5186527371406555\n",
            " - layer2_input.bias grad norm: 0.0006081372848711908\n",
            " - layer3.weight grad norm: 0.002627155976369977\n",
            " - layer3.bias grad norm: 0.0005640430608764291\n",
            " - layer3_input.weight grad norm: 0.48664525151252747\n",
            " - layer3_input.bias grad norm: 0.0005640430608764291\n",
            " - layer4.weight grad norm: 0.0030803056433796883\n",
            " - layer4.bias grad norm: 0.0005573239177465439\n",
            " - layer4_input.weight grad norm: 0.47771701216697693\n",
            " - layer4_input.bias grad norm: 0.0005573239177465439\n",
            " - layer5.weight grad norm: 0.012583082541823387\n",
            " - layer5.bias grad norm: 0.005303250625729561\n",
            "Gradients at iteration 44:\n",
            " - layer1.weight grad norm: 0.5070450901985168\n",
            " - layer1.bias grad norm: 0.0005829835426993668\n",
            " - layer2.weight grad norm: 0.002009088872000575\n",
            " - layer2.bias grad norm: 0.0005805982509627938\n",
            " - layer2_input.weight grad norm: 0.4993634819984436\n",
            " - layer2_input.bias grad norm: 0.0005805982509627938\n",
            " - layer3.weight grad norm: 0.002753461478278041\n",
            " - layer3.bias grad norm: 0.0005959078553132713\n",
            " - layer3_input.weight grad norm: 0.5111474990844727\n",
            " - layer3_input.bias grad norm: 0.0005959078553132713\n",
            " - layer4.weight grad norm: 0.0033282125368714333\n",
            " - layer4.bias grad norm: 0.0005587415071204305\n",
            " - layer4_input.weight grad norm: 0.4817425310611725\n",
            " - layer4_input.bias grad norm: 0.0005587415071204305\n",
            " - layer5.weight grad norm: 0.011746509000658989\n",
            " - layer5.bias grad norm: 0.005537705030292273\n",
            "Gradients at iteration 45:\n",
            " - layer1.weight grad norm: 0.5319842100143433\n",
            " - layer1.bias grad norm: 0.0006108642555773258\n",
            " - layer2.weight grad norm: 0.0018725913250818849\n",
            " - layer2.bias grad norm: 0.0005767064867541194\n",
            " - layer2_input.weight grad norm: 0.5027112364768982\n",
            " - layer2_input.bias grad norm: 0.0005767064867541194\n",
            " - layer3.weight grad norm: 0.0025834888219833374\n",
            " - layer3.bias grad norm: 0.0005550864734686911\n",
            " - layer3_input.weight grad norm: 0.4870719015598297\n",
            " - layer3_input.bias grad norm: 0.0005550864734686911\n",
            " - layer4.weight grad norm: 0.0031217418145388365\n",
            " - layer4.bias grad norm: 0.0005439547239802778\n",
            " - layer4_input.weight grad norm: 0.4762934744358063\n",
            " - layer4_input.bias grad norm: 0.0005439547239802778\n",
            " - layer5.weight grad norm: 0.011406547389924526\n",
            " - layer5.bias grad norm: 0.005244716536253691\n",
            "Gradients at iteration 46:\n",
            " - layer1.weight grad norm: 0.5256772637367249\n",
            " - layer1.bias grad norm: 0.0006010799552313983\n",
            " - layer2.weight grad norm: 0.002014805329963565\n",
            " - layer2.bias grad norm: 0.0006243528332561255\n",
            " - layer2_input.weight grad norm: 0.5357456803321838\n",
            " - layer2_input.bias grad norm: 0.0006243528332561255\n",
            " - layer3.weight grad norm: 0.002782081952318549\n",
            " - layer3.bias grad norm: 0.0005172383389435709\n",
            " - layer3_input.weight grad norm: 0.45300906896591187\n",
            " - layer3_input.bias grad norm: 0.0005172383389435709\n",
            " - layer4.weight grad norm: 0.003380212467163801\n",
            " - layer4.bias grad norm: 0.0005507526802830398\n",
            " - layer4_input.weight grad norm: 0.48084208369255066\n",
            " - layer4_input.bias grad norm: 0.0005507526802830398\n",
            " - layer5.weight grad norm: 0.012501895427703857\n",
            " - layer5.bias grad norm: 0.005632877815514803\n",
            "Gradients at iteration 47:\n",
            " - layer1.weight grad norm: 0.48908358812332153\n",
            " - layer1.bias grad norm: 0.0005588122876361012\n",
            " - layer2.weight grad norm: 0.0019314027158543468\n",
            " - layer2.bias grad norm: 0.0006132272537797689\n",
            " - layer2_input.weight grad norm: 0.5274770259857178\n",
            " - layer2_input.bias grad norm: 0.0006132272537797689\n",
            " - layer3.weight grad norm: 0.002668911125510931\n",
            " - layer3.bias grad norm: 0.0005532971117645502\n",
            " - layer3_input.weight grad norm: 0.4816688299179077\n",
            " - layer3_input.bias grad norm: 0.0005532971117645502\n",
            " - layer4.weight grad norm: 0.003199690952897072\n",
            " - layer4.bias grad norm: 0.0005809858557768166\n",
            " - layer4_input.weight grad norm: 0.5003649592399597\n",
            " - layer4_input.bias grad norm: 0.0005809858557768166\n",
            " - layer5.weight grad norm: 0.011941708624362946\n",
            " - layer5.bias grad norm: 0.0054027545265853405\n",
            "Gradients at iteration 48:\n",
            " - layer1.weight grad norm: 0.5034652352333069\n",
            " - layer1.bias grad norm: 0.0005815488984808326\n",
            " - layer2.weight grad norm: 0.0018716289196163416\n",
            " - layer2.bias grad norm: 0.0005922939162701368\n",
            " - layer2_input.weight grad norm: 0.5115352272987366\n",
            " - layer2_input.bias grad norm: 0.0005922939162701368\n",
            " - layer3.weight grad norm: 0.0026646440383046865\n",
            " - layer3.bias grad norm: 0.0005577693809755147\n",
            " - layer3_input.weight grad norm: 0.48384588956832886\n",
            " - layer3_input.bias grad norm: 0.0005577693809755147\n",
            " - layer4.weight grad norm: 0.003149930154904723\n",
            " - layer4.bias grad norm: 0.0005827223067171872\n",
            " - layer4_input.weight grad norm: 0.500551164150238\n",
            " - layer4_input.bias grad norm: 0.0005827223067171872\n",
            " - layer5.weight grad norm: 0.012036774307489395\n",
            " - layer5.bias grad norm: 0.005335176829248667\n",
            "Gradients at iteration 49:\n",
            " - layer1.weight grad norm: 0.5223283171653748\n",
            " - layer1.bias grad norm: 0.0006103435880504549\n",
            " - layer2.weight grad norm: 0.0019100360805168748\n",
            " - layer2.bias grad norm: 0.0005494694923982024\n",
            " - layer2_input.weight grad norm: 0.47728225588798523\n",
            " - layer2_input.bias grad norm: 0.0005494694923982024\n",
            " - layer3.weight grad norm: 0.0026575829833745956\n",
            " - layer3.bias grad norm: 0.0005695822183042765\n",
            " - layer3_input.weight grad norm: 0.49145451188087463\n",
            " - layer3_input.bias grad norm: 0.0005695822183042765\n",
            " - layer4.weight grad norm: 0.003133131191134453\n",
            " - layer4.bias grad norm: 0.0005948885227553546\n",
            " - layer4_input.weight grad norm: 0.5075744986534119\n",
            " - layer4_input.bias grad norm: 0.0005948885227553546\n",
            " - layer5.weight grad norm: 0.012800708413124084\n",
            " - layer5.bias grad norm: 0.005349361337721348\n",
            "Gradients at iteration 50:\n",
            " - layer1.weight grad norm: 0.5428769588470459\n",
            " - layer1.bias grad norm: 0.0006431144429370761\n",
            " - layer2.weight grad norm: 0.001980977598577738\n",
            " - layer2.bias grad norm: 0.0005437924992293119\n",
            " - layer2_input.weight grad norm: 0.46936482191085815\n",
            " - layer2_input.bias grad norm: 0.0005437924992293119\n",
            " - layer3.weight grad norm: 0.0028200382366776466\n",
            " - layer3.bias grad norm: 0.0006077740108594298\n",
            " - layer3_input.weight grad norm: 0.5135963559150696\n",
            " - layer3_input.bias grad norm: 0.0006077740108594298\n",
            " - layer4.weight grad norm: 0.003340240800753236\n",
            " - layer4.bias grad norm: 0.0005485721630975604\n",
            " - layer4_input.weight grad norm: 0.4701065123081207\n",
            " - layer4_input.bias grad norm: 0.0005485721630975604\n",
            " - layer5.weight grad norm: 0.01196074765175581\n",
            " - layer5.bias grad norm: 0.005626969039440155\n",
            "Gradients at iteration 51:\n",
            " - layer1.weight grad norm: 0.5338253974914551\n",
            " - layer1.bias grad norm: 0.0006278600194491446\n",
            " - layer2.weight grad norm: 0.00197281944565475\n",
            " - layer2.bias grad norm: 0.0005532835493795574\n",
            " - layer2_input.weight grad norm: 0.4769921600818634\n",
            " - layer2_input.bias grad norm: 0.0005532835493795574\n",
            " - layer3.weight grad norm: 0.0027135522104799747\n",
            " - layer3.bias grad norm: 0.0005338445189408958\n",
            " - layer3_input.weight grad norm: 0.4633064568042755\n",
            " - layer3_input.bias grad norm: 0.0005338445189408958\n",
            " - layer4.weight grad norm: 0.003267258871346712\n",
            " - layer4.bias grad norm: 0.0006165597587823868\n",
            " - layer4_input.weight grad norm: 0.5221569538116455\n",
            " - layer4_input.bias grad norm: 0.0006165597587823868\n",
            " - layer5.weight grad norm: 0.01240144856274128\n",
            " - layer5.bias grad norm: 0.005485492292791605\n",
            "Gradients at iteration 52:\n",
            " - layer1.weight grad norm: 0.5192139148712158\n",
            " - layer1.bias grad norm: 0.0006002629525028169\n",
            " - layer2.weight grad norm: 0.0018868668703362346\n",
            " - layer2.bias grad norm: 0.0005555676762014627\n",
            " - layer2_input.weight grad norm: 0.4829520583152771\n",
            " - layer2_input.bias grad norm: 0.0005555676762014627\n",
            " - layer3.weight grad norm: 0.002603947650641203\n",
            " - layer3.bias grad norm: 0.0005414393381215632\n",
            " - layer3_input.weight grad norm: 0.47131484746932983\n",
            " - layer3_input.bias grad norm: 0.0005414393381215632\n",
            " - layer4.weight grad norm: 0.0030801401007920504\n",
            " - layer4.bias grad norm: 0.000610757211688906\n",
            " - layer4_input.weight grad norm: 0.5242653489112854\n",
            " - layer4_input.bias grad norm: 0.000610757211688906\n",
            " - layer5.weight grad norm: 0.011524125002324581\n",
            " - layer5.bias grad norm: 0.005240336060523987\n",
            "Gradients at iteration 53:\n",
            " - layer1.weight grad norm: 0.5098752379417419\n",
            " - layer1.bias grad norm: 0.0005909252213314176\n",
            " - layer2.weight grad norm: 0.0019228648161515594\n",
            " - layer2.bias grad norm: 0.0006013920065015554\n",
            " - layer2_input.weight grad norm: 0.5132623314857483\n",
            " - layer2_input.bias grad norm: 0.0006013920065015554\n",
            " - layer3.weight grad norm: 0.00267296121455729\n",
            " - layer3.bias grad norm: 0.0005404541734606028\n",
            " - layer3_input.weight grad norm: 0.46797946095466614\n",
            " - layer3_input.bias grad norm: 0.0005404541734606028\n",
            " - layer4.weight grad norm: 0.0031628485303372145\n",
            " - layer4.bias grad norm: 0.0005977127584628761\n",
            " - layer4_input.weight grad norm: 0.507320761680603\n",
            " - layer4_input.bias grad norm: 0.0005977127584628761\n",
            " - layer5.weight grad norm: 0.012565954588353634\n",
            " - layer5.bias grad norm: 0.005368590354919434\n",
            "Gradients at iteration 54:\n",
            " - layer1.weight grad norm: 0.5252411365509033\n",
            " - layer1.bias grad norm: 0.0006158138858154416\n",
            " - layer2.weight grad norm: 0.001835889765061438\n",
            " - layer2.bias grad norm: 0.0006028827046975493\n",
            " - layer2_input.weight grad norm: 0.514977216720581\n",
            " - layer2_input.bias grad norm: 0.0006028827046975493\n",
            " - layer3.weight grad norm: 0.0025721366982907057\n",
            " - layer3.bias grad norm: 0.0005386969423852861\n",
            " - layer3_input.weight grad norm: 0.4636784493923187\n",
            " - layer3_input.bias grad norm: 0.0005386969423852861\n",
            " - layer4.weight grad norm: 0.0030313958413898945\n",
            " - layer4.bias grad norm: 0.0005792885203845799\n",
            " - layer4_input.weight grad norm: 0.49369314312934875\n",
            " - layer4_input.bias grad norm: 0.0005792885203845799\n",
            " - layer5.weight grad norm: 0.011902145110070705\n",
            " - layer5.bias grad norm: 0.005146303214132786\n",
            "Gradients at iteration 55:\n",
            " - layer1.weight grad norm: 0.531159520149231\n",
            " - layer1.bias grad norm: 0.0006232155719771981\n",
            " - layer2.weight grad norm: 0.0018458926351740956\n",
            " - layer2.bias grad norm: 0.000558647618163377\n",
            " - layer2_input.weight grad norm: 0.4831753075122833\n",
            " - layer2_input.bias grad norm: 0.000558647618163377\n",
            " - layer3.weight grad norm: 0.0026034540496766567\n",
            " - layer3.bias grad norm: 0.0005926234880462289\n",
            " - layer3_input.weight grad norm: 0.5064848065376282\n",
            " - layer3_input.bias grad norm: 0.0005926234880462289\n",
            " - layer4.weight grad norm: 0.003119078930467367\n",
            " - layer4.bias grad norm: 0.0005513402866199613\n",
            " - layer4_input.weight grad norm: 0.4771747887134552\n",
            " - layer4_input.bias grad norm: 0.0005513402866199613\n",
            " - layer5.weight grad norm: 0.011783083900809288\n",
            " - layer5.bias grad norm: 0.005237307399511337\n",
            "Gradients at iteration 56:\n",
            " - layer1.weight grad norm: 0.49783679842948914\n",
            " - layer1.bias grad norm: 0.0005802908563055098\n",
            " - layer2.weight grad norm: 0.0019202030962333083\n",
            " - layer2.bias grad norm: 0.0006098245503380895\n",
            " - layer2_input.weight grad norm: 0.518258273601532\n",
            " - layer2_input.bias grad norm: 0.0006098245503380895\n",
            " - layer3.weight grad norm: 0.0027015877421945333\n",
            " - layer3.bias grad norm: 0.0006208720151335001\n",
            " - layer3_input.weight grad norm: 0.5259708762168884\n",
            " - layer3_input.bias grad norm: 0.0006208720151335001\n",
            " - layer4.weight grad norm: 0.0032388712279498577\n",
            " - layer4.bias grad norm: 0.0005275097792036831\n",
            " - layer4_input.weight grad norm: 0.45465734601020813\n",
            " - layer4_input.bias grad norm: 0.0005275097792036831\n",
            " - layer5.weight grad norm: 0.012461830861866474\n",
            " - layer5.bias grad norm: 0.005383577663451433\n",
            "Gradients at iteration 57:\n",
            " - layer1.weight grad norm: 0.5154279470443726\n",
            " - layer1.bias grad norm: 0.0005912098567932844\n",
            " - layer2.weight grad norm: 0.0019761433359235525\n",
            " - layer2.bias grad norm: 0.0006146246450953186\n",
            " - layer2_input.weight grad norm: 0.5315249562263489\n",
            " - layer2_input.bias grad norm: 0.0006146246450953186\n",
            " - layer3.weight grad norm: 0.002711121691390872\n",
            " - layer3.bias grad norm: 0.0005806604167446494\n",
            " - layer3_input.weight grad norm: 0.5031077265739441\n",
            " - layer3_input.bias grad norm: 0.0005806604167446494\n",
            " - layer4.weight grad norm: 0.0032407534308731556\n",
            " - layer4.bias grad norm: 0.000508424243889749\n",
            " - layer4_input.weight grad norm: 0.44550585746765137\n",
            " - layer4_input.bias grad norm: 0.000508424243889749\n",
            " - layer5.weight grad norm: 0.012985294684767723\n",
            " - layer5.bias grad norm: 0.005450758617371321\n",
            "Gradients at iteration 58:\n",
            " - layer1.weight grad norm: 0.49527403712272644\n",
            " - layer1.bias grad norm: 0.0005698988679796457\n",
            " - layer2.weight grad norm: 0.002002574037760496\n",
            " - layer2.bias grad norm: 0.000624178268481046\n",
            " - layer2_input.weight grad norm: 0.5328024625778198\n",
            " - layer2_input.bias grad norm: 0.000624178268481046\n",
            " - layer3.weight grad norm: 0.002788887359201908\n",
            " - layer3.bias grad norm: 0.0005643568001687527\n",
            " - layer3_input.weight grad norm: 0.48569759726524353\n",
            " - layer3_input.bias grad norm: 0.0005643568001687527\n",
            " - layer4.weight grad norm: 0.0033632058184593916\n",
            " - layer4.bias grad norm: 0.0005622048629447818\n",
            " - layer4_input.weight grad norm: 0.4844602942466736\n",
            " - layer4_input.bias grad norm: 0.0005622048629447818\n",
            " - layer5.weight grad norm: 0.012799267657101154\n",
            " - layer5.bias grad norm: 0.005641091149300337\n",
            "Gradients at iteration 59:\n",
            " - layer1.weight grad norm: 0.5302746891975403\n",
            " - layer1.bias grad norm: 0.0006157069001346827\n",
            " - layer2.weight grad norm: 0.0018874790985137224\n",
            " - layer2.bias grad norm: 0.0005711517296731472\n",
            " - layer2_input.weight grad norm: 0.4956132769584656\n",
            " - layer2_input.bias grad norm: 0.0005711517296731472\n",
            " - layer3.weight grad norm: 0.002645062515512109\n",
            " - layer3.bias grad norm: 0.0005955887609161437\n",
            " - layer3_input.weight grad norm: 0.5098896026611328\n",
            " - layer3_input.bias grad norm: 0.0005955887609161437\n",
            " - layer4.weight grad norm: 0.0031254570931196213\n",
            " - layer4.bias grad norm: 0.000531573430635035\n",
            " - layer4_input.weight grad norm: 0.461535781621933\n",
            " - layer4_input.bias grad norm: 0.000531573430635035\n",
            " - layer5.weight grad norm: 0.011095368303358555\n",
            " - layer5.bias grad norm: 0.005272792186588049\n",
            "Gradients at iteration 60:\n",
            " - layer1.weight grad norm: 0.542026698589325\n",
            " - layer1.bias grad norm: 0.0006443305755965412\n",
            " - layer2.weight grad norm: 0.0018694306490942836\n",
            " - layer2.bias grad norm: 0.0005780194187536836\n",
            " - layer2_input.weight grad norm: 0.49509161710739136\n",
            " - layer2_input.bias grad norm: 0.0005780194187536836\n",
            " - layer3.weight grad norm: 0.0025551151484251022\n",
            " - layer3.bias grad norm: 0.0005597673007287085\n",
            " - layer3_input.weight grad norm: 0.4784460663795471\n",
            " - layer3_input.bias grad norm: 0.0005597673007287085\n",
            " - layer4.weight grad norm: 0.0030712787993252277\n",
            " - layer4.bias grad norm: 0.0005676796426996589\n",
            " - layer4_input.weight grad norm: 0.4816727340221405\n",
            " - layer4_input.bias grad norm: 0.0005676796426996589\n",
            " - layer5.weight grad norm: 0.011112150736153126\n",
            " - layer5.bias grad norm: 0.005184969864785671\n",
            "Gradients at iteration 61:\n",
            " - layer1.weight grad norm: 0.5367518663406372\n",
            " - layer1.bias grad norm: 0.000614362652413547\n",
            " - layer2.weight grad norm: 0.0018936963751912117\n",
            " - layer2.bias grad norm: 0.0005829656147398055\n",
            " - layer2_input.weight grad norm: 0.5099276304244995\n",
            " - layer2_input.bias grad norm: 0.0005829656147398055\n",
            " - layer3.weight grad norm: 0.002670603571459651\n",
            " - layer3.bias grad norm: 0.000518727523740381\n",
            " - layer3_input.weight grad norm: 0.45903655886650085\n",
            " - layer3_input.bias grad norm: 0.000518727523740381\n",
            " - layer4.weight grad norm: 0.0032162354327738285\n",
            " - layer4.bias grad norm: 0.0005551835638470948\n",
            " - layer4_input.weight grad norm: 0.4908386170864105\n",
            " - layer4_input.bias grad norm: 0.0005551835638470948\n",
            " - layer5.weight grad norm: 0.013488893397152424\n",
            " - layer5.bias grad norm: 0.005374077241867781\n",
            "Gradients at iteration 62:\n",
            " - layer1.weight grad norm: 0.5451958179473877\n",
            " - layer1.bias grad norm: 0.0006331783370114863\n",
            " - layer2.weight grad norm: 0.0019947998225688934\n",
            " - layer2.bias grad norm: 0.0005885640857741237\n",
            " - layer2_input.weight grad norm: 0.5097156763076782\n",
            " - layer2_input.bias grad norm: 0.0005885640857741237\n",
            " - layer3.weight grad norm: 0.002838214859366417\n",
            " - layer3.bias grad norm: 0.0005560143617913127\n",
            " - layer3_input.weight grad norm: 0.4857655465602875\n",
            " - layer3_input.bias grad norm: 0.0005560143617913127\n",
            " - layer4.weight grad norm: 0.0033696519676595926\n",
            " - layer4.bias grad norm: 0.0005144235328771174\n",
            " - layer4_input.weight grad norm: 0.45470961928367615\n",
            " - layer4_input.bias grad norm: 0.0005144235328771174\n",
            " - layer5.weight grad norm: 0.01285700686275959\n",
            " - layer5.bias grad norm: 0.005621425341814756\n",
            "Gradients at iteration 63:\n",
            " - layer1.weight grad norm: 0.5299198627471924\n",
            " - layer1.bias grad norm: 0.0006197674665600061\n",
            " - layer2.weight grad norm: 0.0019201884279027581\n",
            " - layer2.bias grad norm: 0.0005298506002873182\n",
            " - layer2_input.weight grad norm: 0.46456706523895264\n",
            " - layer2_input.bias grad norm: 0.0005298506002873182\n",
            " - layer3.weight grad norm: 0.002652601571753621\n",
            " - layer3.bias grad norm: 0.00058165960945189\n",
            " - layer3_input.weight grad norm: 0.5019941329956055\n",
            " - layer3_input.bias grad norm: 0.00058165960945189\n",
            " - layer4.weight grad norm: 0.0031817727722227573\n",
            " - layer4.bias grad norm: 0.0005841528763994575\n",
            " - layer4_input.weight grad norm: 0.5011332035064697\n",
            " - layer4_input.bias grad norm: 0.0005841528763994575\n",
            " - layer5.weight grad norm: 0.013334397226572037\n",
            " - layer5.bias grad norm: 0.005373377352952957\n",
            "Gradients at iteration 64:\n",
            " - layer1.weight grad norm: 0.5344094038009644\n",
            " - layer1.bias grad norm: 0.000626742432359606\n",
            " - layer2.weight grad norm: 0.0019114867318421602\n",
            " - layer2.bias grad norm: 0.0005583108286373317\n",
            " - layer2_input.weight grad norm: 0.48340779542922974\n",
            " - layer2_input.bias grad norm: 0.0005583108286373317\n",
            " - layer3.weight grad norm: 0.0026282055769115686\n",
            " - layer3.bias grad norm: 0.0005685751093551517\n",
            " - layer3_input.weight grad norm: 0.49132367968559265\n",
            " - layer3_input.bias grad norm: 0.0005685751093551517\n",
            " - layer4.weight grad norm: 0.003188671078532934\n",
            " - layer4.bias grad norm: 0.0005694492720067501\n",
            " - layer4_input.weight grad norm: 0.4889982044696808\n",
            " - layer4_input.bias grad norm: 0.0005694492720067501\n",
            " - layer5.weight grad norm: 0.01241031289100647\n",
            " - layer5.bias grad norm: 0.005317473318427801\n",
            "Gradients at iteration 65:\n",
            " - layer1.weight grad norm: 0.536341667175293\n",
            " - layer1.bias grad norm: 0.0006294883787631989\n",
            " - layer2.weight grad norm: 0.0019568565767258406\n",
            " - layer2.bias grad norm: 0.0006193406297825277\n",
            " - layer2_input.weight grad norm: 0.5273528695106506\n",
            " - layer2_input.bias grad norm: 0.0006193406297825277\n",
            " - layer3.weight grad norm: 0.0027414800133556128\n",
            " - layer3.bias grad norm: 0.0005318405455909669\n",
            " - layer3_input.weight grad norm: 0.461249440908432\n",
            " - layer3_input.bias grad norm: 0.0005318405455909669\n",
            " - layer4.weight grad norm: 0.003233708208426833\n",
            " - layer4.bias grad norm: 0.0005477680824697018\n",
            " - layer4_input.weight grad norm: 0.4703865945339203\n",
            " - layer4_input.bias grad norm: 0.0005477680824697018\n",
            " - layer5.weight grad norm: 0.012947441078722477\n",
            " - layer5.bias grad norm: 0.0055112349800765514\n",
            "Gradients at iteration 66:\n",
            " - layer1.weight grad norm: 0.5289502143859863\n",
            " - layer1.bias grad norm: 0.0006169580155983567\n",
            " - layer2.weight grad norm: 0.001932163373567164\n",
            " - layer2.bias grad norm: 0.0005813674652017653\n",
            " - layer2_input.weight grad norm: 0.5044266581535339\n",
            " - layer2_input.bias grad norm: 0.0005813674652017653\n",
            " - layer3.weight grad norm: 0.002685131272301078\n",
            " - layer3.bias grad norm: 0.0005479850806295872\n",
            " - layer3_input.weight grad norm: 0.474275141954422\n",
            " - layer3_input.bias grad norm: 0.0005479850806295872\n",
            " - layer4.weight grad norm: 0.0031867483630776405\n",
            " - layer4.bias grad norm: 0.0005696634179912508\n",
            " - layer4_input.weight grad norm: 0.4905460774898529\n",
            " - layer4_input.bias grad norm: 0.0005696634179912508\n",
            " - layer5.weight grad norm: 0.011873403564095497\n",
            " - layer5.bias grad norm: 0.005345727317035198\n",
            "Gradients at iteration 67:\n",
            " - layer1.weight grad norm: 0.5273799896240234\n",
            " - layer1.bias grad norm: 0.0006078116130083799\n",
            " - layer2.weight grad norm: 0.0020405102986842394\n",
            " - layer2.bias grad norm: 0.0005835057818330824\n",
            " - layer2_input.weight grad norm: 0.5085675716400146\n",
            " - layer2_input.bias grad norm: 0.0005835057818330824\n",
            " - layer3.weight grad norm: 0.002784965792670846\n",
            " - layer3.bias grad norm: 0.0005880377138964832\n",
            " - layer3_input.weight grad norm: 0.5086557269096375\n",
            " - layer3_input.bias grad norm: 0.0005880377138964832\n",
            " - layer4.weight grad norm: 0.003347729565575719\n",
            " - layer4.bias grad norm: 0.000517060630954802\n",
            " - layer4_input.weight grad norm: 0.4519563615322113\n",
            " - layer4_input.bias grad norm: 0.000517060630954802\n",
            " - layer5.weight grad norm: 0.013300304301083088\n",
            " - layer5.bias grad norm: 0.005665894597768784\n",
            "Gradients at iteration 68:\n",
            " - layer1.weight grad norm: 0.5040491819381714\n",
            " - layer1.bias grad norm: 0.0005899282405152917\n",
            " - layer2.weight grad norm: 0.001909363199956715\n",
            " - layer2.bias grad norm: 0.0006104169879108667\n",
            " - layer2_input.weight grad norm: 0.5173662900924683\n",
            " - layer2_input.bias grad norm: 0.0006104169879108667\n",
            " - layer3.weight grad norm: 0.0026406063698232174\n",
            " - layer3.bias grad norm: 0.000607254623901099\n",
            " - layer3_input.weight grad norm: 0.5131466388702393\n",
            " - layer3_input.bias grad norm: 0.000607254623901099\n",
            " - layer4.weight grad norm: 0.003128737211227417\n",
            " - layer4.bias grad norm: 0.0005453098565340042\n",
            " - layer4_input.weight grad norm: 0.4634168744087219\n",
            " - layer4_input.bias grad norm: 0.0005453098565340042\n",
            " - layer5.weight grad norm: 0.011867146007716656\n",
            " - layer5.bias grad norm: 0.005310700740665197\n",
            "Gradients at iteration 69:\n",
            " - layer1.weight grad norm: 0.5163195133209229\n",
            " - layer1.bias grad norm: 0.0005963147268630564\n",
            " - layer2.weight grad norm: 0.0019107855623587966\n",
            " - layer2.bias grad norm: 0.000587815186008811\n",
            " - layer2_input.weight grad norm: 0.5107372403144836\n",
            " - layer2_input.bias grad norm: 0.000587815186008811\n",
            " - layer3.weight grad norm: 0.002627659123390913\n",
            " - layer3.bias grad norm: 0.0005738153704442084\n",
            " - layer3_input.weight grad norm: 0.4964004158973694\n",
            " - layer3_input.bias grad norm: 0.0005738153704442084\n",
            " - layer4.weight grad norm: 0.0031900506000965834\n",
            " - layer4.bias grad norm: 0.0005506923771463335\n",
            " - layer4_input.weight grad norm: 0.4753430485725403\n",
            " - layer4_input.bias grad norm: 0.0005506923771463335\n",
            " - layer5.weight grad norm: 0.012055939994752407\n",
            " - layer5.bias grad norm: 0.005378038622438908\n",
            "Gradients at iteration 70:\n",
            " - layer1.weight grad norm: 0.522203266620636\n",
            " - layer1.bias grad norm: 0.0006090984679758549\n",
            " - layer2.weight grad norm: 0.0019179434748366475\n",
            " - layer2.bias grad norm: 0.0006175677990540862\n",
            " - layer2_input.weight grad norm: 0.5293606519699097\n",
            " - layer2_input.bias grad norm: 0.0006175677990540862\n",
            " - layer3.weight grad norm: 0.00267934612929821\n",
            " - layer3.bias grad norm: 0.0005808538408018649\n",
            " - layer3_input.weight grad norm: 0.4970338046550751\n",
            " - layer3_input.bias grad norm: 0.0005808538408018649\n",
            " - layer4.weight grad norm: 0.0031441834289580584\n",
            " - layer4.bias grad norm: 0.0005098016117699444\n",
            " - layer4_input.weight grad norm: 0.44703972339630127\n",
            " - layer4_input.bias grad norm: 0.0005098016117699444\n",
            " - layer5.weight grad norm: 0.011904862709343433\n",
            " - layer5.bias grad norm: 0.005405495874583721\n",
            "Gradients at iteration 71:\n",
            " - layer1.weight grad norm: 0.5270944833755493\n",
            " - layer1.bias grad norm: 0.0006129723624326289\n",
            " - layer2.weight grad norm: 0.002000671112909913\n",
            " - layer2.bias grad norm: 0.0006042325403541327\n",
            " - layer2_input.weight grad norm: 0.5208581686019897\n",
            " - layer2_input.bias grad norm: 0.0006042325403541327\n",
            " - layer3.weight grad norm: 0.0027993971016258\n",
            " - layer3.bias grad norm: 0.0005431679892353714\n",
            " - layer3_input.weight grad norm: 0.47238689661026\n",
            " - layer3_input.bias grad norm: 0.0005431679892353714\n",
            " - layer4.weight grad norm: 0.003327477490529418\n",
            " - layer4.bias grad norm: 0.0005538499681279063\n",
            " - layer4_input.weight grad norm: 0.47697463631629944\n",
            " - layer4_input.bias grad norm: 0.0005538499681279063\n",
            " - layer5.weight grad norm: 0.012942121364176273\n",
            " - layer5.bias grad norm: 0.005597363691776991\n",
            "Gradients at iteration 72:\n",
            " - layer1.weight grad norm: 0.5290660858154297\n",
            " - layer1.bias grad norm: 0.0006029849755577743\n",
            " - layer2.weight grad norm: 0.001948875142261386\n",
            " - layer2.bias grad norm: 0.0006167312967590988\n",
            " - layer2_input.weight grad norm: 0.5349947214126587\n",
            " - layer2_input.bias grad norm: 0.0006167312967590988\n",
            " - layer3.weight grad norm: 0.002664347644895315\n",
            " - layer3.bias grad norm: 0.0005014127236790955\n",
            " - layer3_input.weight grad norm: 0.4472227096557617\n",
            " - layer3_input.bias grad norm: 0.0005014127236790955\n",
            " - layer4.weight grad norm: 0.0032715473789721727\n",
            " - layer4.bias grad norm: 0.0005517361569218338\n",
            " - layer4_input.weight grad norm: 0.48335984349250793\n",
            " - layer4_input.bias grad norm: 0.0005517361569218338\n",
            " - layer5.weight grad norm: 0.013088214211165905\n",
            " - layer5.bias grad norm: 0.005458661820739508\n",
            "Gradients at iteration 73:\n",
            " - layer1.weight grad norm: 0.5464935898780823\n",
            " - layer1.bias grad norm: 0.0006415277603082359\n",
            " - layer2.weight grad norm: 0.001928596873767674\n",
            " - layer2.bias grad norm: 0.0005631946260109544\n",
            " - layer2_input.weight grad norm: 0.4901692569255829\n",
            " - layer2_input.bias grad norm: 0.0005631946260109544\n",
            " - layer3.weight grad norm: 0.0026663471944630146\n",
            " - layer3.bias grad norm: 0.0005425954004749656\n",
            " - layer3_input.weight grad norm: 0.47007906436920166\n",
            " - layer3_input.bias grad norm: 0.0005425954004749656\n",
            " - layer4.weight grad norm: 0.0032102041877806187\n",
            " - layer4.bias grad norm: 0.0005718170432373881\n",
            " - layer4_input.weight grad norm: 0.4897880256175995\n",
            " - layer4_input.bias grad norm: 0.0005718170432373881\n",
            " - layer5.weight grad norm: 0.012578221037983894\n",
            " - layer5.bias grad norm: 0.005509782582521439\n",
            "Gradients at iteration 74:\n",
            " - layer1.weight grad norm: 0.4842762053012848\n",
            " - layer1.bias grad norm: 0.0005503916181623936\n",
            " - layer2.weight grad norm: 0.0020068057347089052\n",
            " - layer2.bias grad norm: 0.0005674823187291622\n",
            " - layer2_input.weight grad norm: 0.49349087476730347\n",
            " - layer2_input.bias grad norm: 0.0005674823187291622\n",
            " - layer3.weight grad norm: 0.0027866503223776817\n",
            " - layer3.bias grad norm: 0.0006222891970537603\n",
            " - layer3_input.weight grad norm: 0.5331366658210754\n",
            " - layer3_input.bias grad norm: 0.0006222891970537603\n",
            " - layer4.weight grad norm: 0.003339081536978483\n",
            " - layer4.bias grad norm: 0.0005571250803768635\n",
            " - layer4_input.weight grad norm: 0.4873490333557129\n",
            " - layer4_input.bias grad norm: 0.0005571250803768635\n",
            " - layer5.weight grad norm: 0.01198932807892561\n",
            " - layer5.bias grad norm: 0.005540404934436083\n",
            "Gradients at iteration 75:\n",
            " - layer1.weight grad norm: 0.5549293756484985\n",
            " - layer1.bias grad norm: 0.0006469538784585893\n",
            " - layer2.weight grad norm: 0.002018385101109743\n",
            " - layer2.bias grad norm: 0.0005700051551684737\n",
            " - layer2_input.weight grad norm: 0.4932517111301422\n",
            " - layer2_input.bias grad norm: 0.0005700051551684737\n",
            " - layer3.weight grad norm: 0.0028017337899655104\n",
            " - layer3.bias grad norm: 0.0005444225971587002\n",
            " - layer3_input.weight grad norm: 0.47521600127220154\n",
            " - layer3_input.bias grad norm: 0.0005444225971587002\n",
            " - layer4.weight grad norm: 0.003326770383864641\n",
            " - layer4.bias grad norm: 0.0005435770144686103\n",
            " - layer4_input.weight grad norm: 0.4719250202178955\n",
            " - layer4_input.bias grad norm: 0.0005435770144686103\n",
            " - layer5.weight grad norm: 0.012480116449296474\n",
            " - layer5.bias grad norm: 0.005619502160698175\n",
            "Gradients at iteration 76:\n",
            " - layer1.weight grad norm: 0.5213370323181152\n",
            " - layer1.bias grad norm: 0.0006032820092514157\n",
            " - layer2.weight grad norm: 0.0019181958632543683\n",
            " - layer2.bias grad norm: 0.0005765619571320713\n",
            " - layer2_input.weight grad norm: 0.503045916557312\n",
            " - layer2_input.bias grad norm: 0.0005765619571320713\n",
            " - layer3.weight grad norm: 0.0026676724664866924\n",
            " - layer3.bias grad norm: 0.0005356734618544579\n",
            " - layer3_input.weight grad norm: 0.46538975834846497\n",
            " - layer3_input.bias grad norm: 0.0005356734618544579\n",
            " - layer4.weight grad norm: 0.003158623119816184\n",
            " - layer4.bias grad norm: 0.000590360548812896\n",
            " - layer4_input.weight grad norm: 0.5082879066467285\n",
            " - layer4_input.bias grad norm: 0.000590360548812896\n",
            " - layer5.weight grad norm: 0.012495111674070358\n",
            " - layer5.bias grad norm: 0.005396755412220955\n",
            "Gradients at iteration 77:\n",
            " - layer1.weight grad norm: 0.5405784845352173\n",
            " - layer1.bias grad norm: 0.000639112142380327\n",
            " - layer2.weight grad norm: 0.0018728516297414899\n",
            " - layer2.bias grad norm: 0.0006103077903389931\n",
            " - layer2_input.weight grad norm: 0.5186719298362732\n",
            " - layer2_input.bias grad norm: 0.0006103077903389931\n",
            " - layer3.weight grad norm: 0.0025759064592421055\n",
            " - layer3.bias grad norm: 0.0005804420216009021\n",
            " - layer3_input.weight grad norm: 0.49368155002593994\n",
            " - layer3_input.bias grad norm: 0.0005804420216009021\n",
            " - layer4.weight grad norm: 0.003076536115258932\n",
            " - layer4.bias grad norm: 0.0005061915726400912\n",
            " - layer4_input.weight grad norm: 0.44138988852500916\n",
            " - layer4_input.bias grad norm: 0.0005061915726400912\n",
            " - layer5.weight grad norm: 0.012608799152076244\n",
            " - layer5.bias grad norm: 0.005196201615035534\n",
            "Gradients at iteration 78:\n",
            " - layer1.weight grad norm: 0.5383310317993164\n",
            " - layer1.bias grad norm: 0.0006331196636892855\n",
            " - layer2.weight grad norm: 0.0018675976898521185\n",
            " - layer2.bias grad norm: 0.0005433399346657097\n",
            " - layer2_input.weight grad norm: 0.46923860907554626\n",
            " - layer2_input.bias grad norm: 0.0005433399346657097\n",
            " - layer3.weight grad norm: 0.002631004201248288\n",
            " - layer3.bias grad norm: 0.000580690975766629\n",
            " - layer3_input.weight grad norm: 0.4953545928001404\n",
            " - layer3_input.bias grad norm: 0.000580690975766629\n",
            " - layer4.weight grad norm: 0.0031213252805173397\n",
            " - layer4.bias grad norm: 0.0005789478891529143\n",
            " - layer4_input.weight grad norm: 0.4944084584712982\n",
            " - layer4_input.bias grad norm: 0.0005789478891529143\n",
            " - layer5.weight grad norm: 0.012181442230939865\n",
            " - layer5.bias grad norm: 0.0052948808297514915\n",
            "Gradients at iteration 79:\n",
            " - layer1.weight grad norm: 0.48473823070526123\n",
            " - layer1.bias grad norm: 0.0005467296577990055\n",
            " - layer2.weight grad norm: 0.0020299400202929974\n",
            " - layer2.bias grad norm: 0.0005619594012387097\n",
            " - layer2_input.weight grad norm: 0.4937869906425476\n",
            " - layer2_input.bias grad norm: 0.0005619594012387097\n",
            " - layer3.weight grad norm: 0.0028239076491445303\n",
            " - layer3.bias grad norm: 0.0006213770247995853\n",
            " - layer3_input.weight grad norm: 0.5369610786437988\n",
            " - layer3_input.bias grad norm: 0.0006213770247995853\n",
            " - layer4.weight grad norm: 0.0033177242148667574\n",
            " - layer4.bias grad norm: 0.0005561093566939235\n",
            " - layer4_input.weight grad norm: 0.48232534527778625\n",
            " - layer4_input.bias grad norm: 0.0005561093566939235\n",
            " - layer5.weight grad norm: 0.013456284068524837\n",
            " - layer5.bias grad norm: 0.0056485701352357864\n",
            "Gradients at iteration 80:\n",
            " - layer1.weight grad norm: 0.48790234327316284\n",
            " - layer1.bias grad norm: 0.0005556227988563478\n",
            " - layer2.weight grad norm: 0.0019170051673427224\n",
            " - layer2.bias grad norm: 0.0006330334581434727\n",
            " - layer2_input.weight grad norm: 0.5455073714256287\n",
            " - layer2_input.bias grad norm: 0.0006330334581434727\n",
            " - layer3.weight grad norm: 0.002623490057885647\n",
            " - layer3.bias grad norm: 0.000558024097699672\n",
            " - layer3_input.weight grad norm: 0.4862561821937561\n",
            " - layer3_input.bias grad norm: 0.000558024097699672\n",
            " - layer4.weight grad norm: 0.003100152825936675\n",
            " - layer4.bias grad norm: 0.0005476514343172312\n",
            " - layer4_input.weight grad norm: 0.4772239923477173\n",
            " - layer4_input.bias grad norm: 0.0005476514343172312\n",
            " - layer5.weight grad norm: 0.011595937423408031\n",
            " - layer5.bias grad norm: 0.005309392232447863\n",
            "Gradients at iteration 81:\n",
            " - layer1.weight grad norm: 0.5124300122261047\n",
            " - layer1.bias grad norm: 0.0005945345619693398\n",
            " - layer2.weight grad norm: 0.0018924297764897346\n",
            " - layer2.bias grad norm: 0.0005852515459991992\n",
            " - layer2_input.weight grad norm: 0.5039288401603699\n",
            " - layer2_input.bias grad norm: 0.0005852515459991992\n",
            " - layer3.weight grad norm: 0.0026108273304998875\n",
            " - layer3.bias grad norm: 0.0005885552382096648\n",
            " - layer3_input.weight grad norm: 0.5069951415061951\n",
            " - layer3_input.bias grad norm: 0.0005885552382096648\n",
            " - layer4.weight grad norm: 0.0031375836115330458\n",
            " - layer4.bias grad norm: 0.000554880010895431\n",
            " - layer4_input.weight grad norm: 0.47564783692359924\n",
            " - layer4_input.bias grad norm: 0.000554880010895431\n",
            " - layer5.weight grad norm: 0.011640338227152824\n",
            " - layer5.bias grad norm: 0.005310819949954748\n",
            "Gradients at iteration 82:\n",
            " - layer1.weight grad norm: 0.5456216931343079\n",
            " - layer1.bias grad norm: 0.0006352131604216993\n",
            " - layer2.weight grad norm: 0.0018824426224455237\n",
            " - layer2.bias grad norm: 0.0005517981480807066\n",
            " - layer2_input.weight grad norm: 0.4801191985607147\n",
            " - layer2_input.bias grad norm: 0.0005517981480807066\n",
            " - layer3.weight grad norm: 0.002607279922813177\n",
            " - layer3.bias grad norm: 0.0005975083913654089\n",
            " - layer3_input.weight grad norm: 0.5122454762458801\n",
            " - layer3_input.bias grad norm: 0.0005975083913654089\n",
            " - layer4.weight grad norm: 0.0031819879077374935\n",
            " - layer4.bias grad norm: 0.00052041927119717\n",
            " - layer4_input.weight grad norm: 0.45736637711524963\n",
            " - layer4_input.bias grad norm: 0.00052041927119717\n",
            " - layer5.weight grad norm: 0.012347628362476826\n",
            " - layer5.bias grad norm: 0.005289445165544748\n",
            "Gradients at iteration 83:\n",
            " - layer1.weight grad norm: 0.5287625789642334\n",
            " - layer1.bias grad norm: 0.000610376475378871\n",
            " - layer2.weight grad norm: 0.002012160373851657\n",
            " - layer2.bias grad norm: 0.0005429527955129743\n",
            " - layer2_input.weight grad norm: 0.48089665174484253\n",
            " - layer2_input.bias grad norm: 0.0005429527955129743\n",
            " - layer3.weight grad norm: 0.0028001011814922094\n",
            " - layer3.bias grad norm: 0.0005816018674522638\n",
            " - layer3_input.weight grad norm: 0.5039024949073792\n",
            " - layer3_input.bias grad norm: 0.0005816018674522638\n",
            " - layer4.weight grad norm: 0.0033756897319108248\n",
            " - layer4.bias grad norm: 0.0005584800383076072\n",
            " - layer4_input.weight grad norm: 0.48478376865386963\n",
            " - layer4_input.bias grad norm: 0.0005584800383076072\n",
            " - layer5.weight grad norm: 0.012553921900689602\n",
            " - layer5.bias grad norm: 0.005689187441021204\n",
            "Gradients at iteration 84:\n",
            " - layer1.weight grad norm: 0.49357175827026367\n",
            " - layer1.bias grad norm: 0.0005599944852292538\n",
            " - layer2.weight grad norm: 0.0018946180352941155\n",
            " - layer2.bias grad norm: 0.0005770765710622072\n",
            " - layer2_input.weight grad norm: 0.5048688650131226\n",
            " - layer2_input.bias grad norm: 0.0005770765710622072\n",
            " - layer3.weight grad norm: 0.0026424461975693703\n",
            " - layer3.bias grad norm: 0.0006208096165210009\n",
            " - layer3_input.weight grad norm: 0.5349998474121094\n",
            " - layer3_input.bias grad norm: 0.0006208096165210009\n",
            " - layer4.weight grad norm: 0.003197914455085993\n",
            " - layer4.bias grad norm: 0.0005244686617515981\n",
            " - layer4_input.weight grad norm: 0.4637768268585205\n",
            " - layer4_input.bias grad norm: 0.0005244686617515981\n",
            " - layer5.weight grad norm: 0.011349583975970745\n",
            " - layer5.bias grad norm: 0.005364579614251852\n",
            "Gradients at iteration 85:\n",
            " - layer1.weight grad norm: 0.5477167367935181\n",
            " - layer1.bias grad norm: 0.0006347913877107203\n",
            " - layer2.weight grad norm: 0.001939682406373322\n",
            " - layer2.bias grad norm: 0.0005326438695192337\n",
            " - layer2_input.weight grad norm: 0.4662404954433441\n",
            " - layer2_input.bias grad norm: 0.0005326438695192337\n",
            " - layer3.weight grad norm: 0.0027011798229068518\n",
            " - layer3.bias grad norm: 0.0005249630194157362\n",
            " - layer3_input.weight grad norm: 0.46186748147010803\n",
            " - layer3_input.bias grad norm: 0.0005249630194157362\n",
            " - layer4.weight grad norm: 0.0032054183539003134\n",
            " - layer4.bias grad norm: 0.0006020311266183853\n",
            " - layer4_input.weight grad norm: 0.5187297463417053\n",
            " - layer4_input.bias grad norm: 0.0006020311266183853\n",
            " - layer5.weight grad norm: 0.013052673079073429\n",
            " - layer5.bias grad norm: 0.005495084915310144\n",
            "Gradients at iteration 86:\n",
            " - layer1.weight grad norm: 0.5327678322792053\n",
            " - layer1.bias grad norm: 0.0006178884650580585\n",
            " - layer2.weight grad norm: 0.0019234783248975873\n",
            " - layer2.bias grad norm: 0.0006026802002452314\n",
            " - layer2_input.weight grad norm: 0.5193946957588196\n",
            " - layer2_input.bias grad norm: 0.0006026802002452314\n",
            " - layer3.weight grad norm: 0.0026726119685918093\n",
            " - layer3.bias grad norm: 0.000523975002579391\n",
            " - layer3_input.weight grad norm: 0.45679670572280884\n",
            " - layer3_input.bias grad norm: 0.000523975002579391\n",
            " - layer4.weight grad norm: 0.003132678335532546\n",
            " - layer4.bias grad norm: 0.0005690971156582236\n",
            " - layer4_input.weight grad norm: 0.48735493421554565\n",
            " - layer4_input.bias grad norm: 0.0005690971156582236\n",
            " - layer5.weight grad norm: 0.012569461949169636\n",
            " - layer5.bias grad norm: 0.00534319831058383\n",
            "Gradients at iteration 87:\n",
            " - layer1.weight grad norm: 0.5198538303375244\n",
            " - layer1.bias grad norm: 0.0006084120832383633\n",
            " - layer2.weight grad norm: 0.001894009648822248\n",
            " - layer2.bias grad norm: 0.0006344791618175805\n",
            " - layer2_input.weight grad norm: 0.5415814518928528\n",
            " - layer2_input.bias grad norm: 0.0006344791618175805\n",
            " - layer3.weight grad norm: 0.0026315608993172646\n",
            " - layer3.bias grad norm: 0.0005331219872459769\n",
            " - layer3_input.weight grad norm: 0.46317705512046814\n",
            " - layer3_input.bias grad norm: 0.0005331219872459769\n",
            " - layer4.weight grad norm: 0.0031785855535417795\n",
            " - layer4.bias grad norm: 0.0005459475796669722\n",
            " - layer4_input.weight grad norm: 0.47086572647094727\n",
            " - layer4_input.bias grad norm: 0.0005459475796669722\n",
            " - layer5.weight grad norm: 0.011926865205168724\n",
            " - layer5.bias grad norm: 0.0053766327910125256\n",
            "Gradients at iteration 88:\n",
            " - layer1.weight grad norm: 0.5097334384918213\n",
            " - layer1.bias grad norm: 0.0005864783888682723\n",
            " - layer2.weight grad norm: 0.001928470330312848\n",
            " - layer2.bias grad norm: 0.0005958997644484043\n",
            " - layer2_input.weight grad norm: 0.5156520009040833\n",
            " - layer2_input.bias grad norm: 0.0005958997644484043\n",
            " - layer3.weight grad norm: 0.002688127337023616\n",
            " - layer3.bias grad norm: 0.0005257989396341145\n",
            " - layer3_input.weight grad norm: 0.4603877067565918\n",
            " - layer3_input.bias grad norm: 0.0005257989396341145\n",
            " - layer4.weight grad norm: 0.0031785236205905676\n",
            " - layer4.bias grad norm: 0.0006028934149071574\n",
            " - layer4_input.weight grad norm: 0.5119536519050598\n",
            " - layer4_input.bias grad norm: 0.0006028934149071574\n",
            " - layer5.weight grad norm: 0.01299565564841032\n",
            " - layer5.bias grad norm: 0.005423437803983688\n",
            "Gradients at iteration 89:\n",
            " - layer1.weight grad norm: 0.5089992880821228\n",
            " - layer1.bias grad norm: 0.0005854053306393325\n",
            " - layer2.weight grad norm: 0.001949427300132811\n",
            " - layer2.bias grad norm: 0.0005724047077819705\n",
            " - layer2_input.weight grad norm: 0.4986972510814667\n",
            " - layer2_input.bias grad norm: 0.0005724047077819705\n",
            " - layer3.weight grad norm: 0.002699332544580102\n",
            " - layer3.bias grad norm: 0.0005985808675177395\n",
            " - layer3_input.weight grad norm: 0.5144714117050171\n",
            " - layer3_input.bias grad norm: 0.0005985808675177395\n",
            " - layer4.weight grad norm: 0.003233493771404028\n",
            " - layer4.bias grad norm: 0.0005442231195047498\n",
            " - layer4_input.weight grad norm: 0.4767523407936096\n",
            " - layer4_input.bias grad norm: 0.0005442231195047498\n",
            " - layer5.weight grad norm: 0.01388577651232481\n",
            " - layer5.bias grad norm: 0.00552205927670002\n",
            "Gradients at iteration 90:\n",
            " - layer1.weight grad norm: 0.5342012047767639\n",
            " - layer1.bias grad norm: 0.000624383392278105\n",
            " - layer2.weight grad norm: 0.001910566701553762\n",
            " - layer2.bias grad norm: 0.0006107534281909466\n",
            " - layer2_input.weight grad norm: 0.5223090648651123\n",
            " - layer2_input.bias grad norm: 0.0006107534281909466\n",
            " - layer3.weight grad norm: 0.0026728555094450712\n",
            " - layer3.bias grad norm: 0.0006031226366758347\n",
            " - layer3_input.weight grad norm: 0.5130991339683533\n",
            " - layer3_input.bias grad norm: 0.0006031226366758347\n",
            " - layer4.weight grad norm: 0.0031638420186936855\n",
            " - layer4.bias grad norm: 0.00047832433483563364\n",
            " - layer4_input.weight grad norm: 0.4223291873931885\n",
            " - layer4_input.bias grad norm: 0.00047832433483563364\n",
            " - layer5.weight grad norm: 0.011723347008228302\n",
            " - layer5.bias grad norm: 0.0053933714516460896\n",
            "Gradients at iteration 91:\n",
            " - layer1.weight grad norm: 0.47565457224845886\n",
            " - layer1.bias grad norm: 0.0005354029126465321\n",
            " - layer2.weight grad norm: 0.0018931225640699267\n",
            " - layer2.bias grad norm: 0.0006018387502990663\n",
            " - layer2_input.weight grad norm: 0.5206130146980286\n",
            " - layer2_input.bias grad norm: 0.0006018387502990663\n",
            " - layer3.weight grad norm: 0.0027285018004477024\n",
            " - layer3.bias grad norm: 0.0006039301515556872\n",
            " - layer3_input.weight grad norm: 0.5224199891090393\n",
            " - layer3_input.bias grad norm: 0.0006039301515556872\n",
            " - layer4.weight grad norm: 0.003176894038915634\n",
            " - layer4.bias grad norm: 0.0005522050196304917\n",
            " - layer4_input.weight grad norm: 0.47916555404663086\n",
            " - layer4_input.bias grad norm: 0.0005522050196304917\n",
            " - layer5.weight grad norm: 0.011808390729129314\n",
            " - layer5.bias grad norm: 0.00543700996786356\n",
            "Gradients at iteration 92:\n",
            " - layer1.weight grad norm: 0.5499149560928345\n",
            " - layer1.bias grad norm: 0.0006421336438506842\n",
            " - layer2.weight grad norm: 0.0018738420912995934\n",
            " - layer2.bias grad norm: 0.0005435123457573354\n",
            " - layer2_input.weight grad norm: 0.4779241383075714\n",
            " - layer2_input.bias grad norm: 0.0005435123457573354\n",
            " - layer3.weight grad norm: 0.002583984285593033\n",
            " - layer3.bias grad norm: 0.0005615276168100536\n",
            " - layer3_input.weight grad norm: 0.48618900775909424\n",
            " - layer3_input.bias grad norm: 0.0005615276168100536\n",
            " - layer4.weight grad norm: 0.003131217323243618\n",
            " - layer4.bias grad norm: 0.0005576544790528715\n",
            " - layer4_input.weight grad norm: 0.48229795694351196\n",
            " - layer4_input.bias grad norm: 0.0005576544790528715\n",
            " - layer5.weight grad norm: 0.011864038184285164\n",
            " - layer5.bias grad norm: 0.005281949415802956\n",
            "Gradients at iteration 93:\n",
            " - layer1.weight grad norm: 0.517902672290802\n",
            " - layer1.bias grad norm: 0.0005951427738182247\n",
            " - layer2.weight grad norm: 0.001930603408254683\n",
            " - layer2.bias grad norm: 0.0006420791614800692\n",
            " - layer2_input.weight grad norm: 0.5497629046440125\n",
            " - layer2_input.bias grad norm: 0.0006420791614800692\n",
            " - layer3.weight grad norm: 0.002656578551977873\n",
            " - layer3.bias grad norm: 0.0005639380542561412\n",
            " - layer3_input.weight grad norm: 0.4924919009208679\n",
            " - layer3_input.bias grad norm: 0.0005639380542561412\n",
            " - layer4.weight grad norm: 0.003137646708637476\n",
            " - layer4.bias grad norm: 0.00048769061686471105\n",
            " - layer4_input.weight grad norm: 0.432191401720047\n",
            " - layer4_input.bias grad norm: 0.00048769061686471105\n",
            " - layer5.weight grad norm: 0.01217527687549591\n",
            " - layer5.bias grad norm: 0.005358624272048473\n",
            "Gradients at iteration 94:\n",
            " - layer1.weight grad norm: 0.5275527834892273\n",
            " - layer1.bias grad norm: 0.0006131573463790119\n",
            " - layer2.weight grad norm: 0.0019701276905834675\n",
            " - layer2.bias grad norm: 0.0005876501090824604\n",
            " - layer2_input.weight grad norm: 0.5053507685661316\n",
            " - layer2_input.bias grad norm: 0.0005876501090824604\n",
            " - layer3.weight grad norm: 0.002709384076297283\n",
            " - layer3.bias grad norm: 0.0005991726065985858\n",
            " - layer3_input.weight grad norm: 0.5126291513442993\n",
            " - layer3_input.bias grad norm: 0.0005991726065985858\n",
            " - layer4.weight grad norm: 0.0032763155177235603\n",
            " - layer4.bias grad norm: 0.0005152262165211141\n",
            " - layer4_input.weight grad norm: 0.45086947083473206\n",
            " - layer4_input.bias grad norm: 0.0005152262165211141\n",
            " - layer5.weight grad norm: 0.013483896851539612\n",
            " - layer5.bias grad norm: 0.0055397069081664085\n",
            "Gradients at iteration 95:\n",
            " - layer1.weight grad norm: 0.5057274699211121\n",
            " - layer1.bias grad norm: 0.0005868184962309897\n",
            " - layer2.weight grad norm: 0.0019244650611653924\n",
            " - layer2.bias grad norm: 0.0006008247728459537\n",
            " - layer2_input.weight grad norm: 0.5103427171707153\n",
            " - layer2_input.bias grad norm: 0.0006008247728459537\n",
            " - layer3.weight grad norm: 0.0026544081047177315\n",
            " - layer3.bias grad norm: 0.0006061902968212962\n",
            " - layer3_input.weight grad norm: 0.5159534215927124\n",
            " - layer3_input.bias grad norm: 0.0006061902968212962\n",
            " - layer4.weight grad norm: 0.0032069419976323843\n",
            " - layer4.bias grad norm: 0.0005417944630607963\n",
            " - layer4_input.weight grad norm: 0.46623915433883667\n",
            " - layer4_input.bias grad norm: 0.0005417944630607963\n",
            " - layer5.weight grad norm: 0.012266818434000015\n",
            " - layer5.bias grad norm: 0.0054039666429162025\n",
            "Gradients at iteration 96:\n",
            " - layer1.weight grad norm: 0.5165755152702332\n",
            " - layer1.bias grad norm: 0.0006070025847293437\n",
            " - layer2.weight grad norm: 0.0018787157023325562\n",
            " - layer2.bias grad norm: 0.0005869629676453769\n",
            " - layer2_input.weight grad norm: 0.5005443096160889\n",
            " - layer2_input.bias grad norm: 0.0005869629676453769\n",
            " - layer3.weight grad norm: 0.0025934430304914713\n",
            " - layer3.bias grad norm: 0.0005537438555620611\n",
            " - layer3_input.weight grad norm: 0.4719148278236389\n",
            " - layer3_input.bias grad norm: 0.0005537438555620611\n",
            " - layer4.weight grad norm: 0.0031013197731226683\n",
            " - layer4.bias grad norm: 0.0006015259423293173\n",
            " - layer4_input.weight grad norm: 0.5096054673194885\n",
            " - layer4_input.bias grad norm: 0.0006015259423293173\n",
            " - layer5.weight grad norm: 0.012383958324790001\n",
            " - layer5.bias grad norm: 0.005303173791617155\n",
            "Gradients at iteration 97:\n",
            " - layer1.weight grad norm: 0.5200805068016052\n",
            " - layer1.bias grad norm: 0.0006037131533958018\n",
            " - layer2.weight grad norm: 0.0019300595158711076\n",
            " - layer2.bias grad norm: 0.0005608173087239265\n",
            " - layer2_input.weight grad norm: 0.4874972403049469\n",
            " - layer2_input.bias grad norm: 0.0005608173087239265\n",
            " - layer3.weight grad norm: 0.002680463483557105\n",
            " - layer3.bias grad norm: 0.000586431531701237\n",
            " - layer3_input.weight grad norm: 0.5019124150276184\n",
            " - layer3_input.bias grad norm: 0.000586431531701237\n",
            " - layer4.weight grad norm: 0.0031536526512354612\n",
            " - layer4.bias grad norm: 0.0005693769198842347\n",
            " - layer4_input.weight grad norm: 0.48961153626441956\n",
            " - layer4_input.bias grad norm: 0.0005693769198842347\n",
            " - layer5.weight grad norm: 0.01324242353439331\n",
            " - layer5.bias grad norm: 0.0053712367080152035\n",
            "Gradients at iteration 98:\n",
            " - layer1.weight grad norm: 0.5240671038627625\n",
            " - layer1.bias grad norm: 0.0006063368055038154\n",
            " - layer2.weight grad norm: 0.0019115961622446775\n",
            " - layer2.bias grad norm: 0.0005722740315832198\n",
            " - layer2_input.weight grad norm: 0.4965016841888428\n",
            " - layer2_input.bias grad norm: 0.0005722740315832198\n",
            " - layer3.weight grad norm: 0.002688681473955512\n",
            " - layer3.bias grad norm: 0.0005469508469104767\n",
            " - layer3_input.weight grad norm: 0.4777003526687622\n",
            " - layer3_input.bias grad norm: 0.0005469508469104767\n",
            " - layer4.weight grad norm: 0.0031416176352649927\n",
            " - layer4.bias grad norm: 0.0005787195404991508\n",
            " - layer4_input.weight grad norm: 0.50044184923172\n",
            " - layer4_input.bias grad norm: 0.0005787195404991508\n",
            " - layer5.weight grad norm: 0.012182380072772503\n",
            " - layer5.bias grad norm: 0.005355668254196644\n",
            "Gradients at iteration 99:\n",
            " - layer1.weight grad norm: 0.5415003895759583\n",
            " - layer1.bias grad norm: 0.0006226094556041062\n",
            " - layer2.weight grad norm: 0.0019713391084223986\n",
            " - layer2.bias grad norm: 0.0005779037019237876\n",
            " - layer2_input.weight grad norm: 0.5018795728683472\n",
            " - layer2_input.bias grad norm: 0.0005779037019237876\n",
            " - layer3.weight grad norm: 0.002750305226072669\n",
            " - layer3.bias grad norm: 0.0005161829758435488\n",
            " - layer3_input.weight grad norm: 0.4560879170894623\n",
            " - layer3_input.bias grad norm: 0.0005161829758435488\n",
            " - layer4.weight grad norm: 0.003273001639172435\n",
            " - layer4.bias grad norm: 0.0005723285139538348\n",
            " - layer4_input.weight grad norm: 0.49663910269737244\n",
            " - layer4_input.bias grad norm: 0.0005723285139538348\n",
            " - layer5.weight grad norm: 0.013119826093316078\n",
            " - layer5.bias grad norm: 0.005579660646617413\n",
            "Gradients at iteration 100:\n",
            " - layer1.weight grad norm: 0.5037904381752014\n",
            " - layer1.bias grad norm: 0.000586073671001941\n",
            " - layer2.weight grad norm: 0.0018951011588796973\n",
            " - layer2.bias grad norm: 0.0005926269222982228\n",
            " - layer2_input.weight grad norm: 0.5082477331161499\n",
            " - layer2_input.bias grad norm: 0.0005926269222982228\n",
            " - layer3.weight grad norm: 0.0025832769460976124\n",
            " - layer3.bias grad norm: 0.0006200032657943666\n",
            " - layer3_input.weight grad norm: 0.5269548296928406\n",
            " - layer3_input.bias grad norm: 0.0006200032657943666\n",
            " - layer4.weight grad norm: 0.0030681814532727003\n",
            " - layer4.bias grad norm: 0.0005287870881147683\n",
            " - layer4_input.weight grad norm: 0.4582737982273102\n",
            " - layer4_input.bias grad norm: 0.0005287870881147683\n",
            " - layer5.weight grad norm: 0.011533328332006931\n",
            " - layer5.bias grad norm: 0.0052909813821315765\n",
            "It: 100, Loss: 5.337e+13, Y0: -0.003, Time: 1.66, Learning Rate: 1.000e-03\n",
            "Gradients at iteration 101:\n",
            " - layer1.weight grad norm: 0.5060656070709229\n",
            " - layer1.bias grad norm: 0.0005801169318147004\n",
            " - layer2.weight grad norm: 0.0019285078160464764\n",
            " - layer2.bias grad norm: 0.000574963225517422\n",
            " - layer2_input.weight grad norm: 0.5028654336929321\n",
            " - layer2_input.bias grad norm: 0.000574963225517422\n",
            " - layer3.weight grad norm: 0.002694192575290799\n",
            " - layer3.bias grad norm: 0.0005744920927099884\n",
            " - layer3_input.weight grad norm: 0.49605193734169006\n",
            " - layer3_input.bias grad norm: 0.0005744920927099884\n",
            " - layer4.weight grad norm: 0.003211937379091978\n",
            " - layer4.bias grad norm: 0.0005709812394343317\n",
            " - layer4_input.weight grad norm: 0.4947461187839508\n",
            " - layer4_input.bias grad norm: 0.0005709812394343317\n",
            " - layer5.weight grad norm: 0.011391733773052692\n",
            " - layer5.bias grad norm: 0.00540288258343935\n",
            "Gradients at iteration 102:\n",
            " - layer1.weight grad norm: 0.4970504343509674\n",
            " - layer1.bias grad norm: 0.0005745197995565832\n",
            " - layer2.weight grad norm: 0.001990277785807848\n",
            " - layer2.bias grad norm: 0.0005360701470635831\n",
            " - layer2_input.weight grad norm: 0.46990904211997986\n",
            " - layer2_input.bias grad norm: 0.0005360701470635831\n",
            " - layer3.weight grad norm: 0.002751952735707164\n",
            " - layer3.bias grad norm: 0.000631586299277842\n",
            " - layer3_input.weight grad norm: 0.5372762084007263\n",
            " - layer3_input.bias grad norm: 0.000631586299277842\n",
            " - layer4.weight grad norm: 0.0032391753047704697\n",
            " - layer4.bias grad norm: 0.000575627782382071\n",
            " - layer4_input.weight grad norm: 0.4932236671447754\n",
            " - layer4_input.bias grad norm: 0.000575627782382071\n",
            " - layer5.weight grad norm: 0.011631361208856106\n",
            " - layer5.bias grad norm: 0.005599095020443201\n",
            "Gradients at iteration 103:\n",
            " - layer1.weight grad norm: 0.5471283197402954\n",
            " - layer1.bias grad norm: 0.0006302928668446839\n",
            " - layer2.weight grad norm: 0.0019068678375333548\n",
            " - layer2.bias grad norm: 0.0005897856899537146\n",
            " - layer2_input.weight grad norm: 0.5125096440315247\n",
            " - layer2_input.bias grad norm: 0.0005897856899537146\n",
            " - layer3.weight grad norm: 0.002629735041409731\n",
            " - layer3.bias grad norm: 0.0005476930527947843\n",
            " - layer3_input.weight grad norm: 0.4819798171520233\n",
            " - layer3_input.bias grad norm: 0.0005476930527947843\n",
            " - layer4.weight grad norm: 0.0031470146495848894\n",
            " - layer4.bias grad norm: 0.0005127287586219609\n",
            " - layer4_input.weight grad norm: 0.4532983601093292\n",
            " - layer4_input.bias grad norm: 0.0005127287586219609\n",
            " - layer5.weight grad norm: 0.012201757170259953\n",
            " - layer5.bias grad norm: 0.005384869873523712\n",
            "Gradients at iteration 104:\n",
            " - layer1.weight grad norm: 0.5202395915985107\n",
            " - layer1.bias grad norm: 0.0006069210940040648\n",
            " - layer2.weight grad norm: 0.0019038845784962177\n",
            " - layer2.bias grad norm: 0.0005670393002219498\n",
            " - layer2_input.weight grad norm: 0.49325791001319885\n",
            " - layer2_input.bias grad norm: 0.0005670393002219498\n",
            " - layer3.weight grad norm: 0.002660344121977687\n",
            " - layer3.bias grad norm: 0.0005314773879945278\n",
            " - layer3_input.weight grad norm: 0.4633241891860962\n",
            " - layer3_input.bias grad norm: 0.0005314773879945278\n",
            " - layer4.weight grad norm: 0.0031545530073344707\n",
            " - layer4.bias grad norm: 0.0006081152823753655\n",
            " - layer4_input.weight grad norm: 0.5207337141036987\n",
            " - layer4_input.bias grad norm: 0.0006081152823753655\n",
            " - layer5.weight grad norm: 0.01271958276629448\n",
            " - layer5.bias grad norm: 0.005440711509436369\n",
            "Gradients at iteration 105:\n",
            " - layer1.weight grad norm: 0.5206552147865295\n",
            " - layer1.bias grad norm: 0.0006001454312354326\n",
            " - layer2.weight grad norm: 0.0019580430816859007\n",
            " - layer2.bias grad norm: 0.0005819323123432696\n",
            " - layer2_input.weight grad norm: 0.5061794519424438\n",
            " - layer2_input.bias grad norm: 0.0005819323123432696\n",
            " - layer3.weight grad norm: 0.0027217145543545485\n",
            " - layer3.bias grad norm: 0.0005677914596162736\n",
            " - layer3_input.weight grad norm: 0.4929123520851135\n",
            " - layer3_input.bias grad norm: 0.0005677914596162736\n",
            " - layer4.weight grad norm: 0.003238464007154107\n",
            " - layer4.bias grad norm: 0.0005507796886377037\n",
            " - layer4_input.weight grad norm: 0.4790852963924408\n",
            " - layer4_input.bias grad norm: 0.0005507796886377037\n",
            " - layer5.weight grad norm: 0.012679953128099442\n",
            " - layer5.bias grad norm: 0.005515985190868378\n",
            "Gradients at iteration 106:\n",
            " - layer1.weight grad norm: 0.49916160106658936\n",
            " - layer1.bias grad norm: 0.0005643313052132726\n",
            " - layer2.weight grad norm: 0.0018896990222856402\n",
            " - layer2.bias grad norm: 0.0005760765634477139\n",
            " - layer2_input.weight grad norm: 0.502704381942749\n",
            " - layer2_input.bias grad norm: 0.0005760765634477139\n",
            " - layer3.weight grad norm: 0.0026250043883919716\n",
            " - layer3.bias grad norm: 0.0005853753536939621\n",
            " - layer3_input.weight grad norm: 0.5066577196121216\n",
            " - layer3_input.bias grad norm: 0.0005853753536939621\n",
            " - layer4.weight grad norm: 0.0031590647995471954\n",
            " - layer4.bias grad norm: 0.0005640691379085183\n",
            " - layer4_input.weight grad norm: 0.4911329448223114\n",
            " - layer4_input.bias grad norm: 0.0005640691379085183\n",
            " - layer5.weight grad norm: 0.012681995518505573\n",
            " - layer5.bias grad norm: 0.005365362856537104\n",
            "Gradients at iteration 107:\n",
            " - layer1.weight grad norm: 0.5519347190856934\n",
            " - layer1.bias grad norm: 0.0006517759175039828\n",
            " - layer2.weight grad norm: 0.0019025398651137948\n",
            " - layer2.bias grad norm: 0.0005725103546865284\n",
            " - layer2_input.weight grad norm: 0.4951724410057068\n",
            " - layer2_input.bias grad norm: 0.0005725103546865284\n",
            " - layer3.weight grad norm: 0.002668320666998625\n",
            " - layer3.bias grad norm: 0.0005707645905204117\n",
            " - layer3_input.weight grad norm: 0.4869997501373291\n",
            " - layer3_input.bias grad norm: 0.0005707645905204117\n",
            " - layer4.weight grad norm: 0.003097028471529484\n",
            " - layer4.bias grad norm: 0.0005327464896254241\n",
            " - layer4_input.weight grad norm: 0.46130454540252686\n",
            " - layer4_input.bias grad norm: 0.0005327464896254241\n",
            " - layer5.weight grad norm: 0.012274227105081081\n",
            " - layer5.bias grad norm: 0.005327743012458086\n",
            "Gradients at iteration 108:\n",
            " - layer1.weight grad norm: 0.5016009211540222\n",
            " - layer1.bias grad norm: 0.0005707439268007874\n",
            " - layer2.weight grad norm: 0.0020867723505944014\n",
            " - layer2.bias grad norm: 0.0005359005881473422\n",
            " - layer2_input.weight grad norm: 0.4767761826515198\n",
            " - layer2_input.bias grad norm: 0.0005359005881473422\n",
            " - layer3.weight grad norm: 0.002802439033985138\n",
            " - layer3.bias grad norm: 0.0006148417596705258\n",
            " - layer3_input.weight grad norm: 0.5288575291633606\n",
            " - layer3_input.bias grad norm: 0.0006148417596705258\n",
            " - layer4.weight grad norm: 0.0034149480052292347\n",
            " - layer4.bias grad norm: 0.0005651734536513686\n",
            " - layer4_input.weight grad norm: 0.49106693267822266\n",
            " - layer4_input.bias grad norm: 0.0005651734536513686\n",
            " - layer5.weight grad norm: 0.01356875617057085\n",
            " - layer5.bias grad norm: 0.005798725411295891\n",
            "Gradients at iteration 109:\n",
            " - layer1.weight grad norm: 0.5413846969604492\n",
            " - layer1.bias grad norm: 0.0006378321559168398\n",
            " - layer2.weight grad norm: 0.0018815061775967479\n",
            " - layer2.bias grad norm: 0.0006130294059403241\n",
            " - layer2_input.weight grad norm: 0.5252869725227356\n",
            " - layer2_input.bias grad norm: 0.0006130294059403241\n",
            " - layer3.weight grad norm: 0.0025503539945930243\n",
            " - layer3.bias grad norm: 0.0005523557192645967\n",
            " - layer3_input.weight grad norm: 0.47505488991737366\n",
            " - layer3_input.bias grad norm: 0.0005523557192645967\n",
            " - layer4.weight grad norm: 0.003049816470593214\n",
            " - layer4.bias grad norm: 0.0005191980162635446\n",
            " - layer4_input.weight grad norm: 0.4528927206993103\n",
            " - layer4_input.bias grad norm: 0.0005191980162635446\n",
            " - layer5.weight grad norm: 0.011748123914003372\n",
            " - layer5.bias grad norm: 0.005248514004051685\n",
            "Gradients at iteration 110:\n",
            " - layer1.weight grad norm: 0.5208496451377869\n",
            " - layer1.bias grad norm: 0.0006125279469415545\n",
            " - layer2.weight grad norm: 0.001947340671904385\n",
            " - layer2.bias grad norm: 0.0005847039283253253\n",
            " - layer2_input.weight grad norm: 0.4960443079471588\n",
            " - layer2_input.bias grad norm: 0.0005847039283253253\n",
            " - layer3.weight grad norm: 0.002705230610445142\n",
            " - layer3.bias grad norm: 0.0005936677916906774\n",
            " - layer3_input.weight grad norm: 0.5046412348747253\n",
            " - layer3_input.bias grad norm: 0.0005936677916906774\n",
            " - layer4.weight grad norm: 0.003196016186848283\n",
            " - layer4.bias grad norm: 0.0005643491167575121\n",
            " - layer4_input.weight grad norm: 0.47726020216941833\n",
            " - layer4_input.bias grad norm: 0.0005643491167575121\n",
            " - layer5.weight grad norm: 0.012721879407763481\n",
            " - layer5.bias grad norm: 0.005479652434587479\n",
            "Gradients at iteration 111:\n",
            " - layer1.weight grad norm: 0.4958004355430603\n",
            " - layer1.bias grad norm: 0.0005700726760551333\n",
            " - layer2.weight grad norm: 0.0019048020476475358\n",
            " - layer2.bias grad norm: 0.0005862395628355443\n",
            " - layer2_input.weight grad norm: 0.5067017674446106\n",
            " - layer2_input.bias grad norm: 0.0005862395628355443\n",
            " - layer3.weight grad norm: 0.002640248043462634\n",
            " - layer3.bias grad norm: 0.0005546470056287944\n",
            " - layer3_input.weight grad norm: 0.48252859711647034\n",
            " - layer3_input.bias grad norm: 0.0005546470056287944\n",
            " - layer4.weight grad norm: 0.0031690113246440887\n",
            " - layer4.bias grad norm: 0.0006024061003699899\n",
            " - layer4_input.weight grad norm: 0.5142147541046143\n",
            " - layer4_input.bias grad norm: 0.0006024061003699899\n",
            " - layer5.weight grad norm: 0.011537063866853714\n",
            " - layer5.bias grad norm: 0.0053422292694449425\n",
            "Gradients at iteration 112:\n",
            " - layer1.weight grad norm: 0.5079026818275452\n",
            " - layer1.bias grad norm: 0.0005890382453799248\n",
            " - layer2.weight grad norm: 0.0019678869284689426\n",
            " - layer2.bias grad norm: 0.000602636078838259\n",
            " - layer2_input.weight grad norm: 0.5131493210792542\n",
            " - layer2_input.bias grad norm: 0.000602636078838259\n",
            " - layer3.weight grad norm: 0.0027246838435530663\n",
            " - layer3.bias grad norm: 0.0006064787739887834\n",
            " - layer3_input.weight grad norm: 0.5146728754043579\n",
            " - layer3_input.bias grad norm: 0.0006064787739887834\n",
            " - layer4.weight grad norm: 0.003300586249679327\n",
            " - layer4.bias grad norm: 0.0005368826095946133\n",
            " - layer4_input.weight grad norm: 0.46219027042388916\n",
            " - layer4_input.bias grad norm: 0.0005368826095946133\n",
            " - layer5.weight grad norm: 0.012225145474076271\n",
            " - layer5.bias grad norm: 0.005538554396480322\n",
            "Gradients at iteration 113:\n",
            " - layer1.weight grad norm: 0.5119636058807373\n",
            " - layer1.bias grad norm: 0.00059718411648646\n",
            " - layer2.weight grad norm: 0.001786232809536159\n",
            " - layer2.bias grad norm: 0.0005998516571708024\n",
            " - layer2_input.weight grad norm: 0.5165974497795105\n",
            " - layer2_input.bias grad norm: 0.0005998516571708024\n",
            " - layer3.weight grad norm: 0.002468594117090106\n",
            " - layer3.bias grad norm: 0.000602383108343929\n",
            " - layer3_input.weight grad norm: 0.5167509913444519\n",
            " - layer3_input.bias grad norm: 0.000602383108343929\n",
            " - layer4.weight grad norm: 0.002955299336463213\n",
            " - layer4.bias grad norm: 0.0005212025716900826\n",
            " - layer4_input.weight grad norm: 0.4514644742012024\n",
            " - layer4_input.bias grad norm: 0.0005212025716900826\n",
            " - layer5.weight grad norm: 0.011060807853937149\n",
            " - layer5.bias grad norm: 0.005073464009910822\n",
            "Gradients at iteration 114:\n",
            " - layer1.weight grad norm: 0.5122619271278381\n",
            " - layer1.bias grad norm: 0.0005886688595637679\n",
            " - layer2.weight grad norm: 0.0018239946803078055\n",
            " - layer2.bias grad norm: 0.0005853874026797712\n",
            " - layer2_input.weight grad norm: 0.5123413801193237\n",
            " - layer2_input.bias grad norm: 0.0005853874026797712\n",
            " - layer3.weight grad norm: 0.0025629480369389057\n",
            " - layer3.bias grad norm: 0.0005803803796879947\n",
            " - layer3_input.weight grad norm: 0.5059699416160583\n",
            " - layer3_input.bias grad norm: 0.0005803803796879947\n",
            " - layer4.weight grad norm: 0.003028117585927248\n",
            " - layer4.bias grad norm: 0.0005298542091622949\n",
            " - layer4_input.weight grad norm: 0.4678514301776886\n",
            " - layer4_input.bias grad norm: 0.0005298542091622949\n",
            " - layer5.weight grad norm: 0.01246278639882803\n",
            " - layer5.bias grad norm: 0.005168145056813955\n",
            "Gradients at iteration 115:\n",
            " - layer1.weight grad norm: 0.5103591084480286\n",
            " - layer1.bias grad norm: 0.0005836411728523672\n",
            " - layer2.weight grad norm: 0.001995279686525464\n",
            " - layer2.bias grad norm: 0.0005819543148390949\n",
            " - layer2_input.weight grad norm: 0.5077999830245972\n",
            " - layer2_input.bias grad norm: 0.0005819543148390949\n",
            " - layer3.weight grad norm: 0.002785076154395938\n",
            " - layer3.bias grad norm: 0.0005457742372527719\n",
            " - layer3_input.weight grad norm: 0.48227235674858093\n",
            " - layer3_input.bias grad norm: 0.0005457742372527719\n",
            " - layer4.weight grad norm: 0.0033293496817350388\n",
            " - layer4.bias grad norm: 0.0005712911952286959\n",
            " - layer4_input.weight grad norm: 0.4988749623298645\n",
            " - layer4_input.bias grad norm: 0.0005712911952286959\n",
            " - layer5.weight grad norm: 0.012371744960546494\n",
            " - layer5.bias grad norm: 0.005631101783365011\n",
            "Gradients at iteration 116:\n",
            " - layer1.weight grad norm: 0.508194625377655\n",
            " - layer1.bias grad norm: 0.0005815618787892163\n",
            " - layer2.weight grad norm: 0.0019803259056061506\n",
            " - layer2.bias grad norm: 0.0005964368465356529\n",
            " - layer2_input.weight grad norm: 0.5136151313781738\n",
            " - layer2_input.bias grad norm: 0.0005964368465356529\n",
            " - layer3.weight grad norm: 0.0027363623958081007\n",
            " - layer3.bias grad norm: 0.0005898549570702016\n",
            " - layer3_input.weight grad norm: 0.5140929222106934\n",
            " - layer3_input.bias grad norm: 0.0005898549570702016\n",
            " - layer4.weight grad norm: 0.0032714104745537043\n",
            " - layer4.bias grad norm: 0.000530528835952282\n",
            " - layer4_input.weight grad norm: 0.4619922339916229\n",
            " - layer4_input.bias grad norm: 0.000530528835952282\n",
            " - layer5.weight grad norm: 0.012391939759254456\n",
            " - layer5.bias grad norm: 0.005585935432463884\n",
            "Gradients at iteration 117:\n",
            " - layer1.weight grad norm: 0.5267964601516724\n",
            " - layer1.bias grad norm: 0.0006173417204990983\n",
            " - layer2.weight grad norm: 0.0018702585948631167\n",
            " - layer2.bias grad norm: 0.000532217905856669\n",
            " - layer2_input.weight grad norm: 0.46878141164779663\n",
            " - layer2_input.bias grad norm: 0.000532217905856669\n",
            " - layer3.weight grad norm: 0.0025725960731506348\n",
            " - layer3.bias grad norm: 0.0006198720657266676\n",
            " - layer3_input.weight grad norm: 0.5273655652999878\n",
            " - layer3_input.bias grad norm: 0.0006198720657266676\n",
            " - layer4.weight grad norm: 0.0030726855620741844\n",
            " - layer4.bias grad norm: 0.0005455872160382569\n",
            " - layer4_input.weight grad norm: 0.4737265706062317\n",
            " - layer4_input.bias grad norm: 0.0005455872160382569\n",
            " - layer5.weight grad norm: 0.012181944213807583\n",
            " - layer5.bias grad norm: 0.005289293359965086\n",
            "Gradients at iteration 118:\n",
            " - layer1.weight grad norm: 0.49887701869010925\n",
            " - layer1.bias grad norm: 0.0005647316575050354\n",
            " - layer2.weight grad norm: 0.0019300668500363827\n",
            " - layer2.bias grad norm: 0.0005727348034270108\n",
            " - layer2_input.weight grad norm: 0.5022246241569519\n",
            " - layer2_input.bias grad norm: 0.0005727348034270108\n",
            " - layer3.weight grad norm: 0.0026700191665440798\n",
            " - layer3.bias grad norm: 0.0005531120696105063\n",
            " - layer3_input.weight grad norm: 0.48456108570098877\n",
            " - layer3_input.bias grad norm: 0.0005531120696105063\n",
            " - layer4.weight grad norm: 0.0031747804023325443\n",
            " - layer4.bias grad norm: 0.0005914178909733891\n",
            " - layer4_input.weight grad norm: 0.5136998295783997\n",
            " - layer4_input.bias grad norm: 0.0005914178909733891\n",
            " - layer5.weight grad norm: 0.012352257035672665\n",
            " - layer5.bias grad norm: 0.005414431449025869\n",
            "Gradients at iteration 119:\n",
            " - layer1.weight grad norm: 0.49256640672683716\n",
            " - layer1.bias grad norm: 0.0005693445564247668\n",
            " - layer2.weight grad norm: 0.0020045689307153225\n",
            " - layer2.bias grad norm: 0.000622230174485594\n",
            " - layer2_input.weight grad norm: 0.5303128361701965\n",
            " - layer2_input.bias grad norm: 0.000622230174485594\n",
            " - layer3.weight grad norm: 0.002764009404927492\n",
            " - layer3.bias grad norm: 0.0005764655652455986\n",
            " - layer3_input.weight grad norm: 0.4951270818710327\n",
            " - layer3_input.bias grad norm: 0.0005764655652455986\n",
            " - layer4.weight grad norm: 0.0033066857140511274\n",
            " - layer4.bias grad norm: 0.0005596962873823941\n",
            " - layer4_input.weight grad norm: 0.48037633299827576\n",
            " - layer4_input.bias grad norm: 0.0005596962873823941\n",
            " - layer5.weight grad norm: 0.013346255756914616\n",
            " - layer5.bias grad norm: 0.005609147250652313\n",
            "Gradients at iteration 120:\n",
            " - layer1.weight grad norm: 0.5084177255630493\n",
            " - layer1.bias grad norm: 0.0005844260449521244\n",
            " - layer2.weight grad norm: 0.0019931651186197996\n",
            " - layer2.bias grad norm: 0.0005837102071382105\n",
            " - layer2_input.weight grad norm: 0.5024540424346924\n",
            " - layer2_input.bias grad norm: 0.0005837102071382105\n",
            " - layer3.weight grad norm: 0.002731477143242955\n",
            " - layer3.bias grad norm: 0.000619858386926353\n",
            " - layer3_input.weight grad norm: 0.5298109650611877\n",
            " - layer3_input.bias grad norm: 0.000619858386926353\n",
            " - layer4.weight grad norm: 0.0031643181573599577\n",
            " - layer4.bias grad norm: 0.0005283126374706626\n",
            " - layer4_input.weight grad norm: 0.4562244713306427\n",
            " - layer4_input.bias grad norm: 0.0005283126374706626\n",
            " - layer5.weight grad norm: 0.012535844929516315\n",
            " - layer5.bias grad norm: 0.005488510709255934\n",
            "Gradients at iteration 121:\n",
            " - layer1.weight grad norm: 0.4929419159889221\n",
            " - layer1.bias grad norm: 0.0005681198090314865\n",
            " - layer2.weight grad norm: 0.001960308291018009\n",
            " - layer2.bias grad norm: 0.0006104261847212911\n",
            " - layer2_input.weight grad norm: 0.5235298871994019\n",
            " - layer2_input.bias grad norm: 0.0006104261847212911\n",
            " - layer3.weight grad norm: 0.002691502682864666\n",
            " - layer3.bias grad norm: 0.0005466609727591276\n",
            " - layer3_input.weight grad norm: 0.47499388456344604\n",
            " - layer3_input.bias grad norm: 0.0005466609727591276\n",
            " - layer4.weight grad norm: 0.0032376705203205347\n",
            " - layer4.bias grad norm: 0.0005939146503806114\n",
            " - layer4_input.weight grad norm: 0.5070605278015137\n",
            " - layer4_input.bias grad norm: 0.0005939146503806114\n",
            " - layer5.weight grad norm: 0.011876510456204414\n",
            " - layer5.bias grad norm: 0.005506488960236311\n",
            "Gradients at iteration 122:\n",
            " - layer1.weight grad norm: 0.5405510067939758\n",
            " - layer1.bias grad norm: 0.0006348650204017758\n",
            " - layer2.weight grad norm: 0.0018740007653832436\n",
            " - layer2.bias grad norm: 0.0005477690137922764\n",
            " - layer2_input.weight grad norm: 0.47649863362312317\n",
            " - layer2_input.bias grad norm: 0.0005477690137922764\n",
            " - layer3.weight grad norm: 0.0026288852095603943\n",
            " - layer3.bias grad norm: 0.000555786828044802\n",
            " - layer3_input.weight grad norm: 0.4787522256374359\n",
            " - layer3_input.bias grad norm: 0.000555786828044802\n",
            " - layer4.weight grad norm: 0.003128871787339449\n",
            " - layer4.bias grad norm: 0.0005922436248511076\n",
            " - layer4_input.weight grad norm: 0.5013718605041504\n",
            " - layer4_input.bias grad norm: 0.0005922436248511076\n",
            " - layer5.weight grad norm: 0.011209606193006039\n",
            " - layer5.bias grad norm: 0.005287282634526491\n",
            "Gradients at iteration 123:\n",
            " - layer1.weight grad norm: 0.5024845600128174\n",
            " - layer1.bias grad norm: 0.0005784145323559642\n",
            " - layer2.weight grad norm: 0.001955948304384947\n",
            " - layer2.bias grad norm: 0.0005905069992877543\n",
            " - layer2_input.weight grad norm: 0.5149104595184326\n",
            " - layer2_input.bias grad norm: 0.0005905069992877543\n",
            " - layer3.weight grad norm: 0.0027131568640470505\n",
            " - layer3.bias grad norm: 0.0005884316633455455\n",
            " - layer3_input.weight grad norm: 0.5091547966003418\n",
            " - layer3_input.bias grad norm: 0.0005884316633455455\n",
            " - layer4.weight grad norm: 0.0031955516897141933\n",
            " - layer4.bias grad norm: 0.0005410787998698652\n",
            " - layer4_input.weight grad norm: 0.47216761112213135\n",
            " - layer4_input.bias grad norm: 0.0005410787998698652\n",
            " - layer5.weight grad norm: 0.011903327889740467\n",
            " - layer5.bias grad norm: 0.00549623928964138\n",
            "Gradients at iteration 124:\n",
            " - layer1.weight grad norm: 0.515449583530426\n",
            " - layer1.bias grad norm: 0.0005952592473477125\n",
            " - layer2.weight grad norm: 0.001980250235646963\n",
            " - layer2.bias grad norm: 0.0005545034073293209\n",
            " - layer2_input.weight grad norm: 0.48489347100257874\n",
            " - layer2_input.bias grad norm: 0.0005545034073293209\n",
            " - layer3.weight grad norm: 0.0027358231600373983\n",
            " - layer3.bias grad norm: 0.0005803449312224984\n",
            " - layer3_input.weight grad norm: 0.5005542635917664\n",
            " - layer3_input.bias grad norm: 0.0005803449312224984\n",
            " - layer4.weight grad norm: 0.0033067159820348024\n",
            " - layer4.bias grad norm: 0.0005747712566517293\n",
            " - layer4_input.weight grad norm: 0.4984147548675537\n",
            " - layer4_input.bias grad norm: 0.0005747712566517293\n",
            " - layer5.weight grad norm: 0.01276394072920084\n",
            " - layer5.bias grad norm: 0.005534444935619831\n",
            "Gradients at iteration 125:\n",
            " - layer1.weight grad norm: 0.4842287302017212\n",
            " - layer1.bias grad norm: 0.0005560170393437147\n",
            " - layer2.weight grad norm: 0.001908138277940452\n",
            " - layer2.bias grad norm: 0.0005579855642281473\n",
            " - layer2_input.weight grad norm: 0.4871177077293396\n",
            " - layer2_input.bias grad norm: 0.0005579855642281473\n",
            " - layer3.weight grad norm: 0.002643877873197198\n",
            " - layer3.bias grad norm: 0.0006240435177460313\n",
            " - layer3_input.weight grad norm: 0.5346104502677917\n",
            " - layer3_input.bias grad norm: 0.0006240435177460313\n",
            " - layer4.weight grad norm: 0.00315341935493052\n",
            " - layer4.bias grad norm: 0.0005700779729522765\n",
            " - layer4_input.weight grad norm: 0.4921627938747406\n",
            " - layer4_input.bias grad norm: 0.0005700779729522765\n",
            " - layer5.weight grad norm: 0.012429516762495041\n",
            " - layer5.bias grad norm: 0.005385109223425388\n",
            "Gradients at iteration 126:\n",
            " - layer1.weight grad norm: 0.5277428030967712\n",
            " - layer1.bias grad norm: 0.0006095903809182346\n",
            " - layer2.weight grad norm: 0.0019628088921308517\n",
            " - layer2.bias grad norm: 0.0006031086668372154\n",
            " - layer2_input.weight grad norm: 0.5182988047599792\n",
            " - layer2_input.bias grad norm: 0.0006031086668372154\n",
            " - layer3.weight grad norm: 0.0027261816430836916\n",
            " - layer3.bias grad norm: 0.0005665318458341062\n",
            " - layer3_input.weight grad norm: 0.4896739721298218\n",
            " - layer3_input.bias grad norm: 0.0005665318458341062\n",
            " - layer4.weight grad norm: 0.0032771627884358168\n",
            " - layer4.bias grad norm: 0.0005305290105752647\n",
            " - layer4_input.weight grad norm: 0.4613836109638214\n",
            " - layer4_input.bias grad norm: 0.0005305290105752647\n",
            " - layer5.weight grad norm: 0.011984013020992279\n",
            " - layer5.bias grad norm: 0.005522437393665314\n",
            "Gradients at iteration 127:\n",
            " - layer1.weight grad norm: 0.5354794859886169\n",
            " - layer1.bias grad norm: 0.0006254552281461656\n",
            " - layer2.weight grad norm: 0.0018946886993944645\n",
            " - layer2.bias grad norm: 0.0005725952214561403\n",
            " - layer2_input.weight grad norm: 0.49681273102760315\n",
            " - layer2_input.bias grad norm: 0.0005725952214561403\n",
            " - layer3.weight grad norm: 0.002649437403306365\n",
            " - layer3.bias grad norm: 0.0005976552492938936\n",
            " - layer3_input.weight grad norm: 0.5119839310646057\n",
            " - layer3_input.bias grad norm: 0.0005976552492938936\n",
            " - layer4.weight grad norm: 0.003193986602127552\n",
            " - layer4.bias grad norm: 0.0005185038316994905\n",
            " - layer4_input.weight grad norm: 0.4517812132835388\n",
            " - layer4_input.bias grad norm: 0.0005185038316994905\n",
            " - layer5.weight grad norm: 0.012352968566119671\n",
            " - layer5.bias grad norm: 0.0054015303030610085\n",
            "Gradients at iteration 128:\n",
            " - layer1.weight grad norm: 0.5004959106445312\n",
            " - layer1.bias grad norm: 0.0005687053198926151\n",
            " - layer2.weight grad norm: 0.001982237910851836\n",
            " - layer2.bias grad norm: 0.0005580299184657633\n",
            " - layer2_input.weight grad norm: 0.4889484643936157\n",
            " - layer2_input.bias grad norm: 0.0005580299184657633\n",
            " - layer3.weight grad norm: 0.0027454474475234747\n",
            " - layer3.bias grad norm: 0.0005973223596811295\n",
            " - layer3_input.weight grad norm: 0.5167455673217773\n",
            " - layer3_input.bias grad norm: 0.0005973223596811295\n",
            " - layer4.weight grad norm: 0.0032489949371665716\n",
            " - layer4.bias grad norm: 0.0005630871746689081\n",
            " - layer4_input.weight grad norm: 0.49313393235206604\n",
            " - layer4_input.bias grad norm: 0.0005630871746689081\n",
            " - layer5.weight grad norm: 0.013063955120742321\n",
            " - layer5.bias grad norm: 0.005593435373157263\n",
            "Gradients at iteration 129:\n",
            " - layer1.weight grad norm: 0.4941001832485199\n",
            " - layer1.bias grad norm: 0.0005661840550601482\n",
            " - layer2.weight grad norm: 0.0019512351136654615\n",
            " - layer2.bias grad norm: 0.000612066884059459\n",
            " - layer2_input.weight grad norm: 0.5248501300811768\n",
            " - layer2_input.bias grad norm: 0.000612066884059459\n",
            " - layer3.weight grad norm: 0.002755752531811595\n",
            " - layer3.bias grad norm: 0.0006079193553887308\n",
            " - layer3_input.weight grad norm: 0.5225812792778015\n",
            " - layer3_input.bias grad norm: 0.0006079193553887308\n",
            " - layer4.weight grad norm: 0.0032635899260640144\n",
            " - layer4.bias grad norm: 0.0005202331813052297\n",
            " - layer4_input.weight grad norm: 0.45507925748825073\n",
            " - layer4_input.bias grad norm: 0.0005202331813052297\n",
            " - layer5.weight grad norm: 0.012395174242556095\n",
            " - layer5.bias grad norm: 0.00556485541164875\n",
            "Gradients at iteration 130:\n",
            " - layer1.weight grad norm: 0.4887561500072479\n",
            " - layer1.bias grad norm: 0.0005555887473747134\n",
            " - layer2.weight grad norm: 0.0019773151725530624\n",
            " - layer2.bias grad norm: 0.0005932916537858546\n",
            " - layer2_input.weight grad norm: 0.5152557492256165\n",
            " - layer2_input.bias grad norm: 0.0005932916537858546\n",
            " - layer3.weight grad norm: 0.0026918896473944187\n",
            " - layer3.bias grad norm: 0.000617236306425184\n",
            " - layer3_input.weight grad norm: 0.5306953191757202\n",
            " - layer3_input.bias grad norm: 0.000617236306425184\n",
            " - layer4.weight grad norm: 0.0032234047539532185\n",
            " - layer4.bias grad norm: 0.000537100771907717\n",
            " - layer4_input.weight grad norm: 0.46235954761505127\n",
            " - layer4_input.bias grad norm: 0.000537100771907717\n",
            " - layer5.weight grad norm: 0.012674260884523392\n",
            " - layer5.bias grad norm: 0.0055354563519358635\n",
            "Gradients at iteration 131:\n",
            " - layer1.weight grad norm: 0.5179478526115417\n",
            " - layer1.bias grad norm: 0.0005994475795887411\n",
            " - layer2.weight grad norm: 0.0018744905246421695\n",
            " - layer2.bias grad norm: 0.0006229823338799179\n",
            " - layer2_input.weight grad norm: 0.5292935967445374\n",
            " - layer2_input.bias grad norm: 0.0006229823338799179\n",
            " - layer3.weight grad norm: 0.0026347606908529997\n",
            " - layer3.bias grad norm: 0.0005277019809000194\n",
            " - layer3_input.weight grad norm: 0.4599502384662628\n",
            " - layer3_input.bias grad norm: 0.0005277019809000194\n",
            " - layer4.weight grad norm: 0.003111262107267976\n",
            " - layer4.bias grad norm: 0.0005701281479559839\n",
            " - layer4_input.weight grad norm: 0.48972299695014954\n",
            " - layer4_input.bias grad norm: 0.0005701281479559839\n",
            " - layer5.weight grad norm: 0.012035131454467773\n",
            " - layer5.bias grad norm: 0.005296480376273394\n",
            "Gradients at iteration 132:\n",
            " - layer1.weight grad norm: 0.4885762929916382\n",
            " - layer1.bias grad norm: 0.0005711672129109502\n",
            " - layer2.weight grad norm: 0.0018546986393630505\n",
            " - layer2.bias grad norm: 0.0006147651583887637\n",
            " - layer2_input.weight grad norm: 0.523526132106781\n",
            " - layer2_input.bias grad norm: 0.0006147651583887637\n",
            " - layer3.weight grad norm: 0.002611844101920724\n",
            " - layer3.bias grad norm: 0.0005546684260480106\n",
            " - layer3_input.weight grad norm: 0.4757123589515686\n",
            " - layer3_input.bias grad norm: 0.0005546684260480106\n",
            " - layer4.weight grad norm: 0.003073005238547921\n",
            " - layer4.bias grad norm: 0.000603849592152983\n",
            " - layer4_input.weight grad norm: 0.5106185078620911\n",
            " - layer4_input.bias grad norm: 0.000603849592152983\n",
            " - layer5.weight grad norm: 0.011419882997870445\n",
            " - layer5.bias grad norm: 0.005253743380308151\n",
            "Gradients at iteration 133:\n",
            " - layer1.weight grad norm: 0.528082013130188\n",
            " - layer1.bias grad norm: 0.0006038281717337668\n",
            " - layer2.weight grad norm: 0.002017772989347577\n",
            " - layer2.bias grad norm: 0.0005907281883992255\n",
            " - layer2_input.weight grad norm: 0.5164121389389038\n",
            " - layer2_input.bias grad norm: 0.0005907281883992255\n",
            " - layer3.weight grad norm: 0.002824424533173442\n",
            " - layer3.bias grad norm: 0.0005829650326631963\n",
            " - layer3_input.weight grad norm: 0.5054365992546082\n",
            " - layer3_input.bias grad norm: 0.0005829650326631963\n",
            " - layer4.weight grad norm: 0.0032927561551332474\n",
            " - layer4.bias grad norm: 0.0004983254475519061\n",
            " - layer4_input.weight grad norm: 0.4458465874195099\n",
            " - layer4_input.bias grad norm: 0.0004983254475519061\n",
            " - layer5.weight grad norm: 0.01206396333873272\n",
            " - layer5.bias grad norm: 0.0056340331211686134\n",
            "Gradients at iteration 134:\n",
            " - layer1.weight grad norm: 0.5221626162528992\n",
            " - layer1.bias grad norm: 0.000608207134064287\n",
            " - layer2.weight grad norm: 0.0018628316465765238\n",
            " - layer2.bias grad norm: 0.0005799118662253022\n",
            " - layer2_input.weight grad norm: 0.49727576971054077\n",
            " - layer2_input.bias grad norm: 0.0005799118662253022\n",
            " - layer3.weight grad norm: 0.0025552597362548113\n",
            " - layer3.bias grad norm: 0.0005732516292482615\n",
            " - layer3_input.weight grad norm: 0.4912583529949188\n",
            " - layer3_input.bias grad norm: 0.0005732516292482615\n",
            " - layer4.weight grad norm: 0.003029986983165145\n",
            " - layer4.bias grad norm: 0.0005740987253375351\n",
            " - layer4_input.weight grad norm: 0.48841193318367004\n",
            " - layer4_input.bias grad norm: 0.0005740987253375351\n",
            " - layer5.weight grad norm: 0.011538353748619556\n",
            " - layer5.bias grad norm: 0.0052235000766813755\n",
            "Gradients at iteration 135:\n",
            " - layer1.weight grad norm: 0.5180256366729736\n",
            " - layer1.bias grad norm: 0.0006044646725058556\n",
            " - layer2.weight grad norm: 0.0019384219776839018\n",
            " - layer2.bias grad norm: 0.0006156137096695602\n",
            " - layer2_input.weight grad norm: 0.5231398344039917\n",
            " - layer2_input.bias grad norm: 0.0006156137096695602\n",
            " - layer3.weight grad norm: 0.0027377218939363956\n",
            " - layer3.bias grad norm: 0.0005580211873166263\n",
            " - layer3_input.weight grad norm: 0.4816317856311798\n",
            " - layer3_input.bias grad norm: 0.0005580211873166263\n",
            " - layer4.weight grad norm: 0.003199751954525709\n",
            " - layer4.bias grad norm: 0.0005527771427296102\n",
            " - layer4_input.weight grad norm: 0.475191205739975\n",
            " - layer4_input.bias grad norm: 0.0005527771427296102\n",
            " - layer5.weight grad norm: 0.012016940861940384\n",
            " - layer5.bias grad norm: 0.005498192738741636\n",
            "Gradients at iteration 136:\n",
            " - layer1.weight grad norm: 0.4595472514629364\n",
            " - layer1.bias grad norm: 0.0005172725068405271\n",
            " - layer2.weight grad norm: 0.0019180105300620198\n",
            " - layer2.bias grad norm: 0.0005756621831096709\n",
            " - layer2_input.weight grad norm: 0.49998512864112854\n",
            " - layer2_input.bias grad norm: 0.0005756621831096709\n",
            " - layer3.weight grad norm: 0.0026014484465122223\n",
            " - layer3.bias grad norm: 0.0005853484617546201\n",
            " - layer3_input.weight grad norm: 0.504978358745575\n",
            " - layer3_input.bias grad norm: 0.0005853484617546201\n",
            " - layer4.weight grad norm: 0.003113117767497897\n",
            " - layer4.bias grad norm: 0.0006241737282834947\n",
            " - layer4_input.weight grad norm: 0.5325736999511719\n",
            " - layer4_input.bias grad norm: 0.0006241737282834947\n",
            " - layer5.weight grad norm: 0.011918245814740658\n",
            " - layer5.bias grad norm: 0.005369293503463268\n",
            "Gradients at iteration 137:\n",
            " - layer1.weight grad norm: 0.5129961967468262\n",
            " - layer1.bias grad norm: 0.0005927536403760314\n",
            " - layer2.weight grad norm: 0.0019183536060154438\n",
            " - layer2.bias grad norm: 0.0006405721069313586\n",
            " - layer2_input.weight grad norm: 0.5429599285125732\n",
            " - layer2_input.bias grad norm: 0.0006405721069313586\n",
            " - layer3.weight grad norm: 0.0026104915887117386\n",
            " - layer3.bias grad norm: 0.0005657484871335328\n",
            " - layer3_input.weight grad norm: 0.48628050088882446\n",
            " - layer3_input.bias grad norm: 0.0005657484871335328\n",
            " - layer4.weight grad norm: 0.003134719794616103\n",
            " - layer4.bias grad norm: 0.0005227827932685614\n",
            " - layer4_input.weight grad norm: 0.453145295381546\n",
            " - layer4_input.bias grad norm: 0.0005227827932685614\n",
            " - layer5.weight grad norm: 0.012986574321985245\n",
            " - layer5.bias grad norm: 0.005358698777854443\n",
            "Gradients at iteration 138:\n",
            " - layer1.weight grad norm: 0.49607139825820923\n",
            " - layer1.bias grad norm: 0.0005686099175363779\n",
            " - layer2.weight grad norm: 0.0019031327683478594\n",
            " - layer2.bias grad norm: 0.0005919208051636815\n",
            " - layer2_input.weight grad norm: 0.5114544034004211\n",
            " - layer2_input.bias grad norm: 0.0005919208051636815\n",
            " - layer3.weight grad norm: 0.0025990738067775965\n",
            " - layer3.bias grad norm: 0.0006018324056640267\n",
            " - layer3_input.weight grad norm: 0.5189229846000671\n",
            " - layer3_input.bias grad norm: 0.0006018324056640267\n",
            " - layer4.weight grad norm: 0.0031155613251030445\n",
            " - layer4.bias grad norm: 0.0005408889264799654\n",
            " - layer4_input.weight grad norm: 0.4720771312713623\n",
            " - layer4_input.bias grad norm: 0.0005408889264799654\n",
            " - layer5.weight grad norm: 0.011768954806029797\n",
            " - layer5.bias grad norm: 0.005358153022825718\n",
            "Gradients at iteration 139:\n",
            " - layer1.weight grad norm: 0.5093382596969604\n",
            " - layer1.bias grad norm: 0.000593271863181144\n",
            " - layer2.weight grad norm: 0.0019514126470312476\n",
            " - layer2.bias grad norm: 0.0006078924634493887\n",
            " - layer2_input.weight grad norm: 0.5226258635520935\n",
            " - layer2_input.bias grad norm: 0.0006078924634493887\n",
            " - layer3.weight grad norm: 0.0027345686685293913\n",
            " - layer3.bias grad norm: 0.0006026356713846326\n",
            " - layer3_input.weight grad norm: 0.516748309135437\n",
            " - layer3_input.bias grad norm: 0.0006026356713846326\n",
            " - layer4.weight grad norm: 0.0032678358256816864\n",
            " - layer4.bias grad norm: 0.0005084463045932353\n",
            " - layer4_input.weight grad norm: 0.44743528962135315\n",
            " - layer4_input.bias grad norm: 0.0005084463045932353\n",
            " - layer5.weight grad norm: 0.012441170401871204\n",
            " - layer5.bias grad norm: 0.005522510036826134\n",
            "Gradients at iteration 140:\n",
            " - layer1.weight grad norm: 0.5113512277603149\n",
            " - layer1.bias grad norm: 0.0005961538408882916\n",
            " - layer2.weight grad norm: 0.001981626730412245\n",
            " - layer2.bias grad norm: 0.0005961841670796275\n",
            " - layer2_input.weight grad norm: 0.510704755783081\n",
            " - layer2_input.bias grad norm: 0.0005961841670796275\n",
            " - layer3.weight grad norm: 0.00271054869517684\n",
            " - layer3.bias grad norm: 0.0005499455728568137\n",
            " - layer3_input.weight grad norm: 0.47640642523765564\n",
            " - layer3_input.bias grad norm: 0.0005499455728568137\n",
            " - layer4.weight grad norm: 0.0032388975378125906\n",
            " - layer4.bias grad norm: 0.0005868074367754161\n",
            " - layer4_input.weight grad norm: 0.5005502700805664\n",
            " - layer4_input.bias grad norm: 0.0005868074367754161\n",
            " - layer5.weight grad norm: 0.011486569419503212\n",
            " - layer5.bias grad norm: 0.005543102510273457\n",
            "Gradients at iteration 141:\n",
            " - layer1.weight grad norm: 0.5291244387626648\n",
            " - layer1.bias grad norm: 0.0006193647859618068\n",
            " - layer2.weight grad norm: 0.002009214833378792\n",
            " - layer2.bias grad norm: 0.0005662586190737784\n",
            " - layer2_input.weight grad norm: 0.4898378252983093\n",
            " - layer2_input.bias grad norm: 0.0005662586190737784\n",
            " - layer3.weight grad norm: 0.002750383922830224\n",
            " - layer3.bias grad norm: 0.0005897073424421251\n",
            " - layer3_input.weight grad norm: 0.5047028660774231\n",
            " - layer3_input.bias grad norm: 0.0005897073424421251\n",
            " - layer4.weight grad norm: 0.0032966863363981247\n",
            " - layer4.bias grad norm: 0.0005495463847182691\n",
            " - layer4_input.weight grad norm: 0.4744712710380554\n",
            " - layer4_input.bias grad norm: 0.0005495463847182691\n",
            " - layer5.weight grad norm: 0.013472300954163074\n",
            " - layer5.bias grad norm: 0.005648610647767782\n",
            "Gradients at iteration 142:\n",
            " - layer1.weight grad norm: 0.5120689868927002\n",
            " - layer1.bias grad norm: 0.0005824475083500147\n",
            " - layer2.weight grad norm: 0.0019927152898162603\n",
            " - layer2.bias grad norm: 0.0005878262454643846\n",
            " - layer2_input.weight grad norm: 0.5133529305458069\n",
            " - layer2_input.bias grad norm: 0.0005878262454643846\n",
            " - layer3.weight grad norm: 0.0027961961459368467\n",
            " - layer3.bias grad norm: 0.0005486998707056046\n",
            " - layer3_input.weight grad norm: 0.48072636127471924\n",
            " - layer3_input.bias grad norm: 0.0005486998707056046\n",
            " - layer4.weight grad norm: 0.0033884395379573107\n",
            " - layer4.bias grad norm: 0.0005676804576069117\n",
            " - layer4_input.weight grad norm: 0.4928765594959259\n",
            " - layer4_input.bias grad norm: 0.0005676804576069117\n",
            " - layer5.weight grad norm: 0.01306725014001131\n",
            " - layer5.bias grad norm: 0.00571562722325325\n",
            "Gradients at iteration 143:\n",
            " - layer1.weight grad norm: 0.48764488101005554\n",
            " - layer1.bias grad norm: 0.0005596224218606949\n",
            " - layer2.weight grad norm: 0.0019294752273708582\n",
            " - layer2.bias grad norm: 0.0005975986132398248\n",
            " - layer2_input.weight grad norm: 0.5081316232681274\n",
            " - layer2_input.bias grad norm: 0.0005975986132398248\n",
            " - layer3.weight grad norm: 0.0026251173112541437\n",
            " - layer3.bias grad norm: 0.0006287125288508832\n",
            " - layer3_input.weight grad norm: 0.5321744084358215\n",
            " - layer3_input.bias grad norm: 0.0006287125288508832\n",
            " - layer4.weight grad norm: 0.0031551942229270935\n",
            " - layer4.bias grad norm: 0.0005486074951477349\n",
            " - layer4_input.weight grad norm: 0.4696767032146454\n",
            " - layer4_input.bias grad norm: 0.0005486074951477349\n",
            " - layer5.weight grad norm: 0.012121316976845264\n",
            " - layer5.bias grad norm: 0.005395629908889532\n",
            "Gradients at iteration 144:\n",
            " - layer1.weight grad norm: 0.5453519225120544\n",
            " - layer1.bias grad norm: 0.0006346350419335067\n",
            " - layer2.weight grad norm: 0.0019520471105352044\n",
            " - layer2.bias grad norm: 0.0005730184493586421\n",
            " - layer2_input.weight grad norm: 0.4962003827095032\n",
            " - layer2_input.bias grad norm: 0.0005730184493586421\n",
            " - layer3.weight grad norm: 0.0027047155890613794\n",
            " - layer3.bias grad norm: 0.0005787390400655568\n",
            " - layer3_input.weight grad norm: 0.5001040697097778\n",
            " - layer3_input.bias grad norm: 0.0005787390400655568\n",
            " - layer4.weight grad norm: 0.0032301032915711403\n",
            " - layer4.bias grad norm: 0.0005199940642341971\n",
            " - layer4_input.weight grad norm: 0.4539504051208496\n",
            " - layer4_input.bias grad norm: 0.0005199940642341971\n",
            " - layer5.weight grad norm: 0.012134801596403122\n",
            " - layer5.bias grad norm: 0.005514543037861586\n",
            "Gradients at iteration 145:\n",
            " - layer1.weight grad norm: 0.5301700234413147\n",
            " - layer1.bias grad norm: 0.0006205678801052272\n",
            " - layer2.weight grad norm: 0.0019135168986395001\n",
            " - layer2.bias grad norm: 0.0005912500782869756\n",
            " - layer2_input.weight grad norm: 0.5106538534164429\n",
            " - layer2_input.bias grad norm: 0.0005912500782869756\n",
            " - layer3.weight grad norm: 0.0026224239263683558\n",
            " - layer3.bias grad norm: 0.0005461825639940798\n",
            " - layer3_input.weight grad norm: 0.47364452481269836\n",
            " - layer3_input.bias grad norm: 0.0005461825639940798\n",
            " - layer4.weight grad norm: 0.0031446872744709253\n",
            " - layer4.bias grad norm: 0.0005641969619318843\n",
            " - layer4_input.weight grad norm: 0.48332786560058594\n",
            " - layer4_input.bias grad norm: 0.0005641969619318843\n",
            " - layer5.weight grad norm: 0.012469042092561722\n",
            " - layer5.bias grad norm: 0.0053942399099469185\n",
            "Gradients at iteration 146:\n",
            " - layer1.weight grad norm: 0.5302700996398926\n",
            " - layer1.bias grad norm: 0.000614889373537153\n",
            " - layer2.weight grad norm: 0.0018947917269542813\n",
            " - layer2.bias grad norm: 0.0005977385444566607\n",
            " - layer2_input.weight grad norm: 0.5132896900177002\n",
            " - layer2_input.bias grad norm: 0.0005977385444566607\n",
            " - layer3.weight grad norm: 0.002662953920662403\n",
            " - layer3.bias grad norm: 0.0005236841971054673\n",
            " - layer3_input.weight grad norm: 0.45905911922454834\n",
            " - layer3_input.bias grad norm: 0.0005236841971054673\n",
            " - layer4.weight grad norm: 0.003123978618532419\n",
            " - layer4.bias grad norm: 0.0005739081534557045\n",
            " - layer4_input.weight grad norm: 0.49437689781188965\n",
            " - layer4_input.bias grad norm: 0.0005739081534557045\n",
            " - layer5.weight grad norm: 0.012326030991971493\n",
            " - layer5.bias grad norm: 0.0053761303424835205\n",
            "Gradients at iteration 147:\n",
            " - layer1.weight grad norm: 0.5118131041526794\n",
            " - layer1.bias grad norm: 0.0005947403842583299\n",
            " - layer2.weight grad norm: 0.001890613348223269\n",
            " - layer2.bias grad norm: 0.0006094303098507226\n",
            " - layer2_input.weight grad norm: 0.5241864323616028\n",
            " - layer2_input.bias grad norm: 0.0006094303098507226\n",
            " - layer3.weight grad norm: 0.002610108582302928\n",
            " - layer3.bias grad norm: 0.0005751895369030535\n",
            " - layer3_input.weight grad norm: 0.4941692054271698\n",
            " - layer3_input.bias grad norm: 0.0005751895369030535\n",
            " - layer4.weight grad norm: 0.0031109927222132683\n",
            " - layer4.bias grad norm: 0.000538381515070796\n",
            " - layer4_input.weight grad norm: 0.4678330719470978\n",
            " - layer4_input.bias grad norm: 0.000538381515070796\n",
            " - layer5.weight grad norm: 0.012381648644804955\n",
            " - layer5.bias grad norm: 0.00539184408262372\n",
            "Gradients at iteration 148:\n",
            " - layer1.weight grad norm: 0.5206083655357361\n",
            " - layer1.bias grad norm: 0.0006077625439502299\n",
            " - layer2.weight grad norm: 0.0020110802724957466\n",
            " - layer2.bias grad norm: 0.0005695941508747637\n",
            " - layer2_input.weight grad norm: 0.49300500750541687\n",
            " - layer2_input.bias grad norm: 0.0005695941508747637\n",
            " - layer3.weight grad norm: 0.002761858981102705\n",
            " - layer3.bias grad norm: 0.0005885232239961624\n",
            " - layer3_input.weight grad norm: 0.5049030780792236\n",
            " - layer3_input.bias grad norm: 0.0005885232239961624\n",
            " - layer4.weight grad norm: 0.0033065471798181534\n",
            " - layer4.bias grad norm: 0.0005560301360674202\n",
            " - layer4_input.weight grad norm: 0.48038288950920105\n",
            " - layer4_input.bias grad norm: 0.0005560301360674202\n",
            " - layer5.weight grad norm: 0.01271082367748022\n",
            " - layer5.bias grad norm: 0.0056349108926951885\n",
            "Gradients at iteration 149:\n",
            " - layer1.weight grad norm: 0.5047335624694824\n",
            " - layer1.bias grad norm: 0.0005890410393476486\n",
            " - layer2.weight grad norm: 0.0019015343859791756\n",
            " - layer2.bias grad norm: 0.0005847630673088133\n",
            " - layer2_input.weight grad norm: 0.5002121329307556\n",
            " - layer2_input.bias grad norm: 0.0005847630673088133\n",
            " - layer3.weight grad norm: 0.002620080253109336\n",
            " - layer3.bias grad norm: 0.0006041254382580519\n",
            " - layer3_input.weight grad norm: 0.5153754353523254\n",
            " - layer3_input.bias grad norm: 0.0006041254382580519\n",
            " - layer4.weight grad norm: 0.0031022382900118828\n",
            " - layer4.bias grad norm: 0.0005622016033157706\n",
            " - layer4_input.weight grad norm: 0.4787883460521698\n",
            " - layer4_input.bias grad norm: 0.0005622016033157706\n",
            " - layer5.weight grad norm: 0.011432827450335026\n",
            " - layer5.bias grad norm: 0.005347830709069967\n",
            "Gradients at iteration 150:\n",
            " - layer1.weight grad norm: 0.55950927734375\n",
            " - layer1.bias grad norm: 0.0006557009764946997\n",
            " - layer2.weight grad norm: 0.0019745209719985723\n",
            " - layer2.bias grad norm: 0.0005473797209560871\n",
            " - layer2_input.weight grad norm: 0.47822269797325134\n",
            " - layer2_input.bias grad norm: 0.0005473797209560871\n",
            " - layer3.weight grad norm: 0.002727376064285636\n",
            " - layer3.bias grad norm: 0.0005206978530623019\n",
            " - layer3_input.weight grad norm: 0.4595888555049896\n",
            " - layer3_input.bias grad norm: 0.0005206978530623019\n",
            " - layer4.weight grad norm: 0.003283412428572774\n",
            " - layer4.bias grad norm: 0.0005753401783294976\n",
            " - layer4_input.weight grad norm: 0.4968106746673584\n",
            " - layer4_input.bias grad norm: 0.0005753401783294976\n",
            " - layer5.weight grad norm: 0.012445085681974888\n",
            " - layer5.bias grad norm: 0.005529962945729494\n",
            "Gradients at iteration 151:\n",
            " - layer1.weight grad norm: 0.5376551747322083\n",
            " - layer1.bias grad norm: 0.0006259595393203199\n",
            " - layer2.weight grad norm: 0.0018924333853647113\n",
            " - layer2.bias grad norm: 0.0005670205573551357\n",
            " - layer2_input.weight grad norm: 0.4904624819755554\n",
            " - layer2_input.bias grad norm: 0.0005670205573551357\n",
            " - layer3.weight grad norm: 0.002644079504534602\n",
            " - layer3.bias grad norm: 0.0006217737100087106\n",
            " - layer3_input.weight grad norm: 0.5318993330001831\n",
            " - layer3_input.bias grad norm: 0.0006217737100087106\n",
            " - layer4.weight grad norm: 0.0031078392639756203\n",
            " - layer4.bias grad norm: 0.0004934205790050328\n",
            " - layer4_input.weight grad norm: 0.4327288269996643\n",
            " - layer4_input.bias grad norm: 0.0004934205790050328\n",
            " - layer5.weight grad norm: 0.012307645753026009\n",
            " - layer5.bias grad norm: 0.005312098655849695\n",
            "Gradients at iteration 152:\n",
            " - layer1.weight grad norm: 0.5612298846244812\n",
            " - layer1.bias grad norm: 0.0006613855366595089\n",
            " - layer2.weight grad norm: 0.00186127086635679\n",
            " - layer2.bias grad norm: 0.0006012859521433711\n",
            " - layer2_input.weight grad norm: 0.5113083720207214\n",
            " - layer2_input.bias grad norm: 0.0006012859521433711\n",
            " - layer3.weight grad norm: 0.0025361927691847086\n",
            " - layer3.bias grad norm: 0.0005587092600762844\n",
            " - layer3_input.weight grad norm: 0.47845152020454407\n",
            " - layer3_input.bias grad norm: 0.0005587092600762844\n",
            " - layer4.weight grad norm: 0.0030331427697092295\n",
            " - layer4.bias grad norm: 0.0005040038377046585\n",
            " - layer4_input.weight grad norm: 0.44099161028862\n",
            " - layer4_input.bias grad norm: 0.0005040038377046585\n",
            " - layer5.weight grad norm: 0.012124243192374706\n",
            " - layer5.bias grad norm: 0.005209086928516626\n",
            "Gradients at iteration 153:\n",
            " - layer1.weight grad norm: 0.5244500041007996\n",
            " - layer1.bias grad norm: 0.0006126147927716374\n",
            " - layer2.weight grad norm: 0.0018771864706650376\n",
            " - layer2.bias grad norm: 0.0005845398409292102\n",
            " - layer2_input.weight grad norm: 0.5033530592918396\n",
            " - layer2_input.bias grad norm: 0.0005845398409292102\n",
            " - layer3.weight grad norm: 0.0026215538382530212\n",
            " - layer3.bias grad norm: 0.000584286346565932\n",
            " - layer3_input.weight grad norm: 0.5008382797241211\n",
            " - layer3_input.bias grad norm: 0.000584286346565932\n",
            " - layer4.weight grad norm: 0.003153955563902855\n",
            " - layer4.bias grad norm: 0.000540194334462285\n",
            " - layer4_input.weight grad norm: 0.4696252942085266\n",
            " - layer4_input.bias grad norm: 0.000540194334462285\n",
            " - layer5.weight grad norm: 0.012238721363246441\n",
            " - layer5.bias grad norm: 0.00534252543002367\n",
            "Gradients at iteration 154:\n",
            " - layer1.weight grad norm: 0.5082538723945618\n",
            " - layer1.bias grad norm: 0.0005839266814291477\n",
            " - layer2.weight grad norm: 0.0019764797762036324\n",
            " - layer2.bias grad norm: 0.0005503720021806657\n",
            " - layer2_input.weight grad norm: 0.4812527894973755\n",
            " - layer2_input.bias grad norm: 0.0005503720021806657\n",
            " - layer3.weight grad norm: 0.002709261141717434\n",
            " - layer3.bias grad norm: 0.0006319085950963199\n",
            " - layer3_input.weight grad norm: 0.5396929979324341\n",
            " - layer3_input.bias grad norm: 0.0006319085950963199\n",
            " - layer4.weight grad norm: 0.003251387970522046\n",
            " - layer4.bias grad norm: 0.0005380312213674188\n",
            " - layer4_input.weight grad norm: 0.467537522315979\n",
            " - layer4_input.bias grad norm: 0.0005380312213674188\n",
            " - layer5.weight grad norm: 0.012575434520840645\n",
            " - layer5.bias grad norm: 0.005598155781626701\n",
            "Gradients at iteration 155:\n",
            " - layer1.weight grad norm: 0.5463868975639343\n",
            " - layer1.bias grad norm: 0.0006311227916739881\n",
            " - layer2.weight grad norm: 0.001932907267473638\n",
            " - layer2.bias grad norm: 0.0005330786225385964\n",
            " - layer2_input.weight grad norm: 0.47146549820899963\n",
            " - layer2_input.bias grad norm: 0.0005330786225385964\n",
            " - layer3.weight grad norm: 0.0027107868809252977\n",
            " - layer3.bias grad norm: 0.0005761674838140607\n",
            " - layer3_input.weight grad norm: 0.5020132660865784\n",
            " - layer3_input.bias grad norm: 0.0005761674838140607\n",
            " - layer4.weight grad norm: 0.0032192713115364313\n",
            " - layer4.bias grad norm: 0.0005478577804751694\n",
            " - layer4_input.weight grad norm: 0.47640877962112427\n",
            " - layer4_input.bias grad norm: 0.0005478577804751694\n",
            " - layer5.weight grad norm: 0.01204587146639824\n",
            " - layer5.bias grad norm: 0.005494924262166023\n",
            "Gradients at iteration 156:\n",
            " - layer1.weight grad norm: 0.5054199695587158\n",
            " - layer1.bias grad norm: 0.0005789838614873588\n",
            " - layer2.weight grad norm: 0.0019149933941662312\n",
            " - layer2.bias grad norm: 0.0005816164193674922\n",
            " - layer2_input.weight grad norm: 0.5011187791824341\n",
            " - layer2_input.bias grad norm: 0.0005816164193674922\n",
            " - layer3.weight grad norm: 0.002622455358505249\n",
            " - layer3.bias grad norm: 0.0005819614161737263\n",
            " - layer3_input.weight grad norm: 0.5042270421981812\n",
            " - layer3_input.bias grad norm: 0.0005819614161737263\n",
            " - layer4.weight grad norm: 0.0031023011542856693\n",
            " - layer4.bias grad norm: 0.0005622849566861987\n",
            " - layer4_input.weight grad norm: 0.4888623356819153\n",
            " - layer4_input.bias grad norm: 0.0005622849566861987\n",
            " - layer5.weight grad norm: 0.012174326926469803\n",
            " - layer5.bias grad norm: 0.0053429328836500645\n",
            "Gradients at iteration 157:\n",
            " - layer1.weight grad norm: 0.5147001147270203\n",
            " - layer1.bias grad norm: 0.000591318472288549\n",
            " - layer2.weight grad norm: 0.0019691730849444866\n",
            " - layer2.bias grad norm: 0.0006286768475547433\n",
            " - layer2_input.weight grad norm: 0.5403085350990295\n",
            " - layer2_input.bias grad norm: 0.0006286768475547433\n",
            " - layer3.weight grad norm: 0.002757992595434189\n",
            " - layer3.bias grad norm: 0.0005417606444098055\n",
            " - layer3_input.weight grad norm: 0.4743698537349701\n",
            " - layer3_input.bias grad norm: 0.0005417606444098055\n",
            " - layer4.weight grad norm: 0.0032605354208499193\n",
            " - layer4.bias grad norm: 0.0005361746298149228\n",
            " - layer4_input.weight grad norm: 0.466802716255188\n",
            " - layer4_input.bias grad norm: 0.0005361746298149228\n",
            " - layer5.weight grad norm: 0.012768346816301346\n",
            " - layer5.bias grad norm: 0.005616672337055206\n",
            "Gradients at iteration 158:\n",
            " - layer1.weight grad norm: 0.5102169513702393\n",
            " - layer1.bias grad norm: 0.0005955046508461237\n",
            " - layer2.weight grad norm: 0.0018527652136981487\n",
            " - layer2.bias grad norm: 0.0005404285620898008\n",
            " - layer2_input.weight grad norm: 0.47160181403160095\n",
            " - layer2_input.bias grad norm: 0.0005404285620898008\n",
            " - layer3.weight grad norm: 0.0025869819801300764\n",
            " - layer3.bias grad norm: 0.0006132801645435393\n",
            " - layer3_input.weight grad norm: 0.5199230313301086\n",
            " - layer3_input.bias grad norm: 0.0006132801645435393\n",
            " - layer4.weight grad norm: 0.0031015437562018633\n",
            " - layer4.bias grad norm: 0.0005837353528477252\n",
            " - layer4_input.weight grad norm: 0.4967213571071625\n",
            " - layer4_input.bias grad norm: 0.0005837353528477252\n",
            " - layer5.weight grad norm: 0.012971081770956516\n",
            " - layer5.bias grad norm: 0.005287374835461378\n",
            "Gradients at iteration 159:\n",
            " - layer1.weight grad norm: 0.5034044981002808\n",
            " - layer1.bias grad norm: 0.0005843557883054018\n",
            " - layer2.weight grad norm: 0.0019049348775297403\n",
            " - layer2.bias grad norm: 0.0006336456863209605\n",
            " - layer2_input.weight grad norm: 0.5365103483200073\n",
            " - layer2_input.bias grad norm: 0.0006336456863209605\n",
            " - layer3.weight grad norm: 0.0026487603317946196\n",
            " - layer3.bias grad norm: 0.0005785896209999919\n",
            " - layer3_input.weight grad norm: 0.49780353903770447\n",
            " - layer3_input.bias grad norm: 0.0005785896209999919\n",
            " - layer4.weight grad norm: 0.0031555842142552137\n",
            " - layer4.bias grad norm: 0.0005296211456879973\n",
            " - layer4_input.weight grad norm: 0.4590405225753784\n",
            " - layer4_input.bias grad norm: 0.0005296211456879973\n",
            " - layer5.weight grad norm: 0.012712450698018074\n",
            " - layer5.bias grad norm: 0.005420912057161331\n",
            "Gradients at iteration 160:\n",
            " - layer1.weight grad norm: 0.5094220042228699\n",
            " - layer1.bias grad norm: 0.0005898095550946891\n",
            " - layer2.weight grad norm: 0.001876884838566184\n",
            " - layer2.bias grad norm: 0.0005585215403698385\n",
            " - layer2_input.weight grad norm: 0.4868319630622864\n",
            " - layer2_input.bias grad norm: 0.0005585215403698385\n",
            " - layer3.weight grad norm: 0.0025992037262767553\n",
            " - layer3.bias grad norm: 0.0006304577109403908\n",
            " - layer3_input.weight grad norm: 0.5320723056793213\n",
            " - layer3_input.bias grad norm: 0.0006304577109403908\n",
            " - layer4.weight grad norm: 0.0031329523772001266\n",
            " - layer4.bias grad norm: 0.0005370734725147486\n",
            " - layer4_input.weight grad norm: 0.4692370295524597\n",
            " - layer4_input.bias grad norm: 0.0005370734725147486\n",
            " - layer5.weight grad norm: 0.012179562821984291\n",
            " - layer5.bias grad norm: 0.005362228024750948\n",
            "Gradients at iteration 161:\n",
            " - layer1.weight grad norm: 0.5039178729057312\n",
            " - layer1.bias grad norm: 0.0005844433326274157\n",
            " - layer2.weight grad norm: 0.0019642114639282227\n",
            " - layer2.bias grad norm: 0.0006100047612562776\n",
            " - layer2_input.weight grad norm: 0.5234362483024597\n",
            " - layer2_input.bias grad norm: 0.0006100047612562776\n",
            " - layer3.weight grad norm: 0.0027089579962193966\n",
            " - layer3.bias grad norm: 0.0005513745127245784\n",
            " - layer3_input.weight grad norm: 0.47595447301864624\n",
            " - layer3_input.bias grad norm: 0.0005513745127245784\n",
            " - layer4.weight grad norm: 0.003244588617235422\n",
            " - layer4.bias grad norm: 0.0005776580655947328\n",
            " - layer4_input.weight grad norm: 0.49530479311943054\n",
            " - layer4_input.bias grad norm: 0.0005776580655947328\n",
            " - layer5.weight grad norm: 0.012918461114168167\n",
            " - layer5.bias grad norm: 0.005551171023398638\n",
            "Gradients at iteration 162:\n",
            " - layer1.weight grad norm: 0.5020269155502319\n",
            " - layer1.bias grad norm: 0.0005749911651946604\n",
            " - layer2.weight grad norm: 0.0019849666859954596\n",
            " - layer2.bias grad norm: 0.0006039051222614944\n",
            " - layer2_input.weight grad norm: 0.5211688280105591\n",
            " - layer2_input.bias grad norm: 0.0006039051222614944\n",
            " - layer3.weight grad norm: 0.002714991569519043\n",
            " - layer3.bias grad norm: 0.0005751147400587797\n",
            " - layer3_input.weight grad norm: 0.4973854124546051\n",
            " - layer3_input.bias grad norm: 0.0005751147400587797\n",
            " - layer4.weight grad norm: 0.0031979214400053024\n",
            " - layer4.bias grad norm: 0.0005561506259255111\n",
            " - layer4_input.weight grad norm: 0.4782821536064148\n",
            " - layer4_input.bias grad norm: 0.0005561506259255111\n",
            " - layer5.weight grad norm: 0.012298790737986565\n",
            " - layer5.bias grad norm: 0.005565892904996872\n",
            "Gradients at iteration 163:\n",
            " - layer1.weight grad norm: 0.528866171836853\n",
            " - layer1.bias grad norm: 0.0006150841945782304\n",
            " - layer2.weight grad norm: 0.0019286500755697489\n",
            " - layer2.bias grad norm: 0.0005861500394530594\n",
            " - layer2_input.weight grad norm: 0.5060948133468628\n",
            " - layer2_input.bias grad norm: 0.0005861500394530594\n",
            " - layer3.weight grad norm: 0.0026700603775680065\n",
            " - layer3.bias grad norm: 0.0005880690878257155\n",
            " - layer3_input.weight grad norm: 0.5055579543113708\n",
            " - layer3_input.bias grad norm: 0.0005880690878257155\n",
            " - layer4.weight grad norm: 0.003174861427396536\n",
            " - layer4.bias grad norm: 0.0005231613176874816\n",
            " - layer4_input.weight grad norm: 0.4564620554447174\n",
            " - layer4_input.bias grad norm: 0.0005231613176874816\n",
            " - layer5.weight grad norm: 0.013017354533076286\n",
            " - layer5.bias grad norm: 0.00544704170897603\n",
            "Gradients at iteration 164:\n",
            " - layer1.weight grad norm: 0.4949035942554474\n",
            " - layer1.bias grad norm: 0.000572528166230768\n",
            " - layer2.weight grad norm: 0.0019625225104391575\n",
            " - layer2.bias grad norm: 0.0005895838839933276\n",
            " - layer2_input.weight grad norm: 0.5118048191070557\n",
            " - layer2_input.bias grad norm: 0.0005895838839933276\n",
            " - layer3.weight grad norm: 0.0027197450399398804\n",
            " - layer3.bias grad norm: 0.0006080911844037473\n",
            " - layer3_input.weight grad norm: 0.5209755301475525\n",
            " - layer3_input.bias grad norm: 0.0006080911844037473\n",
            " - layer4.weight grad norm: 0.003275406314060092\n",
            " - layer4.bias grad norm: 0.0005412120372056961\n",
            " - layer4_input.weight grad norm: 0.47063878178596497\n",
            " - layer4_input.bias grad norm: 0.0005412120372056961\n",
            " - layer5.weight grad norm: 0.01244925707578659\n",
            " - layer5.bias grad norm: 0.005543521139770746\n",
            "Gradients at iteration 165:\n",
            " - layer1.weight grad norm: 0.5164270997047424\n",
            " - layer1.bias grad norm: 0.0005981993163004518\n",
            " - layer2.weight grad norm: 0.0019848982337862253\n",
            " - layer2.bias grad norm: 0.0006148868124000728\n",
            " - layer2_input.weight grad norm: 0.529909610748291\n",
            " - layer2_input.bias grad norm: 0.0006148868124000728\n",
            " - layer3.weight grad norm: 0.0027253534644842148\n",
            " - layer3.bias grad norm: 0.0005546592874452472\n",
            " - layer3_input.weight grad norm: 0.4778672456741333\n",
            " - layer3_input.bias grad norm: 0.0005546592874452472\n",
            " - layer4.weight grad norm: 0.0032689415384083986\n",
            " - layer4.bias grad norm: 0.0005494540091603994\n",
            " - layer4_input.weight grad norm: 0.4731924831867218\n",
            " - layer4_input.bias grad norm: 0.0005494540091603994\n",
            " - layer5.weight grad norm: 0.013223670423030853\n",
            " - layer5.bias grad norm: 0.005607806611806154\n",
            "Gradients at iteration 166:\n",
            " - layer1.weight grad norm: 0.5247858166694641\n",
            " - layer1.bias grad norm: 0.0006042712484486401\n",
            " - layer2.weight grad norm: 0.001958334818482399\n",
            " - layer2.bias grad norm: 0.0005999323329888284\n",
            " - layer2_input.weight grad norm: 0.5205370783805847\n",
            " - layer2_input.bias grad norm: 0.0005999323329888284\n",
            " - layer3.weight grad norm: 0.0026825093664228916\n",
            " - layer3.bias grad norm: 0.0005636916030198336\n",
            " - layer3_input.weight grad norm: 0.48834964632987976\n",
            " - layer3_input.bias grad norm: 0.0005636916030198336\n",
            " - layer4.weight grad norm: 0.003191526746377349\n",
            " - layer4.bias grad norm: 0.000532067846506834\n",
            " - layer4_input.weight grad norm: 0.463617742061615\n",
            " - layer4_input.bias grad norm: 0.000532067846506834\n",
            " - layer5.weight grad norm: 0.012666049413383007\n",
            " - layer5.bias grad norm: 0.005494084674865007\n",
            "Gradients at iteration 167:\n",
            " - layer1.weight grad norm: 0.526239275932312\n",
            " - layer1.bias grad norm: 0.0006141157355159521\n",
            " - layer2.weight grad norm: 0.001867509912699461\n",
            " - layer2.bias grad norm: 0.0006540422909893095\n",
            " - layer2_input.weight grad norm: 0.5563761591911316\n",
            " - layer2_input.bias grad norm: 0.0006540422909893095\n",
            " - layer3.weight grad norm: 0.002565191825851798\n",
            " - layer3.bias grad norm: 0.00048571827937848866\n",
            " - layer3_input.weight grad norm: 0.4274214804172516\n",
            " - layer3_input.bias grad norm: 0.00048571827937848866\n",
            " - layer4.weight grad norm: 0.0030298891942948103\n",
            " - layer4.bias grad norm: 0.0005599387222900987\n",
            " - layer4_input.weight grad norm: 0.4802655875682831\n",
            " - layer4_input.bias grad norm: 0.0005599387222900987\n",
            " - layer5.weight grad norm: 0.011173369362950325\n",
            " - layer5.bias grad norm: 0.00522827310487628\n",
            "Gradients at iteration 168:\n",
            " - layer1.weight grad norm: 0.5220784544944763\n",
            " - layer1.bias grad norm: 0.0006013166857883334\n",
            " - layer2.weight grad norm: 0.0019446831429377198\n",
            " - layer2.bias grad norm: 0.000594684446696192\n",
            " - layer2_input.weight grad norm: 0.5165405869483948\n",
            " - layer2_input.bias grad norm: 0.000594684446696192\n",
            " - layer3.weight grad norm: 0.0026467873249202967\n",
            " - layer3.bias grad norm: 0.0005845451378263533\n",
            " - layer3_input.weight grad norm: 0.5012198686599731\n",
            " - layer3_input.bias grad norm: 0.0005845451378263533\n",
            " - layer4.weight grad norm: 0.003132070414721966\n",
            " - layer4.bias grad norm: 0.0005271964473649859\n",
            " - layer4_input.weight grad norm: 0.4573580026626587\n",
            " - layer4_input.bias grad norm: 0.0005271964473649859\n",
            " - layer5.weight grad norm: 0.013038606382906437\n",
            " - layer5.bias grad norm: 0.00540493568405509\n",
            "Gradients at iteration 169:\n",
            " - layer1.weight grad norm: 0.4874702990055084\n",
            " - layer1.bias grad norm: 0.0005598131683655083\n",
            " - layer2.weight grad norm: 0.0019097430631518364\n",
            " - layer2.bias grad norm: 0.0006061848835088313\n",
            " - layer2_input.weight grad norm: 0.5178062319755554\n",
            " - layer2_input.bias grad norm: 0.0006061848835088313\n",
            " - layer3.weight grad norm: 0.0026756124570965767\n",
            " - layer3.bias grad norm: 0.0006036449340172112\n",
            " - layer3_input.weight grad norm: 0.5130319595336914\n",
            " - layer3_input.bias grad norm: 0.0006036449340172112\n",
            " - layer4.weight grad norm: 0.0031913965940475464\n",
            " - layer4.bias grad norm: 0.000559418520424515\n",
            " - layer4_input.weight grad norm: 0.48046350479125977\n",
            " - layer4_input.bias grad norm: 0.000559418520424515\n",
            " - layer5.weight grad norm: 0.012230182066559792\n",
            " - layer5.bias grad norm: 0.005418313201516867\n",
            "Gradients at iteration 170:\n",
            " - layer1.weight grad norm: 0.5321342945098877\n",
            " - layer1.bias grad norm: 0.0006213868036866188\n",
            " - layer2.weight grad norm: 0.001927301287651062\n",
            " - layer2.bias grad norm: 0.0006169198895804584\n",
            " - layer2_input.weight grad norm: 0.5262776017189026\n",
            " - layer2_input.bias grad norm: 0.0006169198895804584\n",
            " - layer3.weight grad norm: 0.0027094020042568445\n",
            " - layer3.bias grad norm: 0.0005516792880371213\n",
            " - layer3_input.weight grad norm: 0.4826475977897644\n",
            " - layer3_input.bias grad norm: 0.0005516792880371213\n",
            " - layer4.weight grad norm: 0.0032238787971436977\n",
            " - layer4.bias grad norm: 0.0005210548988543451\n",
            " - layer4_input.weight grad norm: 0.45462891459465027\n",
            " - layer4_input.bias grad norm: 0.0005210548988543451\n",
            " - layer5.weight grad norm: 0.01321531180292368\n",
            " - layer5.bias grad norm: 0.0055192457512021065\n",
            "Gradients at iteration 171:\n",
            " - layer1.weight grad norm: 0.5151057839393616\n",
            " - layer1.bias grad norm: 0.000591017073020339\n",
            " - layer2.weight grad norm: 0.0018796134972944856\n",
            " - layer2.bias grad norm: 0.0006211200961843133\n",
            " - layer2_input.weight grad norm: 0.5340414643287659\n",
            " - layer2_input.bias grad norm: 0.0006211200961843133\n",
            " - layer3.weight grad norm: 0.0025685366708785295\n",
            " - layer3.bias grad norm: 0.0005647775833494961\n",
            " - layer3_input.weight grad norm: 0.4879878759384155\n",
            " - layer3_input.bias grad norm: 0.0005647775833494961\n",
            " - layer4.weight grad norm: 0.003054909873753786\n",
            " - layer4.bias grad norm: 0.0005269526736810803\n",
            " - layer4_input.weight grad norm: 0.45949074625968933\n",
            " - layer4_input.bias grad norm: 0.0005269526736810803\n",
            " - layer5.weight grad norm: 0.012325380928814411\n",
            " - layer5.bias grad norm: 0.0053070527501404285\n",
            "Gradients at iteration 172:\n",
            " - layer1.weight grad norm: 0.4760192632675171\n",
            " - layer1.bias grad norm: 0.0005421681562438607\n",
            " - layer2.weight grad norm: 0.0018810642650350928\n",
            " - layer2.bias grad norm: 0.0005561868892982602\n",
            " - layer2_input.weight grad norm: 0.4835587739944458\n",
            " - layer2_input.bias grad norm: 0.0005561868892982602\n",
            " - layer3.weight grad norm: 0.002649142639711499\n",
            " - layer3.bias grad norm: 0.0006083901971578598\n",
            " - layer3_input.weight grad norm: 0.5188795924186707\n",
            " - layer3_input.bias grad norm: 0.0006083901971578598\n",
            " - layer4.weight grad norm: 0.0031000711023807526\n",
            " - layer4.bias grad norm: 0.0006104859639890492\n",
            " - layer4_input.weight grad norm: 0.5197418332099915\n",
            " - layer4_input.bias grad norm: 0.0006104859639890492\n",
            " - layer5.weight grad norm: 0.012550147250294685\n",
            " - layer5.bias grad norm: 0.005367707461118698\n",
            "Gradients at iteration 173:\n",
            " - layer1.weight grad norm: 0.49335864186286926\n",
            " - layer1.bias grad norm: 0.0005668826051987708\n",
            " - layer2.weight grad norm: 0.0018577500013634562\n",
            " - layer2.bias grad norm: 0.0005809623398818076\n",
            " - layer2_input.weight grad norm: 0.5021781325340271\n",
            " - layer2_input.bias grad norm: 0.0005809623398818076\n",
            " - layer3.weight grad norm: 0.0025625007692724466\n",
            " - layer3.bias grad norm: 0.0006012541707605124\n",
            " - layer3_input.weight grad norm: 0.5116817951202393\n",
            " - layer3_input.bias grad norm: 0.0006012541707605124\n",
            " - layer4.weight grad norm: 0.003018087474629283\n",
            " - layer4.bias grad norm: 0.0005754094454459846\n",
            " - layer4_input.weight grad norm: 0.49232447147369385\n",
            " - layer4_input.bias grad norm: 0.0005754094454459846\n",
            " - layer5.weight grad norm: 0.012830543331801891\n",
            " - layer5.bias grad norm: 0.005165001843124628\n",
            "Gradients at iteration 174:\n",
            " - layer1.weight grad norm: 0.5265831351280212\n",
            " - layer1.bias grad norm: 0.0006148093962110579\n",
            " - layer2.weight grad norm: 0.0019019984174519777\n",
            " - layer2.bias grad norm: 0.0005608271458186209\n",
            " - layer2_input.weight grad norm: 0.48781388998031616\n",
            " - layer2_input.bias grad norm: 0.0005608271458186209\n",
            " - layer3.weight grad norm: 0.0026725544594228268\n",
            " - layer3.bias grad norm: 0.0005791608709841967\n",
            " - layer3_input.weight grad norm: 0.4971303939819336\n",
            " - layer3_input.bias grad norm: 0.0005791608709841967\n",
            " - layer4.weight grad norm: 0.0031782968435436487\n",
            " - layer4.bias grad norm: 0.0005581627483479679\n",
            " - layer4_input.weight grad norm: 0.48723673820495605\n",
            " - layer4_input.bias grad norm: 0.0005581627483479679\n",
            " - layer5.weight grad norm: 0.012525268830358982\n",
            " - layer5.bias grad norm: 0.005418205633759499\n",
            "Gradients at iteration 175:\n",
            " - layer1.weight grad norm: 0.5194914937019348\n",
            " - layer1.bias grad norm: 0.0006058007129468024\n",
            " - layer2.weight grad norm: 0.0018409444019198418\n",
            " - layer2.bias grad norm: 0.0005540950805880129\n",
            " - layer2_input.weight grad norm: 0.4773092269897461\n",
            " - layer2_input.bias grad norm: 0.0005540950805880129\n",
            " - layer3.weight grad norm: 0.0025724596343934536\n",
            " - layer3.bias grad norm: 0.0005768864066340029\n",
            " - layer3_input.weight grad norm: 0.4943910837173462\n",
            " - layer3_input.bias grad norm: 0.0005768864066340029\n",
            " - layer4.weight grad norm: 0.00303320842795074\n",
            " - layer4.bias grad norm: 0.0005999343702569604\n",
            " - layer4_input.weight grad norm: 0.5076342225074768\n",
            " - layer4_input.bias grad norm: 0.0005999343702569604\n",
            " - layer5.weight grad norm: 0.011851685121655464\n",
            " - layer5.bias grad norm: 0.005234388168901205\n",
            "Gradients at iteration 176:\n",
            " - layer1.weight grad norm: 0.4953043460845947\n",
            " - layer1.bias grad norm: 0.0005685989744961262\n",
            " - layer2.weight grad norm: 0.0018795531941577792\n",
            " - layer2.bias grad norm: 0.0005890214233659208\n",
            " - layer2_input.weight grad norm: 0.505499541759491\n",
            " - layer2_input.bias grad norm: 0.0005890214233659208\n",
            " - layer3.weight grad norm: 0.002620590850710869\n",
            " - layer3.bias grad norm: 0.0006069382070563734\n",
            " - layer3_input.weight grad norm: 0.5188100934028625\n",
            " - layer3_input.bias grad norm: 0.0006069382070563734\n",
            " - layer4.weight grad norm: 0.0031259923707693815\n",
            " - layer4.bias grad norm: 0.0005607178900390863\n",
            " - layer4_input.weight grad norm: 0.47936272621154785\n",
            " - layer4_input.bias grad norm: 0.0005607178900390863\n",
            " - layer5.weight grad norm: 0.011816266924142838\n",
            " - layer5.bias grad norm: 0.005388717167079449\n",
            "Gradients at iteration 177:\n",
            " - layer1.weight grad norm: 0.5116596221923828\n",
            " - layer1.bias grad norm: 0.0005936146480962634\n",
            " - layer2.weight grad norm: 0.0019693386275321245\n",
            " - layer2.bias grad norm: 0.000627847679425031\n",
            " - layer2_input.weight grad norm: 0.5399156212806702\n",
            " - layer2_input.bias grad norm: 0.000627847679425031\n",
            " - layer3.weight grad norm: 0.0027016932144761086\n",
            " - layer3.bias grad norm: 0.0005216570571064949\n",
            " - layer3_input.weight grad norm: 0.45994192361831665\n",
            " - layer3_input.bias grad norm: 0.0005216570571064949\n",
            " - layer4.weight grad norm: 0.0031991400755941868\n",
            " - layer4.bias grad norm: 0.0005612328532151878\n",
            " - layer4_input.weight grad norm: 0.48472335934638977\n",
            " - layer4_input.bias grad norm: 0.0005612328532151878\n",
            " - layer5.weight grad norm: 0.011743150651454926\n",
            " - layer5.bias grad norm: 0.005554595030844212\n",
            "Gradients at iteration 178:\n",
            " - layer1.weight grad norm: 0.49987897276878357\n",
            " - layer1.bias grad norm: 0.0005758298211731017\n",
            " - layer2.weight grad norm: 0.0019179399823769927\n",
            " - layer2.bias grad norm: 0.0006135535659268498\n",
            " - layer2_input.weight grad norm: 0.5243517756462097\n",
            " - layer2_input.bias grad norm: 0.0006135535659268498\n",
            " - layer3.weight grad norm: 0.002690674737095833\n",
            " - layer3.bias grad norm: 0.0005278625176288188\n",
            " - layer3_input.weight grad norm: 0.46080079674720764\n",
            " - layer3_input.bias grad norm: 0.0005278625176288188\n",
            " - layer4.weight grad norm: 0.003172214375808835\n",
            " - layer4.bias grad norm: 0.0006028579082340002\n",
            " - layer4_input.weight grad norm: 0.5124737024307251\n",
            " - layer4_input.bias grad norm: 0.0006028579082340002\n",
            " - layer5.weight grad norm: 0.012513687834143639\n",
            " - layer5.bias grad norm: 0.0054459115490317345\n",
            "Gradients at iteration 179:\n",
            " - layer1.weight grad norm: 0.5256611108779907\n",
            " - layer1.bias grad norm: 0.0006123619969002903\n",
            " - layer2.weight grad norm: 0.0019560225773602724\n",
            " - layer2.bias grad norm: 0.0005878370720893145\n",
            " - layer2_input.weight grad norm: 0.509865939617157\n",
            " - layer2_input.bias grad norm: 0.0005878370720893145\n",
            " - layer3.weight grad norm: 0.002726990031078458\n",
            " - layer3.bias grad norm: 0.0005672785337083042\n",
            " - layer3_input.weight grad norm: 0.4887465238571167\n",
            " - layer3_input.bias grad norm: 0.0005672785337083042\n",
            " - layer4.weight grad norm: 0.0032622842118144035\n",
            " - layer4.bias grad norm: 0.0005471143522299826\n",
            " - layer4_input.weight grad norm: 0.4739696681499481\n",
            " - layer4_input.bias grad norm: 0.0005471143522299826\n",
            " - layer5.weight grad norm: 0.011888940818607807\n",
            " - layer5.bias grad norm: 0.005578608252108097\n",
            "Gradients at iteration 180:\n",
            " - layer1.weight grad norm: 0.5441082715988159\n",
            " - layer1.bias grad norm: 0.0006319965468719602\n",
            " - layer2.weight grad norm: 0.002068702597171068\n",
            " - layer2.bias grad norm: 0.0006045189220458269\n",
            " - layer2_input.weight grad norm: 0.5161973834037781\n",
            " - layer2_input.bias grad norm: 0.0006045189220458269\n",
            " - layer3.weight grad norm: 0.002781463088467717\n",
            " - layer3.bias grad norm: 0.0005360536742955446\n",
            " - layer3_input.weight grad norm: 0.46720677614212036\n",
            " - layer3_input.bias grad norm: 0.0005360536742955446\n",
            " - layer4.weight grad norm: 0.0034100417979061604\n",
            " - layer4.bias grad norm: 0.0005395356565713882\n",
            " - layer4_input.weight grad norm: 0.46796584129333496\n",
            " - layer4_input.bias grad norm: 0.0005395356565713882\n",
            " - layer5.weight grad norm: 0.012384505942463875\n",
            " - layer5.bias grad norm: 0.005727139767259359\n",
            "Gradients at iteration 181:\n",
            " - layer1.weight grad norm: 0.5018444061279297\n",
            " - layer1.bias grad norm: 0.0005670836544595659\n",
            " - layer2.weight grad norm: 0.0019880770705640316\n",
            " - layer2.bias grad norm: 0.0005730100092478096\n",
            " - layer2_input.weight grad norm: 0.5024625062942505\n",
            " - layer2_input.bias grad norm: 0.0005730100092478096\n",
            " - layer3.weight grad norm: 0.002752268221229315\n",
            " - layer3.bias grad norm: 0.0005882518598809838\n",
            " - layer3_input.weight grad norm: 0.5077090859413147\n",
            " - layer3_input.bias grad norm: 0.0005882518598809838\n",
            " - layer4.weight grad norm: 0.0033172722905874252\n",
            " - layer4.bias grad norm: 0.0005565888131968677\n",
            " - layer4_input.weight grad norm: 0.4875544607639313\n",
            " - layer4_input.bias grad norm: 0.0005565888131968677\n",
            " - layer5.weight grad norm: 0.012218091636896133\n",
            " - layer5.bias grad norm: 0.0056160795502364635\n",
            "Gradients at iteration 182:\n",
            " - layer1.weight grad norm: 0.538427472114563\n",
            " - layer1.bias grad norm: 0.0006287604919634759\n",
            " - layer2.weight grad norm: 0.001932665822096169\n",
            " - layer2.bias grad norm: 0.0006091308896429837\n",
            " - layer2_input.weight grad norm: 0.5228719711303711\n",
            " - layer2_input.bias grad norm: 0.0006091308896429837\n",
            " - layer3.weight grad norm: 0.002679432276636362\n",
            " - layer3.bias grad norm: 0.0005507700843736529\n",
            " - layer3_input.weight grad norm: 0.4787246882915497\n",
            " - layer3_input.bias grad norm: 0.0005507700843736529\n",
            " - layer4.weight grad norm: 0.003177021164447069\n",
            " - layer4.bias grad norm: 0.0005261476035229862\n",
            " - layer4_input.weight grad norm: 0.4553247094154358\n",
            " - layer4_input.bias grad norm: 0.0005261476035229862\n",
            " - layer5.weight grad norm: 0.012234070338308811\n",
            " - layer5.bias grad norm: 0.005471586249768734\n",
            "Gradients at iteration 183:\n",
            " - layer1.weight grad norm: 0.5364593863487244\n",
            " - layer1.bias grad norm: 0.0006233786698430777\n",
            " - layer2.weight grad norm: 0.0018577821319922805\n",
            " - layer2.bias grad norm: 0.0006342134438455105\n",
            " - layer2_input.weight grad norm: 0.544018030166626\n",
            " - layer2_input.bias grad norm: 0.0006342134438455105\n",
            " - layer3.weight grad norm: 0.002560744760558009\n",
            " - layer3.bias grad norm: 0.0005036577931605279\n",
            " - layer3_input.weight grad norm: 0.44534680247306824\n",
            " - layer3_input.bias grad norm: 0.0005036577931605279\n",
            " - layer4.weight grad norm: 0.003095985157415271\n",
            " - layer4.bias grad norm: 0.0005328674451448023\n",
            " - layer4_input.weight grad norm: 0.4666334390640259\n",
            " - layer4_input.bias grad norm: 0.0005328674451448023\n",
            " - layer5.weight grad norm: 0.0112175727263093\n",
            " - layer5.bias grad norm: 0.005230641458183527\n",
            "Gradients at iteration 184:\n",
            " - layer1.weight grad norm: 0.5424478054046631\n",
            " - layer1.bias grad norm: 0.0006284191622398794\n",
            " - layer2.weight grad norm: 0.0019047006499022245\n",
            " - layer2.bias grad norm: 0.0005567707121372223\n",
            " - layer2_input.weight grad norm: 0.48773980140686035\n",
            " - layer2_input.bias grad norm: 0.0005567707121372223\n",
            " - layer3.weight grad norm: 0.0026261883322149515\n",
            " - layer3.bias grad norm: 0.0005785342655144632\n",
            " - layer3_input.weight grad norm: 0.5006269812583923\n",
            " - layer3_input.bias grad norm: 0.0005785342655144632\n",
            " - layer4.weight grad norm: 0.0031273486092686653\n",
            " - layer4.bias grad norm: 0.0005357115878723562\n",
            " - layer4_input.weight grad norm: 0.46587589383125305\n",
            " - layer4_input.bias grad norm: 0.0005357115878723562\n",
            " - layer5.weight grad norm: 0.011888240464031696\n",
            " - layer5.bias grad norm: 0.005356600973755121\n",
            "Gradients at iteration 185:\n",
            " - layer1.weight grad norm: 0.4925377368927002\n",
            " - layer1.bias grad norm: 0.0005623281467705965\n",
            " - layer2.weight grad norm: 0.0019109639106318355\n",
            " - layer2.bias grad norm: 0.00056711450451985\n",
            " - layer2_input.weight grad norm: 0.49243083596229553\n",
            " - layer2_input.bias grad norm: 0.00056711450451985\n",
            " - layer3.weight grad norm: 0.0025970160495489836\n",
            " - layer3.bias grad norm: 0.0006175642483867705\n",
            " - layer3_input.weight grad norm: 0.5318778157234192\n",
            " - layer3_input.bias grad norm: 0.0006175642483867705\n",
            " - layer4.weight grad norm: 0.00314496923238039\n",
            " - layer4.bias grad norm: 0.0005527156754396856\n",
            " - layer4_input.weight grad norm: 0.4815056025981903\n",
            " - layer4_input.bias grad norm: 0.0005527156754396856\n",
            " - layer5.weight grad norm: 0.01119355857372284\n",
            " - layer5.bias grad norm: 0.005383978132158518\n",
            "Gradients at iteration 186:\n",
            " - layer1.weight grad norm: 0.5155747532844543\n",
            " - layer1.bias grad norm: 0.0005934598157182336\n",
            " - layer2.weight grad norm: 0.002004239708185196\n",
            " - layer2.bias grad norm: 0.0005521973944269121\n",
            " - layer2_input.weight grad norm: 0.48541250824928284\n",
            " - layer2_input.bias grad norm: 0.0005521973944269121\n",
            " - layer3.weight grad norm: 0.0027663360815495253\n",
            " - layer3.bias grad norm: 0.0005600950680673122\n",
            " - layer3_input.weight grad norm: 0.48671242594718933\n",
            " - layer3_input.bias grad norm: 0.0005600950680673122\n",
            " - layer4.weight grad norm: 0.0033000714611262083\n",
            " - layer4.bias grad norm: 0.0005928269820287824\n",
            " - layer4_input.weight grad norm: 0.511329174041748\n",
            " - layer4_input.bias grad norm: 0.0005928269820287824\n",
            " - layer5.weight grad norm: 0.012430231086909771\n",
            " - layer5.bias grad norm: 0.005603807047009468\n",
            "Gradients at iteration 187:\n",
            " - layer1.weight grad norm: 0.5366178154945374\n",
            " - layer1.bias grad norm: 0.0006196371396072209\n",
            " - layer2.weight grad norm: 0.001891300198622048\n",
            " - layer2.bias grad norm: 0.0005564362509176135\n",
            " - layer2_input.weight grad norm: 0.4882824420928955\n",
            " - layer2_input.bias grad norm: 0.0005564362509176135\n",
            " - layer3.weight grad norm: 0.002652660943567753\n",
            " - layer3.bias grad norm: 0.0005561570869758725\n",
            " - layer3_input.weight grad norm: 0.48700040578842163\n",
            " - layer3_input.bias grad norm: 0.0005561570869758725\n",
            " - layer4.weight grad norm: 0.003164402674883604\n",
            " - layer4.bias grad norm: 0.0005573208327405155\n",
            " - layer4_input.weight grad norm: 0.4860560894012451\n",
            " - layer4_input.bias grad norm: 0.0005573208327405155\n",
            " - layer5.weight grad norm: 0.012208748608827591\n",
            " - layer5.bias grad norm: 0.005436417181044817\n",
            "Gradients at iteration 188:\n",
            " - layer1.weight grad norm: 0.5218175053596497\n",
            " - layer1.bias grad norm: 0.0006034274701960385\n",
            " - layer2.weight grad norm: 0.001916108769364655\n",
            " - layer2.bias grad norm: 0.0005604675970971584\n",
            " - layer2_input.weight grad norm: 0.4861289858818054\n",
            " - layer2_input.bias grad norm: 0.0005604675970971584\n",
            " - layer3.weight grad norm: 0.002621888881549239\n",
            " - layer3.bias grad norm: 0.0005766135873273015\n",
            " - layer3_input.weight grad norm: 0.4977644979953766\n",
            " - layer3_input.bias grad norm: 0.0005766135873273015\n",
            " - layer4.weight grad norm: 0.0030839971732348204\n",
            " - layer4.bias grad norm: 0.0005752092110924423\n",
            " - layer4_input.weight grad norm: 0.49335047602653503\n",
            " - layer4_input.bias grad norm: 0.0005752092110924423\n",
            " - layer5.weight grad norm: 0.01302067469805479\n",
            " - layer5.bias grad norm: 0.005390118341892958\n",
            "Gradients at iteration 189:\n",
            " - layer1.weight grad norm: 0.5119386911392212\n",
            " - layer1.bias grad norm: 0.0005827455897815526\n",
            " - layer2.weight grad norm: 0.001924974494613707\n",
            " - layer2.bias grad norm: 0.0005653361440636218\n",
            " - layer2_input.weight grad norm: 0.49654620885849\n",
            " - layer2_input.bias grad norm: 0.0005653361440636218\n",
            " - layer3.weight grad norm: 0.002649947302415967\n",
            " - layer3.bias grad norm: 0.0005716991727240384\n",
            " - layer3_input.weight grad norm: 0.49910637736320496\n",
            " - layer3_input.bias grad norm: 0.0005716991727240384\n",
            " - layer4.weight grad norm: 0.0031878056470304728\n",
            " - layer4.bias grad norm: 0.0005670058890245855\n",
            " - layer4_input.weight grad norm: 0.49198997020721436\n",
            " - layer4_input.bias grad norm: 0.0005670058890245855\n",
            " - layer5.weight grad norm: 0.01209446880966425\n",
            " - layer5.bias grad norm: 0.005458165425807238\n",
            "Gradients at iteration 190:\n",
            " - layer1.weight grad norm: 0.5065381526947021\n",
            " - layer1.bias grad norm: 0.0005772909498773515\n",
            " - layer2.weight grad norm: 0.0019289165502414107\n",
            " - layer2.bias grad norm: 0.0005335605819709599\n",
            " - layer2_input.weight grad norm: 0.4687515199184418\n",
            " - layer2_input.bias grad norm: 0.0005335605819709599\n",
            " - layer3.weight grad norm: 0.002630737842991948\n",
            " - layer3.bias grad norm: 0.0005995772080495954\n",
            " - layer3_input.weight grad norm: 0.5212987065315247\n",
            " - layer3_input.bias grad norm: 0.0005995772080495954\n",
            " - layer4.weight grad norm: 0.003127970267087221\n",
            " - layer4.bias grad norm: 0.0005811910377815366\n",
            " - layer4_input.weight grad norm: 0.5017390847206116\n",
            " - layer4_input.bias grad norm: 0.0005811910377815366\n",
            " - layer5.weight grad norm: 0.012013089843094349\n",
            " - layer5.bias grad norm: 0.005440760403871536\n",
            "Gradients at iteration 191:\n",
            " - layer1.weight grad norm: 0.5221734046936035\n",
            " - layer1.bias grad norm: 0.0006051812670193613\n",
            " - layer2.weight grad norm: 0.0019479418406262994\n",
            " - layer2.bias grad norm: 0.00055842031724751\n",
            " - layer2_input.weight grad norm: 0.48662710189819336\n",
            " - layer2_input.bias grad norm: 0.00055842031724751\n",
            " - layer3.weight grad norm: 0.002752043539658189\n",
            " - layer3.bias grad norm: 0.0005660231690853834\n",
            " - layer3_input.weight grad norm: 0.48574358224868774\n",
            " - layer3_input.bias grad norm: 0.0005660231690853834\n",
            " - layer4.weight grad norm: 0.0032866261899471283\n",
            " - layer4.bias grad norm: 0.0005928199389018118\n",
            " - layer4_input.weight grad norm: 0.5043416619300842\n",
            " - layer4_input.bias grad norm: 0.0005928199389018118\n",
            " - layer5.weight grad norm: 0.012868968769907951\n",
            " - layer5.bias grad norm: 0.005607597064226866\n",
            "Gradients at iteration 192:\n",
            " - layer1.weight grad norm: 0.5146223306655884\n",
            " - layer1.bias grad norm: 0.0005949459155090153\n",
            " - layer2.weight grad norm: 0.0018867617473006248\n",
            " - layer2.bias grad norm: 0.0005382016533985734\n",
            " - layer2_input.weight grad norm: 0.47291484475135803\n",
            " - layer2_input.bias grad norm: 0.0005382016533985734\n",
            " - layer3.weight grad norm: 0.002627320820465684\n",
            " - layer3.bias grad norm: 0.0006151613197289407\n",
            " - layer3_input.weight grad norm: 0.5251829624176025\n",
            " - layer3_input.bias grad norm: 0.0006151613197289407\n",
            " - layer4.weight grad norm: 0.0031503355130553246\n",
            " - layer4.bias grad norm: 0.0005571278161369264\n",
            " - layer4_input.weight grad norm: 0.48529335856437683\n",
            " - layer4_input.bias grad norm: 0.0005571278161369264\n",
            " - layer5.weight grad norm: 0.011706987395882607\n",
            " - layer5.bias grad norm: 0.005372657906264067\n",
            "Gradients at iteration 193:\n",
            " - layer1.weight grad norm: 0.5392698645591736\n",
            " - layer1.bias grad norm: 0.0006316440412774682\n",
            " - layer2.weight grad norm: 0.0019556409679353237\n",
            " - layer2.bias grad norm: 0.0005580023862421513\n",
            " - layer2_input.weight grad norm: 0.48589879274368286\n",
            " - layer2_input.bias grad norm: 0.0005580023862421513\n",
            " - layer3.weight grad norm: 0.0028098321054130793\n",
            " - layer3.bias grad norm: 0.0005498899263329804\n",
            " - layer3_input.weight grad norm: 0.4790937602519989\n",
            " - layer3_input.bias grad norm: 0.0005498899263329804\n",
            " - layer4.weight grad norm: 0.0032966607250273228\n",
            " - layer4.bias grad norm: 0.0005733849829994142\n",
            " - layer4_input.weight grad norm: 0.49328550696372986\n",
            " - layer4_input.bias grad norm: 0.0005733849829994142\n",
            " - layer5.weight grad norm: 0.013144196942448616\n",
            " - layer5.bias grad norm: 0.005605696234852076\n",
            "Gradients at iteration 194:\n",
            " - layer1.weight grad norm: 0.502812385559082\n",
            " - layer1.bias grad norm: 0.0005834224866703153\n",
            " - layer2.weight grad norm: 0.0018121051834896207\n",
            " - layer2.bias grad norm: 0.0005877165822312236\n",
            " - layer2_input.weight grad norm: 0.5060403347015381\n",
            " - layer2_input.bias grad norm: 0.0005877165822312236\n",
            " - layer3.weight grad norm: 0.0025168105494230986\n",
            " - layer3.bias grad norm: 0.0005849740700796247\n",
            " - layer3_input.weight grad norm: 0.5011597871780396\n",
            " - layer3_input.bias grad norm: 0.0005849740700796247\n",
            " - layer4.weight grad norm: 0.0030419197864830494\n",
            " - layer4.bias grad norm: 0.0005668060039170086\n",
            " - layer4_input.weight grad norm: 0.4896567761898041\n",
            " - layer4_input.bias grad norm: 0.0005668060039170086\n",
            " - layer5.weight grad norm: 0.01140941958874464\n",
            " - layer5.bias grad norm: 0.0051635196432471275\n",
            "Gradients at iteration 195:\n",
            " - layer1.weight grad norm: 0.5189448595046997\n",
            " - layer1.bias grad norm: 0.0005918251117691398\n",
            " - layer2.weight grad norm: 0.0019224935676902533\n",
            " - layer2.bias grad norm: 0.0005836172495037317\n",
            " - layer2_input.weight grad norm: 0.5141332149505615\n",
            " - layer2_input.bias grad norm: 0.0005836172495037317\n",
            " - layer3.weight grad norm: 0.002618771744892001\n",
            " - layer3.bias grad norm: 0.0005657862056978047\n",
            " - layer3_input.weight grad norm: 0.4911954700946808\n",
            " - layer3_input.bias grad norm: 0.0005657862056978047\n",
            " - layer4.weight grad norm: 0.0031467818189412355\n",
            " - layer4.bias grad norm: 0.000542083231266588\n",
            " - layer4_input.weight grad norm: 0.4742312431335449\n",
            " - layer4_input.bias grad norm: 0.000542083231266588\n",
            " - layer5.weight grad norm: 0.011967080645263195\n",
            " - layer5.bias grad norm: 0.0053904238156974316\n",
            "Gradients at iteration 196:\n",
            " - layer1.weight grad norm: 0.5410170555114746\n",
            " - layer1.bias grad norm: 0.00062950374558568\n",
            " - layer2.weight grad norm: 0.0020021749660372734\n",
            " - layer2.bias grad norm: 0.0005968682235106826\n",
            " - layer2_input.weight grad norm: 0.5116101503372192\n",
            " - layer2_input.bias grad norm: 0.0005968682235106826\n",
            " - layer3.weight grad norm: 0.0027199937030673027\n",
            " - layer3.bias grad norm: 0.0005452324403449893\n",
            " - layer3_input.weight grad norm: 0.4750291109085083\n",
            " - layer3_input.bias grad norm: 0.0005452324403449893\n",
            " - layer4.weight grad norm: 0.003246947890147567\n",
            " - layer4.bias grad norm: 0.000541133398655802\n",
            " - layer4_input.weight grad norm: 0.4687018096446991\n",
            " - layer4_input.bias grad norm: 0.000541133398655802\n",
            " - layer5.weight grad norm: 0.012886464595794678\n",
            " - layer5.bias grad norm: 0.005582808516919613\n",
            "Gradients at iteration 197:\n",
            " - layer1.weight grad norm: 0.4992275834083557\n",
            " - layer1.bias grad norm: 0.0005709487595595419\n",
            " - layer2.weight grad norm: 0.001982953865081072\n",
            " - layer2.bias grad norm: 0.0005345952231436968\n",
            " - layer2_input.weight grad norm: 0.47429534792900085\n",
            " - layer2_input.bias grad norm: 0.0005345952231436968\n",
            " - layer3.weight grad norm: 0.0027284573297947645\n",
            " - layer3.bias grad norm: 0.0006149968830868602\n",
            " - layer3_input.weight grad norm: 0.530504584312439\n",
            " - layer3_input.bias grad norm: 0.0006149968830868602\n",
            " - layer4.weight grad norm: 0.003239766927435994\n",
            " - layer4.bias grad norm: 0.0005665807984769344\n",
            " - layer4_input.weight grad norm: 0.4941107928752899\n",
            " - layer4_input.bias grad norm: 0.0005665807984769344\n",
            " - layer5.weight grad norm: 0.01341803465038538\n",
            " - layer5.bias grad norm: 0.0055640689097344875\n",
            "Gradients at iteration 198:\n",
            " - layer1.weight grad norm: 0.49891597032546997\n",
            " - layer1.bias grad norm: 0.0005737862666137516\n",
            " - layer2.weight grad norm: 0.001899602822959423\n",
            " - layer2.bias grad norm: 0.0006221897201612592\n",
            " - layer2_input.weight grad norm: 0.5346198678016663\n",
            " - layer2_input.bias grad norm: 0.0006221897201612592\n",
            " - layer3.weight grad norm: 0.0026015732437372208\n",
            " - layer3.bias grad norm: 0.0005567729822359979\n",
            " - layer3_input.weight grad norm: 0.481916606426239\n",
            " - layer3_input.bias grad norm: 0.0005567729822359979\n",
            " - layer4.weight grad norm: 0.0030959458090364933\n",
            " - layer4.bias grad norm: 0.0005613239482045174\n",
            " - layer4_input.weight grad norm: 0.48250433802604675\n",
            " - layer4_input.bias grad norm: 0.0005613239482045174\n",
            " - layer5.weight grad norm: 0.012626290321350098\n",
            " - layer5.bias grad norm: 0.005362709052860737\n",
            "Gradients at iteration 199:\n",
            " - layer1.weight grad norm: 0.5456256866455078\n",
            " - layer1.bias grad norm: 0.0006396681419573724\n",
            " - layer2.weight grad norm: 0.0018631480634212494\n",
            " - layer2.bias grad norm: 0.0005496028461493552\n",
            " - layer2_input.weight grad norm: 0.4775172173976898\n",
            " - layer2_input.bias grad norm: 0.0005496028461493552\n",
            " - layer3.weight grad norm: 0.002553436439484358\n",
            " - layer3.bias grad norm: 0.0005696034641005099\n",
            " - layer3_input.weight grad norm: 0.4891987144947052\n",
            " - layer3_input.bias grad norm: 0.0005696034641005099\n",
            " - layer4.weight grad norm: 0.0030636871233582497\n",
            " - layer4.bias grad norm: 0.0005651903338730335\n",
            " - layer4_input.weight grad norm: 0.48453855514526367\n",
            " - layer4_input.bias grad norm: 0.0005651903338730335\n",
            " - layer5.weight grad norm: 0.011311524547636509\n",
            " - layer5.bias grad norm: 0.005225669592618942\n",
            "Gradients at iteration 200:\n",
            " - layer1.weight grad norm: 0.5199437737464905\n",
            " - layer1.bias grad norm: 0.0006038667052052915\n",
            " - layer2.weight grad norm: 0.0018945016199722886\n",
            " - layer2.bias grad norm: 0.000575340585783124\n",
            " - layer2_input.weight grad norm: 0.49824801087379456\n",
            " - layer2_input.bias grad norm: 0.000575340585783124\n",
            " - layer3.weight grad norm: 0.002642751205712557\n",
            " - layer3.bias grad norm: 0.0005743252113461494\n",
            " - layer3_input.weight grad norm: 0.49471786618232727\n",
            " - layer3_input.bias grad norm: 0.0005743252113461494\n",
            " - layer4.weight grad norm: 0.003209882415831089\n",
            " - layer4.bias grad norm: 0.0005664768978022039\n",
            " - layer4_input.weight grad norm: 0.4862937331199646\n",
            " - layer4_input.bias grad norm: 0.0005664768978022039\n",
            " - layer5.weight grad norm: 0.011293459683656693\n",
            " - layer5.bias grad norm: 0.0054026818834245205\n",
            "It: 200, Loss: 5.408e+13, Y0: 0.968, Time: 1.61, Learning Rate: 1.000e-03\n",
            "Gradients at iteration 201:\n",
            " - layer1.weight grad norm: 0.5192460417747498\n",
            " - layer1.bias grad norm: 0.0006030692020431161\n",
            " - layer2.weight grad norm: 0.0019450039835646749\n",
            " - layer2.bias grad norm: 0.0005760314525105059\n",
            " - layer2_input.weight grad norm: 0.49696534872055054\n",
            " - layer2_input.bias grad norm: 0.0005760314525105059\n",
            " - layer3.weight grad norm: 0.0026817487087100744\n",
            " - layer3.bias grad norm: 0.0006130559486337006\n",
            " - layer3_input.weight grad norm: 0.5228678584098816\n",
            " - layer3_input.bias grad norm: 0.0006130559486337006\n",
            " - layer4.weight grad norm: 0.003171671414747834\n",
            " - layer4.bias grad norm: 0.00052353908540681\n",
            " - layer4_input.weight grad norm: 0.4580658972263336\n",
            " - layer4_input.bias grad norm: 0.00052353908540681\n",
            " - layer5.weight grad norm: 0.011854068376123905\n",
            " - layer5.bias grad norm: 0.005481061525642872\n",
            "Gradients at iteration 202:\n",
            " - layer1.weight grad norm: 0.5490580201148987\n",
            " - layer1.bias grad norm: 0.0006393565563485026\n",
            " - layer2.weight grad norm: 0.001961990725249052\n",
            " - layer2.bias grad norm: 0.000533945974893868\n",
            " - layer2_input.weight grad norm: 0.472474068403244\n",
            " - layer2_input.bias grad norm: 0.000533945974893868\n",
            " - layer3.weight grad norm: 0.0027139242738485336\n",
            " - layer3.bias grad norm: 0.0005595135735347867\n",
            " - layer3_input.weight grad norm: 0.4882510006427765\n",
            " - layer3_input.bias grad norm: 0.0005595135735347867\n",
            " - layer4.weight grad norm: 0.003317317459732294\n",
            " - layer4.bias grad norm: 0.0005613144021481276\n",
            " - layer4_input.weight grad norm: 0.4864990711212158\n",
            " - layer4_input.bias grad norm: 0.0005613144021481276\n",
            " - layer5.weight grad norm: 0.013333810493350029\n",
            " - layer5.bias grad norm: 0.00557328574359417\n",
            "Gradients at iteration 203:\n",
            " - layer1.weight grad norm: 0.5133050084114075\n",
            " - layer1.bias grad norm: 0.0005922990967519581\n",
            " - layer2.weight grad norm: 0.001887808321043849\n",
            " - layer2.bias grad norm: 0.0005992948426865041\n",
            " - layer2_input.weight grad norm: 0.5155974626541138\n",
            " - layer2_input.bias grad norm: 0.0005992948426865041\n",
            " - layer3.weight grad norm: 0.0026268463116139174\n",
            " - layer3.bias grad norm: 0.0005231526447460055\n",
            " - layer3_input.weight grad norm: 0.4601556956768036\n",
            " - layer3_input.bias grad norm: 0.0005231526447460055\n",
            " - layer4.weight grad norm: 0.0031150428112596273\n",
            " - layer4.bias grad norm: 0.0005886090220883489\n",
            " - layer4_input.weight grad norm: 0.5086816549301147\n",
            " - layer4_input.bias grad norm: 0.0005886090220883489\n",
            " - layer5.weight grad norm: 0.011230288073420525\n",
            " - layer5.bias grad norm: 0.005313662346452475\n",
            "Gradients at iteration 204:\n",
            " - layer1.weight grad norm: 0.5210081338882446\n",
            " - layer1.bias grad norm: 0.0005981570575386286\n",
            " - layer2.weight grad norm: 0.0019359035650268197\n",
            " - layer2.bias grad norm: 0.0006284728879109025\n",
            " - layer2_input.weight grad norm: 0.5420231223106384\n",
            " - layer2_input.bias grad norm: 0.0006284728879109025\n",
            " - layer3.weight grad norm: 0.0026753440033644438\n",
            " - layer3.bias grad norm: 0.0005393778556026518\n",
            " - layer3_input.weight grad norm: 0.46982625126838684\n",
            " - layer3_input.bias grad norm: 0.0005393778556026518\n",
            " - layer4.weight grad norm: 0.0032201455906033516\n",
            " - layer4.bias grad norm: 0.000527413678355515\n",
            " - layer4_input.weight grad norm: 0.4624209702014923\n",
            " - layer4_input.bias grad norm: 0.000527413678355515\n",
            " - layer5.weight grad norm: 0.011736778542399406\n",
            " - layer5.bias grad norm: 0.005489989649504423\n",
            "Gradients at iteration 205:\n",
            " - layer1.weight grad norm: 0.5736656785011292\n",
            " - layer1.bias grad norm: 0.0006780925905331969\n",
            " - layer2.weight grad norm: 0.002020117361098528\n",
            " - layer2.bias grad norm: 0.0004984975676052272\n",
            " - layer2_input.weight grad norm: 0.44122862815856934\n",
            " - layer2_input.bias grad norm: 0.0004984975676052272\n",
            " - layer3.weight grad norm: 0.0027542004827409983\n",
            " - layer3.bias grad norm: 0.000536580104380846\n",
            " - layer3_input.weight grad norm: 0.4709252715110779\n",
            " - layer3_input.bias grad norm: 0.000536580104380846\n",
            " - layer4.weight grad norm: 0.003319490933790803\n",
            " - layer4.bias grad norm: 0.0005887022125534713\n",
            " - layer4_input.weight grad norm: 0.5042282342910767\n",
            " - layer4_input.bias grad norm: 0.0005887022125534713\n",
            " - layer5.weight grad norm: 0.01230101753026247\n",
            " - layer5.bias grad norm: 0.005658913869410753\n",
            "Gradients at iteration 206:\n",
            " - layer1.weight grad norm: 0.5385159254074097\n",
            " - layer1.bias grad norm: 0.0006211042637005448\n",
            " - layer2.weight grad norm: 0.001870370819233358\n",
            " - layer2.bias grad norm: 0.0005580670549534261\n",
            " - layer2_input.weight grad norm: 0.48979809880256653\n",
            " - layer2_input.bias grad norm: 0.0005580670549534261\n",
            " - layer3.weight grad norm: 0.00259153894148767\n",
            " - layer3.bias grad norm: 0.0005684216157533228\n",
            " - layer3_input.weight grad norm: 0.49829086661338806\n",
            " - layer3_input.bias grad norm: 0.0005684216157533228\n",
            " - layer4.weight grad norm: 0.003106506308540702\n",
            " - layer4.bias grad norm: 0.0005394713953137398\n",
            " - layer4_input.weight grad norm: 0.47071415185928345\n",
            " - layer4_input.bias grad norm: 0.0005394713953137398\n",
            " - layer5.weight grad norm: 0.013497237116098404\n",
            " - layer5.bias grad norm: 0.005330661311745644\n",
            "Gradients at iteration 207:\n",
            " - layer1.weight grad norm: 0.5497919321060181\n",
            " - layer1.bias grad norm: 0.0006401121499948204\n",
            " - layer2.weight grad norm: 0.0018802727572619915\n",
            " - layer2.bias grad norm: 0.0005188197828829288\n",
            " - layer2_input.weight grad norm: 0.46271175146102905\n",
            " - layer2_input.bias grad norm: 0.0005188197828829288\n",
            " - layer3.weight grad norm: 0.00260778795927763\n",
            " - layer3.bias grad norm: 0.0005450327298603952\n",
            " - layer3_input.weight grad norm: 0.47851845622062683\n",
            " - layer3_input.bias grad norm: 0.0005450327298603952\n",
            " - layer4.weight grad norm: 0.00315742171369493\n",
            " - layer4.bias grad norm: 0.0005852364120073617\n",
            " - layer4_input.weight grad norm: 0.5044164061546326\n",
            " - layer4_input.bias grad norm: 0.0005852364120073617\n",
            " - layer5.weight grad norm: 0.012649431824684143\n",
            " - layer5.bias grad norm: 0.005325841251760721\n",
            "Gradients at iteration 208:\n",
            " - layer1.weight grad norm: 0.5377122759819031\n",
            " - layer1.bias grad norm: 0.0006260998779907823\n",
            " - layer2.weight grad norm: 0.001970829674974084\n",
            " - layer2.bias grad norm: 0.0005321109783835709\n",
            " - layer2_input.weight grad norm: 0.468728631734848\n",
            " - layer2_input.bias grad norm: 0.0005321109783835709\n",
            " - layer3.weight grad norm: 0.002682554069906473\n",
            " - layer3.bias grad norm: 0.0005916678346693516\n",
            " - layer3_input.weight grad norm: 0.5086014270782471\n",
            " - layer3_input.bias grad norm: 0.0005916678346693516\n",
            " - layer4.weight grad norm: 0.003215402364730835\n",
            " - layer4.bias grad norm: 0.000559741398319602\n",
            " - layer4_input.weight grad norm: 0.4819517731666565\n",
            " - layer4_input.bias grad norm: 0.000559741398319602\n",
            " - layer5.weight grad norm: 0.012340391986072063\n",
            " - layer5.bias grad norm: 0.005477813072502613\n",
            "Gradients at iteration 209:\n",
            " - layer1.weight grad norm: 0.49817025661468506\n",
            " - layer1.bias grad norm: 0.0005757414037361741\n",
            " - layer2.weight grad norm: 0.0019174811895936728\n",
            " - layer2.bias grad norm: 0.0006085290806367993\n",
            " - layer2_input.weight grad norm: 0.5237165689468384\n",
            " - layer2_input.bias grad norm: 0.0006085290806367993\n",
            " - layer3.weight grad norm: 0.0026860700454562902\n",
            " - layer3.bias grad norm: 0.0005303299403749406\n",
            " - layer3_input.weight grad norm: 0.46403810381889343\n",
            " - layer3_input.bias grad norm: 0.0005303299403749406\n",
            " - layer4.weight grad norm: 0.0031903437338769436\n",
            " - layer4.bias grad norm: 0.0005951754283159971\n",
            " - layer4_input.weight grad norm: 0.5118671655654907\n",
            " - layer4_input.bias grad norm: 0.0005951754283159971\n",
            " - layer5.weight grad norm: 0.012427383102476597\n",
            " - layer5.bias grad norm: 0.005491681396961212\n",
            "Gradients at iteration 210:\n",
            " - layer1.weight grad norm: 0.5225347876548767\n",
            " - layer1.bias grad norm: 0.0006081073079258204\n",
            " - layer2.weight grad norm: 0.0018853252986446023\n",
            " - layer2.bias grad norm: 0.0006145605584606528\n",
            " - layer2_input.weight grad norm: 0.5255643129348755\n",
            " - layer2_input.bias grad norm: 0.0006145605584606528\n",
            " - layer3.weight grad norm: 0.002591267228126526\n",
            " - layer3.bias grad norm: 0.0005699445027858019\n",
            " - layer3_input.weight grad norm: 0.49049443006515503\n",
            " - layer3_input.bias grad norm: 0.0005699445027858019\n",
            " - layer4.weight grad norm: 0.0030771223828196526\n",
            " - layer4.bias grad norm: 0.0005321918288245797\n",
            " - layer4_input.weight grad norm: 0.45822009444236755\n",
            " - layer4_input.bias grad norm: 0.0005321918288245797\n",
            " - layer5.weight grad norm: 0.011792873963713646\n",
            " - layer5.bias grad norm: 0.005286469589918852\n",
            "Gradients at iteration 211:\n",
            " - layer1.weight grad norm: 0.4746082127094269\n",
            " - layer1.bias grad norm: 0.0005352717125788331\n",
            " - layer2.weight grad norm: 0.002077548298984766\n",
            " - layer2.bias grad norm: 0.000581214262638241\n",
            " - layer2_input.weight grad norm: 0.5092839002609253\n",
            " - layer2_input.bias grad norm: 0.000581214262638241\n",
            " - layer3.weight grad norm: 0.0028653759509325027\n",
            " - layer3.bias grad norm: 0.0006392729701474309\n",
            " - layer3_input.weight grad norm: 0.54997718334198\n",
            " - layer3_input.bias grad norm: 0.0006392729701474309\n",
            " - layer4.weight grad norm: 0.003350547282025218\n",
            " - layer4.bias grad norm: 0.0005238416488282382\n",
            " - layer4_input.weight grad norm: 0.4611515998840332\n",
            " - layer4_input.bias grad norm: 0.0005238416488282382\n",
            " - layer5.weight grad norm: 0.013471001759171486\n",
            " - layer5.bias grad norm: 0.0057875122874975204\n",
            "Gradients at iteration 212:\n",
            " - layer1.weight grad norm: 0.5297374725341797\n",
            " - layer1.bias grad norm: 0.0006193186854943633\n",
            " - layer2.weight grad norm: 0.0019501278875395656\n",
            " - layer2.bias grad norm: 0.0005794886965304613\n",
            " - layer2_input.weight grad norm: 0.5027716755867004\n",
            " - layer2_input.bias grad norm: 0.0005794886965304613\n",
            " - layer3.weight grad norm: 0.0026777470484375954\n",
            " - layer3.bias grad norm: 0.0005851987516507506\n",
            " - layer3_input.weight grad norm: 0.5048012733459473\n",
            " - layer3_input.bias grad norm: 0.0005851987516507506\n",
            " - layer4.weight grad norm: 0.003196452045813203\n",
            " - layer4.bias grad norm: 0.000526280200574547\n",
            " - layer4_input.weight grad norm: 0.45998698472976685\n",
            " - layer4_input.bias grad norm: 0.000526280200574547\n",
            " - layer5.weight grad norm: 0.011538687162101269\n",
            " - layer5.bias grad norm: 0.005475159268826246\n",
            "Gradients at iteration 213:\n",
            " - layer1.weight grad norm: 0.5374950170516968\n",
            " - layer1.bias grad norm: 0.0006309628952294588\n",
            " - layer2.weight grad norm: 0.0018438908737152815\n",
            " - layer2.bias grad norm: 0.0005606277263723314\n",
            " - layer2_input.weight grad norm: 0.4838683009147644\n",
            " - layer2_input.bias grad norm: 0.0005606277263723314\n",
            " - layer3.weight grad norm: 0.002569302450865507\n",
            " - layer3.bias grad norm: 0.0005846327403560281\n",
            " - layer3_input.weight grad norm: 0.5014246702194214\n",
            " - layer3_input.bias grad norm: 0.0005846327403560281\n",
            " - layer4.weight grad norm: 0.003036293899640441\n",
            " - layer4.bias grad norm: 0.000553866324480623\n",
            " - layer4_input.weight grad norm: 0.4747221767902374\n",
            " - layer4_input.bias grad norm: 0.000553866324480623\n",
            " - layer5.weight grad norm: 0.01158132590353489\n",
            " - layer5.bias grad norm: 0.005201756022870541\n",
            "Gradients at iteration 214:\n",
            " - layer1.weight grad norm: 0.5159239768981934\n",
            " - layer1.bias grad norm: 0.0005961243878118694\n",
            " - layer2.weight grad norm: 0.0018367680022493005\n",
            " - layer2.bias grad norm: 0.0006088153459131718\n",
            " - layer2_input.weight grad norm: 0.5234312415122986\n",
            " - layer2_input.bias grad norm: 0.0006088153459131718\n",
            " - layer3.weight grad norm: 0.0025543493684381247\n",
            " - layer3.bias grad norm: 0.0006160939228720963\n",
            " - layer3_input.weight grad norm: 0.528653085231781\n",
            " - layer3_input.bias grad norm: 0.0006160939228720963\n",
            " - layer4.weight grad norm: 0.0030710601713508368\n",
            " - layer4.bias grad norm: 0.00048313423758372664\n",
            " - layer4_input.weight grad norm: 0.4244673550128937\n",
            " - layer4_input.bias grad norm: 0.00048313423758372664\n",
            " - layer5.weight grad norm: 0.01209349650889635\n",
            " - layer5.bias grad norm: 0.005245281849056482\n",
            "Gradients at iteration 215:\n",
            " - layer1.weight grad norm: 0.5313834547996521\n",
            " - layer1.bias grad norm: 0.0006213537999428809\n",
            " - layer2.weight grad norm: 0.0019156396156176925\n",
            " - layer2.bias grad norm: 0.0005695305881090462\n",
            " - layer2_input.weight grad norm: 0.4932714104652405\n",
            " - layer2_input.bias grad norm: 0.0005695305881090462\n",
            " - layer3.weight grad norm: 0.002645641565322876\n",
            " - layer3.bias grad norm: 0.0005528377369046211\n",
            " - layer3_input.weight grad norm: 0.47994548082351685\n",
            " - layer3_input.bias grad norm: 0.0005528377369046211\n",
            " - layer4.weight grad norm: 0.0031467571388930082\n",
            " - layer4.bias grad norm: 0.0005727876559831202\n",
            " - layer4_input.weight grad norm: 0.4937325119972229\n",
            " - layer4_input.bias grad norm: 0.0005727876559831202\n",
            " - layer5.weight grad norm: 0.011975876986980438\n",
            " - layer5.bias grad norm: 0.005396535620093346\n",
            "Gradients at iteration 216:\n",
            " - layer1.weight grad norm: 0.4804421365261078\n",
            " - layer1.bias grad norm: 0.000553919468075037\n",
            " - layer2.weight grad norm: 0.001919330796226859\n",
            " - layer2.bias grad norm: 0.0006272626342251897\n",
            " - layer2_input.weight grad norm: 0.5335444808006287\n",
            " - layer2_input.bias grad norm: 0.0006272626342251897\n",
            " - layer3.weight grad norm: 0.002724273595958948\n",
            " - layer3.bias grad norm: 0.0005761671345680952\n",
            " - layer3_input.weight grad norm: 0.49476417899131775\n",
            " - layer3_input.bias grad norm: 0.0005761671345680952\n",
            " - layer4.weight grad norm: 0.003230262780562043\n",
            " - layer4.bias grad norm: 0.0005721669876947999\n",
            " - layer4_input.weight grad norm: 0.4893718361854553\n",
            " - layer4_input.bias grad norm: 0.0005721669876947999\n",
            " - layer5.weight grad norm: 0.013234000653028488\n",
            " - layer5.bias grad norm: 0.0054954360239207745\n",
            "Gradients at iteration 217:\n",
            " - layer1.weight grad norm: 0.5215260982513428\n",
            " - layer1.bias grad norm: 0.0006054570549167693\n",
            " - layer2.weight grad norm: 0.00186545227188617\n",
            " - layer2.bias grad norm: 0.0006046199705451727\n",
            " - layer2_input.weight grad norm: 0.5214934349060059\n",
            " - layer2_input.bias grad norm: 0.0006046199705451727\n",
            " - layer3.weight grad norm: 0.002592244418337941\n",
            " - layer3.bias grad norm: 0.000559212698135525\n",
            " - layer3_input.weight grad norm: 0.4800831079483032\n",
            " - layer3_input.bias grad norm: 0.000559212698135525\n",
            " - layer4.weight grad norm: 0.003099376568570733\n",
            " - layer4.bias grad norm: 0.0005472292541526258\n",
            " - layer4_input.weight grad norm: 0.4747461676597595\n",
            " - layer4_input.bias grad norm: 0.0005472292541526258\n",
            " - layer5.weight grad norm: 0.011877025477588177\n",
            " - layer5.bias grad norm: 0.005328735336661339\n",
            "Gradients at iteration 218:\n",
            " - layer1.weight grad norm: 0.4897025525569916\n",
            " - layer1.bias grad norm: 0.0005608955398201942\n",
            " - layer2.weight grad norm: 0.001976029248908162\n",
            " - layer2.bias grad norm: 0.0006019914289936423\n",
            " - layer2_input.weight grad norm: 0.5210540294647217\n",
            " - layer2_input.bias grad norm: 0.0006019914289936423\n",
            " - layer3.weight grad norm: 0.002720485208556056\n",
            " - layer3.bias grad norm: 0.0005786580150015652\n",
            " - layer3_input.weight grad norm: 0.5038752555847168\n",
            " - layer3_input.bias grad norm: 0.0005786580150015652\n",
            " - layer4.weight grad norm: 0.003237872151657939\n",
            " - layer4.bias grad norm: 0.0005602382589131594\n",
            " - layer4_input.weight grad norm: 0.48432546854019165\n",
            " - layer4_input.bias grad norm: 0.0005602382589131594\n",
            " - layer5.weight grad norm: 0.013341889716684818\n",
            " - layer5.bias grad norm: 0.005546115804463625\n",
            "Gradients at iteration 219:\n",
            " - layer1.weight grad norm: 0.5310763120651245\n",
            " - layer1.bias grad norm: 0.00061495864065364\n",
            " - layer2.weight grad norm: 0.0018944338662549853\n",
            " - layer2.bias grad norm: 0.0006106409127824008\n",
            " - layer2_input.weight grad norm: 0.5208780169487\n",
            " - layer2_input.bias grad norm: 0.0006106409127824008\n",
            " - layer3.weight grad norm: 0.0025918863248080015\n",
            " - layer3.bias grad norm: 0.0005851007299497724\n",
            " - layer3_input.weight grad norm: 0.5028643608093262\n",
            " - layer3_input.bias grad norm: 0.0005851007299497724\n",
            " - layer4.weight grad norm: 0.0030679749324917793\n",
            " - layer4.bias grad norm: 0.0004970395821146667\n",
            " - layer4_input.weight grad norm: 0.4399610757827759\n",
            " - layer4_input.bias grad norm: 0.0004970395821146667\n",
            " - layer5.weight grad norm: 0.012467601336538792\n",
            " - layer5.bias grad norm: 0.005319445859640837\n",
            "Gradients at iteration 220:\n",
            " - layer1.weight grad norm: 0.5124250650405884\n",
            " - layer1.bias grad norm: 0.0005916344234719872\n",
            " - layer2.weight grad norm: 0.0019369706278666854\n",
            " - layer2.bias grad norm: 0.0006535075954161584\n",
            " - layer2_input.weight grad norm: 0.5545194149017334\n",
            " - layer2_input.bias grad norm: 0.0006535075954161584\n",
            " - layer3.weight grad norm: 0.0026748531963676214\n",
            " - layer3.bias grad norm: 0.0005439764354377985\n",
            " - layer3_input.weight grad norm: 0.4731264114379883\n",
            " - layer3_input.bias grad norm: 0.0005439764354377985\n",
            " - layer4.weight grad norm: 0.003121099201962352\n",
            " - layer4.bias grad norm: 0.0005263122729957104\n",
            " - layer4_input.weight grad norm: 0.4537375271320343\n",
            " - layer4_input.bias grad norm: 0.0005263122729957104\n",
            " - layer5.weight grad norm: 0.012250701896846294\n",
            " - layer5.bias grad norm: 0.005427345633506775\n",
            "Gradients at iteration 221:\n",
            " - layer1.weight grad norm: 0.5104395151138306\n",
            " - layer1.bias grad norm: 0.000589140981901437\n",
            " - layer2.weight grad norm: 0.0019460461335256696\n",
            " - layer2.bias grad norm: 0.0006001022411510348\n",
            " - layer2_input.weight grad norm: 0.5163602828979492\n",
            " - layer2_input.bias grad norm: 0.0006001022411510348\n",
            " - layer3.weight grad norm: 0.002652232302352786\n",
            " - layer3.bias grad norm: 0.0005563728627748787\n",
            " - layer3_input.weight grad norm: 0.4852627217769623\n",
            " - layer3_input.bias grad norm: 0.0005563728627748787\n",
            " - layer4.weight grad norm: 0.0032497053034603596\n",
            " - layer4.bias grad norm: 0.0005622401367872953\n",
            " - layer4_input.weight grad norm: 0.4869702458381653\n",
            " - layer4_input.bias grad norm: 0.0005622401367872953\n",
            " - layer5.weight grad norm: 0.012235074304044247\n",
            " - layer5.bias grad norm: 0.005494105629622936\n",
            "Gradients at iteration 222:\n",
            " - layer1.weight grad norm: 0.5295621156692505\n",
            " - layer1.bias grad norm: 0.0006224283133633435\n",
            " - layer2.weight grad norm: 0.001869176747277379\n",
            " - layer2.bias grad norm: 0.0006227159174159169\n",
            " - layer2_input.weight grad norm: 0.5306922197341919\n",
            " - layer2_input.bias grad norm: 0.0006227159174159169\n",
            " - layer3.weight grad norm: 0.002582904417067766\n",
            " - layer3.bias grad norm: 0.0005317109171301126\n",
            " - layer3_input.weight grad norm: 0.46111515164375305\n",
            " - layer3_input.bias grad norm: 0.0005317109171301126\n",
            " - layer4.weight grad norm: 0.003079351270571351\n",
            " - layer4.bias grad norm: 0.0005564109887927771\n",
            " - layer4_input.weight grad norm: 0.4744414687156677\n",
            " - layer4_input.bias grad norm: 0.0005564109887927771\n",
            " - layer5.weight grad norm: 0.01255385484546423\n",
            " - layer5.bias grad norm: 0.005310433451086283\n",
            "Gradients at iteration 223:\n",
            " - layer1.weight grad norm: 0.46259939670562744\n",
            " - layer1.bias grad norm: 0.0005285160732455552\n",
            " - layer2.weight grad norm: 0.0018493577372282743\n",
            " - layer2.bias grad norm: 0.0005763546796515584\n",
            " - layer2_input.weight grad norm: 0.49728184938430786\n",
            " - layer2_input.bias grad norm: 0.0005763546796515584\n",
            " - layer3.weight grad norm: 0.0025903477799147367\n",
            " - layer3.bias grad norm: 0.0006757259834557772\n",
            " - layer3_input.weight grad norm: 0.5657991766929626\n",
            " - layer3_input.bias grad norm: 0.0006757259834557772\n",
            " - layer4.weight grad norm: 0.0030513915698975325\n",
            " - layer4.bias grad norm: 0.0005456790095195174\n",
            " - layer4_input.weight grad norm: 0.4673222005367279\n",
            " - layer4_input.bias grad norm: 0.0005456790095195174\n",
            " - layer5.weight grad norm: 0.012028565630316734\n",
            " - layer5.bias grad norm: 0.005224083084613085\n",
            "Gradients at iteration 224:\n",
            " - layer1.weight grad norm: 0.5194899439811707\n",
            " - layer1.bias grad norm: 0.0005984561867080629\n",
            " - layer2.weight grad norm: 0.0019064252264797688\n",
            " - layer2.bias grad norm: 0.0005540376878343523\n",
            " - layer2_input.weight grad norm: 0.4856177270412445\n",
            " - layer2_input.bias grad norm: 0.0005540376878343523\n",
            " - layer3.weight grad norm: 0.0026206686161458492\n",
            " - layer3.bias grad norm: 0.000586671638302505\n",
            " - layer3_input.weight grad norm: 0.5032075047492981\n",
            " - layer3_input.bias grad norm: 0.000586671638302505\n",
            " - layer4.weight grad norm: 0.00313448254019022\n",
            " - layer4.bias grad norm: 0.0005718132015317678\n",
            " - layer4_input.weight grad norm: 0.49079716205596924\n",
            " - layer4_input.bias grad norm: 0.0005718132015317678\n",
            " - layer5.weight grad norm: 0.012426990084350109\n",
            " - layer5.bias grad norm: 0.005364762153476477\n",
            "Gradients at iteration 225:\n",
            " - layer1.weight grad norm: 0.5243927240371704\n",
            " - layer1.bias grad norm: 0.0006058044964447618\n",
            " - layer2.weight grad norm: 0.0019613553304225206\n",
            " - layer2.bias grad norm: 0.0006006766925565898\n",
            " - layer2_input.weight grad norm: 0.5175194144248962\n",
            " - layer2_input.bias grad norm: 0.0006006766925565898\n",
            " - layer3.weight grad norm: 0.0026638777926564217\n",
            " - layer3.bias grad norm: 0.0005368160200305283\n",
            " - layer3_input.weight grad norm: 0.47081682085990906\n",
            " - layer3_input.bias grad norm: 0.0005368160200305283\n",
            " - layer4.weight grad norm: 0.003202928928658366\n",
            " - layer4.bias grad norm: 0.000555589038413018\n",
            " - layer4_input.weight grad norm: 0.4850577712059021\n",
            " - layer4_input.bias grad norm: 0.000555589038413018\n",
            " - layer5.weight grad norm: 0.01352603081613779\n",
            " - layer5.bias grad norm: 0.00546500226482749\n",
            "Gradients at iteration 226:\n",
            " - layer1.weight grad norm: 0.5335599184036255\n",
            " - layer1.bias grad norm: 0.0006220797076821327\n",
            " - layer2.weight grad norm: 0.001859787036664784\n",
            " - layer2.bias grad norm: 0.0006066198693588376\n",
            " - layer2_input.weight grad norm: 0.5200106501579285\n",
            " - layer2_input.bias grad norm: 0.0006066198693588376\n",
            " - layer3.weight grad norm: 0.0026007520500570536\n",
            " - layer3.bias grad norm: 0.0005512668867595494\n",
            " - layer3_input.weight grad norm: 0.4785890281200409\n",
            " - layer3_input.bias grad norm: 0.0005512668867595494\n",
            " - layer4.weight grad norm: 0.0031283164862543344\n",
            " - layer4.bias grad norm: 0.0005331362481229007\n",
            " - layer4_input.weight grad norm: 0.46438074111938477\n",
            " - layer4_input.bias grad norm: 0.0005331362481229007\n",
            " - layer5.weight grad norm: 0.012442241422832012\n",
            " - layer5.bias grad norm: 0.00534192705526948\n",
            "Gradients at iteration 227:\n",
            " - layer1.weight grad norm: 0.5295288562774658\n",
            " - layer1.bias grad norm: 0.0006211218424141407\n",
            " - layer2.weight grad norm: 0.0018422851571813226\n",
            " - layer2.bias grad norm: 0.0006037081475369632\n",
            " - layer2_input.weight grad norm: 0.5141173005104065\n",
            " - layer2_input.bias grad norm: 0.0006037081475369632\n",
            " - layer3.weight grad norm: 0.002609182382002473\n",
            " - layer3.bias grad norm: 0.0005397217464633286\n",
            " - layer3_input.weight grad norm: 0.46655943989753723\n",
            " - layer3_input.bias grad norm: 0.0005397217464633286\n",
            " - layer4.weight grad norm: 0.0030710031278431416\n",
            " - layer4.bias grad norm: 0.0005700773908756673\n",
            " - layer4_input.weight grad norm: 0.48723381757736206\n",
            " - layer4_input.bias grad norm: 0.0005700773908756673\n",
            " - layer5.weight grad norm: 0.012583961710333824\n",
            " - layer5.bias grad norm: 0.005267446860671043\n",
            "Gradients at iteration 228:\n",
            " - layer1.weight grad norm: 0.5114104151725769\n",
            " - layer1.bias grad norm: 0.0005926321027800441\n",
            " - layer2.weight grad norm: 0.0019668452441692352\n",
            " - layer2.bias grad norm: 0.000574121717363596\n",
            " - layer2_input.weight grad norm: 0.4984210133552551\n",
            " - layer2_input.bias grad norm: 0.000574121717363596\n",
            " - layer3.weight grad norm: 0.002679665107280016\n",
            " - layer3.bias grad norm: 0.0005871515604667366\n",
            " - layer3_input.weight grad norm: 0.507226288318634\n",
            " - layer3_input.bias grad norm: 0.0005871515604667366\n",
            " - layer4.weight grad norm: 0.003220567712560296\n",
            " - layer4.bias grad norm: 0.0005582799203693867\n",
            " - layer4_input.weight grad norm: 0.4822257161140442\n",
            " - layer4_input.bias grad norm: 0.0005582799203693867\n",
            " - layer5.weight grad norm: 0.012698335573077202\n",
            " - layer5.bias grad norm: 0.005551327019929886\n",
            "Gradients at iteration 229:\n",
            " - layer1.weight grad norm: 0.4883306622505188\n",
            " - layer1.bias grad norm: 0.0005649391678161919\n",
            " - layer2.weight grad norm: 0.0019457737216725945\n",
            " - layer2.bias grad norm: 0.0005927134188823402\n",
            " - layer2_input.weight grad norm: 0.5078144669532776\n",
            " - layer2_input.bias grad norm: 0.0005927134188823402\n",
            " - layer3.weight grad norm: 0.002653310773894191\n",
            " - layer3.bias grad norm: 0.0006086339126341045\n",
            " - layer3_input.weight grad norm: 0.5187433958053589\n",
            " - layer3_input.bias grad norm: 0.0006086339126341045\n",
            " - layer4.weight grad norm: 0.003196116304025054\n",
            " - layer4.bias grad norm: 0.0005601679440587759\n",
            " - layer4_input.weight grad norm: 0.484109491109848\n",
            " - layer4_input.bias grad norm: 0.0005601679440587759\n",
            " - layer5.weight grad norm: 0.01213480718433857\n",
            " - layer5.bias grad norm: 0.005482482258230448\n",
            "Gradients at iteration 230:\n",
            " - layer1.weight grad norm: 0.5229508280754089\n",
            " - layer1.bias grad norm: 0.0006043227040208876\n",
            " - layer2.weight grad norm: 0.001938236877322197\n",
            " - layer2.bias grad norm: 0.0005684206262230873\n",
            " - layer2_input.weight grad norm: 0.49249470233917236\n",
            " - layer2_input.bias grad norm: 0.0005684206262230873\n",
            " - layer3.weight grad norm: 0.002652378287166357\n",
            " - layer3.bias grad norm: 0.0005740677588619292\n",
            " - layer3_input.weight grad norm: 0.4983936846256256\n",
            " - layer3_input.bias grad norm: 0.0005740677588619292\n",
            " - layer4.weight grad norm: 0.003183557651937008\n",
            " - layer4.bias grad norm: 0.000566971255466342\n",
            " - layer4_input.weight grad norm: 0.4851479232311249\n",
            " - layer4_input.bias grad norm: 0.000566971255466342\n",
            " - layer5.weight grad norm: 0.012393574230372906\n",
            " - layer5.bias grad norm: 0.005455659236758947\n",
            "Gradients at iteration 231:\n",
            " - layer1.weight grad norm: 0.5096009969711304\n",
            " - layer1.bias grad norm: 0.0005866067367605865\n",
            " - layer2.weight grad norm: 0.001938713132403791\n",
            " - layer2.bias grad norm: 0.0006186337559483945\n",
            " - layer2_input.weight grad norm: 0.5292682647705078\n",
            " - layer2_input.bias grad norm: 0.0006186337559483945\n",
            " - layer3.weight grad norm: 0.002618065569549799\n",
            " - layer3.bias grad norm: 0.0005567226908169687\n",
            " - layer3_input.weight grad norm: 0.4836631715297699\n",
            " - layer3_input.bias grad norm: 0.0005567226908169687\n",
            " - layer4.weight grad norm: 0.0031409242656081915\n",
            " - layer4.bias grad norm: 0.0005496694939211011\n",
            " - layer4_input.weight grad norm: 0.47544699907302856\n",
            " - layer4_input.bias grad norm: 0.0005496694939211011\n",
            " - layer5.weight grad norm: 0.012230683118104935\n",
            " - layer5.bias grad norm: 0.00545782083645463\n",
            "Gradients at iteration 232:\n",
            " - layer1.weight grad norm: 0.5034465193748474\n",
            " - layer1.bias grad norm: 0.0005890383035875857\n",
            " - layer2.weight grad norm: 0.001826609019190073\n",
            " - layer2.bias grad norm: 0.0005752247525379062\n",
            " - layer2_input.weight grad norm: 0.4969082176685333\n",
            " - layer2_input.bias grad norm: 0.0005752247525379062\n",
            " - layer3.weight grad norm: 0.002528625074774027\n",
            " - layer3.bias grad norm: 0.0005440046661533415\n",
            " - layer3_input.weight grad norm: 0.469249963760376\n",
            " - layer3_input.bias grad norm: 0.0005440046661533415\n",
            " - layer4.weight grad norm: 0.0030172369442880154\n",
            " - layer4.bias grad norm: 0.0006235433393158019\n",
            " - layer4_input.weight grad norm: 0.5284264087677002\n",
            " - layer4_input.bias grad norm: 0.0006235433393158019\n",
            " - layer5.weight grad norm: 0.012071718461811543\n",
            " - layer5.bias grad norm: 0.0051930369809269905\n",
            "Gradients at iteration 233:\n",
            " - layer1.weight grad norm: 0.5137889385223389\n",
            " - layer1.bias grad norm: 0.0005955062224529684\n",
            " - layer2.weight grad norm: 0.0019814663100987673\n",
            " - layer2.bias grad norm: 0.0005997854168526828\n",
            " - layer2_input.weight grad norm: 0.5137237310409546\n",
            " - layer2_input.bias grad norm: 0.0005997854168526828\n",
            " - layer3.weight grad norm: 0.0027542621828615665\n",
            " - layer3.bias grad norm: 0.00055175821762532\n",
            " - layer3_input.weight grad norm: 0.4795767664909363\n",
            " - layer3_input.bias grad norm: 0.00055175821762532\n",
            " - layer4.weight grad norm: 0.003274823073297739\n",
            " - layer4.bias grad norm: 0.0005722096539102495\n",
            " - layer4_input.weight grad norm: 0.4918384552001953\n",
            " - layer4_input.bias grad norm: 0.0005722096539102495\n",
            " - layer5.weight grad norm: 0.012389939278364182\n",
            " - layer5.bias grad norm: 0.005648958031088114\n",
            "Gradients at iteration 234:\n",
            " - layer1.weight grad norm: 0.5532243847846985\n",
            " - layer1.bias grad norm: 0.0006500052404589951\n",
            " - layer2.weight grad norm: 0.0018009439809247851\n",
            " - layer2.bias grad norm: 0.0006195610039867461\n",
            " - layer2_input.weight grad norm: 0.5250591039657593\n",
            " - layer2_input.bias grad norm: 0.0006195610039867461\n",
            " - layer3.weight grad norm: 0.002566344104707241\n",
            " - layer3.bias grad norm: 0.0005346792167983949\n",
            " - layer3_input.weight grad norm: 0.46039509773254395\n",
            " - layer3_input.bias grad norm: 0.0005346792167983949\n",
            " - layer4.weight grad norm: 0.0029697902500629425\n",
            " - layer4.bias grad norm: 0.0005272724665701389\n",
            " - layer4_input.weight grad norm: 0.4539875388145447\n",
            " - layer4_input.bias grad norm: 0.0005272724665701389\n",
            " - layer5.weight grad norm: 0.01181095652282238\n",
            " - layer5.bias grad norm: 0.005194685887545347\n",
            "Gradients at iteration 235:\n",
            " - layer1.weight grad norm: 0.5468609929084778\n",
            " - layer1.bias grad norm: 0.0006406164029613137\n",
            " - layer2.weight grad norm: 0.0019306304166093469\n",
            " - layer2.bias grad norm: 0.0005957785760983825\n",
            " - layer2_input.weight grad norm: 0.5124109983444214\n",
            " - layer2_input.bias grad norm: 0.0005957785760983825\n",
            " - layer3.weight grad norm: 0.0026809668634086847\n",
            " - layer3.bias grad norm: 0.000526929390616715\n",
            " - layer3_input.weight grad norm: 0.45765355229377747\n",
            " - layer3_input.bias grad norm: 0.000526929390616715\n",
            " - layer4.weight grad norm: 0.003188694827258587\n",
            " - layer4.bias grad norm: 0.0005574868409894407\n",
            " - layer4_input.weight grad norm: 0.4782341718673706\n",
            " - layer4_input.bias grad norm: 0.0005574868409894407\n",
            " - layer5.weight grad norm: 0.013028676621615887\n",
            " - layer5.bias grad norm: 0.0054812622256577015\n",
            "Gradients at iteration 236:\n",
            " - layer1.weight grad norm: 0.5322659611701965\n",
            " - layer1.bias grad norm: 0.0006182415527291596\n",
            " - layer2.weight grad norm: 0.0020035768393427134\n",
            " - layer2.bias grad norm: 0.0005980567657388747\n",
            " - layer2_input.weight grad norm: 0.5161610245704651\n",
            " - layer2_input.bias grad norm: 0.0005980567657388747\n",
            " - layer3.weight grad norm: 0.0028052134439349174\n",
            " - layer3.bias grad norm: 0.0005335606983862817\n",
            " - layer3_input.weight grad norm: 0.4688512086868286\n",
            " - layer3_input.bias grad norm: 0.0005335606983862817\n",
            " - layer4.weight grad norm: 0.003312329761683941\n",
            " - layer4.bias grad norm: 0.0005478064413182437\n",
            " - layer4_input.weight grad norm: 0.4798169732093811\n",
            " - layer4_input.bias grad norm: 0.0005478064413182437\n",
            " - layer5.weight grad norm: 0.012949593365192413\n",
            " - layer5.bias grad norm: 0.005674711428582668\n",
            "Gradients at iteration 237:\n",
            " - layer1.weight grad norm: 0.46098634600639343\n",
            " - layer1.bias grad norm: 0.0005258356686681509\n",
            " - layer2.weight grad norm: 0.0019014221616089344\n",
            " - layer2.bias grad norm: 0.0005481129046529531\n",
            " - layer2_input.weight grad norm: 0.476703941822052\n",
            " - layer2_input.bias grad norm: 0.0005481129046529531\n",
            " - layer3.weight grad norm: 0.0026715793646872044\n",
            " - layer3.bias grad norm: 0.0006482586031779647\n",
            " - layer3_input.weight grad norm: 0.5484285354614258\n",
            " - layer3_input.bias grad norm: 0.0006482586031779647\n",
            " - layer4.weight grad norm: 0.003155158832669258\n",
            " - layer4.bias grad norm: 0.0005984047893434763\n",
            " - layer4_input.weight grad norm: 0.5091986656188965\n",
            " - layer4_input.bias grad norm: 0.0005984047893434763\n",
            " - layer5.weight grad norm: 0.011631437577307224\n",
            " - layer5.bias grad norm: 0.005422820337116718\n",
            "Gradients at iteration 238:\n",
            " - layer1.weight grad norm: 0.5245873332023621\n",
            " - layer1.bias grad norm: 0.0006051505915820599\n",
            " - layer2.weight grad norm: 0.0019118316704407334\n",
            " - layer2.bias grad norm: 0.0005657186848111451\n",
            " - layer2_input.weight grad norm: 0.4932873249053955\n",
            " - layer2_input.bias grad norm: 0.0005657186848111451\n",
            " - layer3.weight grad norm: 0.0026779314503073692\n",
            " - layer3.bias grad norm: 0.0005799542996101081\n",
            " - layer3_input.weight grad norm: 0.5038446187973022\n",
            " - layer3_input.bias grad norm: 0.0005799542996101081\n",
            " - layer4.weight grad norm: 0.003184305503964424\n",
            " - layer4.bias grad norm: 0.0005484459106810391\n",
            " - layer4_input.weight grad norm: 0.4768778085708618\n",
            " - layer4_input.bias grad norm: 0.0005484459106810391\n",
            " - layer5.weight grad norm: 0.012278665788471699\n",
            " - layer5.bias grad norm: 0.005481609143316746\n",
            "Gradients at iteration 239:\n",
            " - layer1.weight grad norm: 0.5477284789085388\n",
            " - layer1.bias grad norm: 0.000644079816993326\n",
            " - layer2.weight grad norm: 0.0018638749606907368\n",
            " - layer2.bias grad norm: 0.00055359594989568\n",
            " - layer2_input.weight grad norm: 0.47652968764305115\n",
            " - layer2_input.bias grad norm: 0.00055359594989568\n",
            " - layer3.weight grad norm: 0.0026042875833809376\n",
            " - layer3.bias grad norm: 0.0005613728426396847\n",
            " - layer3_input.weight grad norm: 0.4829060137271881\n",
            " - layer3_input.bias grad norm: 0.0005613728426396847\n",
            " - layer4.weight grad norm: 0.0031370967626571655\n",
            " - layer4.bias grad norm: 0.0005752231809310615\n",
            " - layer4_input.weight grad norm: 0.4893927276134491\n",
            " - layer4_input.bias grad norm: 0.0005752231809310615\n",
            " - layer5.weight grad norm: 0.012605253607034683\n",
            " - layer5.bias grad norm: 0.005309324245899916\n",
            "Gradients at iteration 240:\n",
            " - layer1.weight grad norm: 0.5170722007751465\n",
            " - layer1.bias grad norm: 0.0006003307644277811\n",
            " - layer2.weight grad norm: 0.001874328125268221\n",
            " - layer2.bias grad norm: 0.0006225554971024394\n",
            " - layer2_input.weight grad norm: 0.532839298248291\n",
            " - layer2_input.bias grad norm: 0.0006225554971024394\n",
            " - layer3.weight grad norm: 0.0026378333568573\n",
            " - layer3.bias grad norm: 0.0005247691296972334\n",
            " - layer3_input.weight grad norm: 0.4599316716194153\n",
            " - layer3_input.bias grad norm: 0.0005247691296972334\n",
            " - layer4.weight grad norm: 0.0031357065308839083\n",
            " - layer4.bias grad norm: 0.0005601190496236086\n",
            " - layer4_input.weight grad norm: 0.48681187629699707\n",
            " - layer4_input.bias grad norm: 0.0005601190496236086\n",
            " - layer5.weight grad norm: 0.012014785781502724\n",
            " - layer5.bias grad norm: 0.005353052169084549\n",
            "Gradients at iteration 241:\n",
            " - layer1.weight grad norm: 0.5097975730895996\n",
            " - layer1.bias grad norm: 0.0005861003301106393\n",
            " - layer2.weight grad norm: 0.001968919299542904\n",
            " - layer2.bias grad norm: 0.0006291982135735452\n",
            " - layer2_input.weight grad norm: 0.5391017198562622\n",
            " - layer2_input.bias grad norm: 0.0006291982135735452\n",
            " - layer3.weight grad norm: 0.0027463457081466913\n",
            " - layer3.bias grad norm: 0.0005907212616875768\n",
            " - layer3_input.weight grad norm: 0.5081433653831482\n",
            " - layer3_input.bias grad norm: 0.0005907212616875768\n",
            " - layer4.weight grad norm: 0.0032130384352058172\n",
            " - layer4.bias grad norm: 0.000491163635160774\n",
            " - layer4_input.weight grad norm: 0.43707242608070374\n",
            " - layer4_input.bias grad norm: 0.000491163635160774\n",
            " - layer5.weight grad norm: 0.013366385363042355\n",
            " - layer5.bias grad norm: 0.005572666879743338\n",
            "Gradients at iteration 242:\n",
            " - layer1.weight grad norm: 0.5180264711380005\n",
            " - layer1.bias grad norm: 0.0005965311429463327\n",
            " - layer2.weight grad norm: 0.001985702896490693\n",
            " - layer2.bias grad norm: 0.0006305211572907865\n",
            " - layer2_input.weight grad norm: 0.5380343198776245\n",
            " - layer2_input.bias grad norm: 0.0006305211572907865\n",
            " - layer3.weight grad norm: 0.0027667037211358547\n",
            " - layer3.bias grad norm: 0.0005186687340028584\n",
            " - layer3_input.weight grad norm: 0.45815509557724\n",
            " - layer3_input.bias grad norm: 0.0005186687340028584\n",
            " - layer4.weight grad norm: 0.0033277093898504972\n",
            " - layer4.bias grad norm: 0.0005532742943614721\n",
            " - layer4_input.weight grad norm: 0.48170700669288635\n",
            " - layer4_input.bias grad norm: 0.0005532742943614721\n",
            " - layer5.weight grad norm: 0.012776273302733898\n",
            " - layer5.bias grad norm: 0.005627356003969908\n",
            "Gradients at iteration 243:\n",
            " - layer1.weight grad norm: 0.4824267029762268\n",
            " - layer1.bias grad norm: 0.0005532825016416609\n",
            " - layer2.weight grad norm: 0.0018997915321961045\n",
            " - layer2.bias grad norm: 0.0005925738951191306\n",
            " - layer2_input.weight grad norm: 0.5091001987457275\n",
            " - layer2_input.bias grad norm: 0.0005925738951191306\n",
            " - layer3.weight grad norm: 0.0026694342959672213\n",
            " - layer3.bias grad norm: 0.000618753838352859\n",
            " - layer3_input.weight grad norm: 0.5264445543289185\n",
            " - layer3_input.bias grad norm: 0.000618753838352859\n",
            " - layer4.weight grad norm: 0.0031748805195093155\n",
            " - layer4.bias grad norm: 0.0005607638158835471\n",
            " - layer4_input.weight grad norm: 0.4803313910961151\n",
            " - layer4_input.bias grad norm: 0.0005607638158835471\n",
            " - layer5.weight grad norm: 0.012898577377200127\n",
            " - layer5.bias grad norm: 0.00545100262388587\n",
            "Gradients at iteration 244:\n",
            " - layer1.weight grad norm: 0.5319092273712158\n",
            " - layer1.bias grad norm: 0.0006135668372735381\n",
            " - layer2.weight grad norm: 0.0019818847067654133\n",
            " - layer2.bias grad norm: 0.0005373867461457849\n",
            " - layer2_input.weight grad norm: 0.4735478460788727\n",
            " - layer2_input.bias grad norm: 0.0005373867461457849\n",
            " - layer3.weight grad norm: 0.0026909392327070236\n",
            " - layer3.bias grad norm: 0.0005929180770181119\n",
            " - layer3_input.weight grad norm: 0.5128762722015381\n",
            " - layer3_input.bias grad norm: 0.0005929180770181119\n",
            " - layer4.weight grad norm: 0.003230344969779253\n",
            " - layer4.bias grad norm: 0.0005516496021300554\n",
            " - layer4_input.weight grad norm: 0.4791153371334076\n",
            " - layer4_input.bias grad norm: 0.0005516496021300554\n",
            " - layer5.weight grad norm: 0.013295924291014671\n",
            " - layer5.bias grad norm: 0.005549020133912563\n",
            "Gradients at iteration 245:\n",
            " - layer1.weight grad norm: 0.5187000036239624\n",
            " - layer1.bias grad norm: 0.0006053636316210032\n",
            " - layer2.weight grad norm: 0.0017668964574113488\n",
            " - layer2.bias grad norm: 0.0005762344808317721\n",
            " - layer2_input.weight grad norm: 0.49634742736816406\n",
            " - layer2_input.bias grad norm: 0.0005762344808317721\n",
            " - layer3.weight grad norm: 0.00246287789195776\n",
            " - layer3.bias grad norm: 0.000593974778894335\n",
            " - layer3_input.weight grad norm: 0.5089485049247742\n",
            " - layer3_input.bias grad norm: 0.000593974778894335\n",
            " - layer4.weight grad norm: 0.002913616830483079\n",
            " - layer4.bias grad norm: 0.0005558907287195325\n",
            " - layer4_input.weight grad norm: 0.4747646450996399\n",
            " - layer4_input.bias grad norm: 0.0005558907287195325\n",
            " - layer5.weight grad norm: 0.010692400857806206\n",
            " - layer5.bias grad norm: 0.00501300860196352\n",
            "Gradients at iteration 246:\n",
            " - layer1.weight grad norm: 0.49526703357696533\n",
            " - layer1.bias grad norm: 0.0005671089165844023\n",
            " - layer2.weight grad norm: 0.001923340605571866\n",
            " - layer2.bias grad norm: 0.00059267517644912\n",
            " - layer2_input.weight grad norm: 0.5141216516494751\n",
            " - layer2_input.bias grad norm: 0.00059267517644912\n",
            " - layer3.weight grad norm: 0.00263443891890347\n",
            " - layer3.bias grad norm: 0.0005581515142694116\n",
            " - layer3_input.weight grad norm: 0.48303237557411194\n",
            " - layer3_input.bias grad norm: 0.0005581515142694116\n",
            " - layer4.weight grad norm: 0.0031572768930345774\n",
            " - layer4.bias grad norm: 0.0005903031560592353\n",
            " - layer4_input.weight grad norm: 0.5068252682685852\n",
            " - layer4_input.bias grad norm: 0.0005903031560592353\n",
            " - layer5.weight grad norm: 0.01204410009086132\n",
            " - layer5.bias grad norm: 0.005412840750068426\n",
            "Gradients at iteration 247:\n",
            " - layer1.weight grad norm: 0.5077061057090759\n",
            " - layer1.bias grad norm: 0.0005892791668884456\n",
            " - layer2.weight grad norm: 0.001966634066775441\n",
            " - layer2.bias grad norm: 0.0005479456158354878\n",
            " - layer2_input.weight grad norm: 0.4765947461128235\n",
            " - layer2_input.bias grad norm: 0.0005479456158354878\n",
            " - layer3.weight grad norm: 0.0026850865688174963\n",
            " - layer3.bias grad norm: 0.0005965211312286556\n",
            " - layer3_input.weight grad norm: 0.5110515356063843\n",
            " - layer3_input.bias grad norm: 0.0005965211312286556\n",
            " - layer4.weight grad norm: 0.0032672828529030085\n",
            " - layer4.bias grad norm: 0.0005907778977416456\n",
            " - layer4_input.weight grad norm: 0.5036829710006714\n",
            " - layer4_input.bias grad norm: 0.0005907778977416456\n",
            " - layer5.weight grad norm: 0.01290409080684185\n",
            " - layer5.bias grad norm: 0.0055675264447927475\n",
            "Gradients at iteration 248:\n",
            " - layer1.weight grad norm: 0.5032659769058228\n",
            " - layer1.bias grad norm: 0.0005844940315000713\n",
            " - layer2.weight grad norm: 0.0019417052390053868\n",
            " - layer2.bias grad norm: 0.0005930056795477867\n",
            " - layer2_input.weight grad norm: 0.5083972215652466\n",
            " - layer2_input.bias grad norm: 0.0005930056795477867\n",
            " - layer3.weight grad norm: 0.0027156518772244453\n",
            " - layer3.bias grad norm: 0.0005819475627504289\n",
            " - layer3_input.weight grad norm: 0.5003829002380371\n",
            " - layer3_input.bias grad norm: 0.0005819475627504289\n",
            " - layer4.weight grad norm: 0.003218310419470072\n",
            " - layer4.bias grad norm: 0.0005674657295458019\n",
            " - layer4_input.weight grad norm: 0.48749426007270813\n",
            " - layer4_input.bias grad norm: 0.0005674657295458019\n",
            " - layer5.weight grad norm: 0.01294802874326706\n",
            " - layer5.bias grad norm: 0.005505172535777092\n",
            "Gradients at iteration 249:\n",
            " - layer1.weight grad norm: 0.5355340838432312\n",
            " - layer1.bias grad norm: 0.000628015783149749\n",
            " - layer2.weight grad norm: 0.0018936038250103593\n",
            " - layer2.bias grad norm: 0.0005515501252375543\n",
            " - layer2_input.weight grad norm: 0.4791952669620514\n",
            " - layer2_input.bias grad norm: 0.0005515501252375543\n",
            " - layer3.weight grad norm: 0.0026106741279363632\n",
            " - layer3.bias grad norm: 0.0006254439358599484\n",
            " - layer3_input.weight grad norm: 0.5292690992355347\n",
            " - layer3_input.bias grad norm: 0.0006254439358599484\n",
            " - layer4.weight grad norm: 0.0030862942803651094\n",
            " - layer4.bias grad norm: 0.0005163135356269777\n",
            " - layer4_input.weight grad norm: 0.4508357346057892\n",
            " - layer4_input.bias grad norm: 0.0005163135356269777\n",
            " - layer5.weight grad norm: 0.012077591381967068\n",
            " - layer5.bias grad norm: 0.005319688934832811\n",
            "Gradients at iteration 250:\n",
            " - layer1.weight grad norm: 0.5172702074050903\n",
            " - layer1.bias grad norm: 0.0005938984104432166\n",
            " - layer2.weight grad norm: 0.0018791784532368183\n",
            " - layer2.bias grad norm: 0.0005591714289039373\n",
            " - layer2_input.weight grad norm: 0.4895903170108795\n",
            " - layer2_input.bias grad norm: 0.0005591714289039373\n",
            " - layer3.weight grad norm: 0.0026411740109324455\n",
            " - layer3.bias grad norm: 0.0006047739298082888\n",
            " - layer3_input.weight grad norm: 0.5183414220809937\n",
            " - layer3_input.bias grad norm: 0.0006047739298082888\n",
            " - layer4.weight grad norm: 0.0030893993098288774\n",
            " - layer4.bias grad norm: 0.0005466541042551398\n",
            " - layer4_input.weight grad norm: 0.4731214642524719\n",
            " - layer4_input.bias grad norm: 0.0005466541042551398\n",
            " - layer5.weight grad norm: 0.012655381113290787\n",
            " - layer5.bias grad norm: 0.005345778074115515\n",
            "Gradients at iteration 251:\n",
            " - layer1.weight grad norm: 0.5330504179000854\n",
            " - layer1.bias grad norm: 0.0006206762627698481\n",
            " - layer2.weight grad norm: 0.0019232333870604634\n",
            " - layer2.bias grad norm: 0.0005770584102720022\n",
            " - layer2_input.weight grad norm: 0.5003769397735596\n",
            " - layer2_input.bias grad norm: 0.0005770584102720022\n",
            " - layer3.weight grad norm: 0.0026599611155688763\n",
            " - layer3.bias grad norm: 0.0005542755825445056\n",
            " - layer3_input.weight grad norm: 0.48199015855789185\n",
            " - layer3_input.bias grad norm: 0.0005542755825445056\n",
            " - layer4.weight grad norm: 0.00317011633887887\n",
            " - layer4.bias grad norm: 0.0005605045589618385\n",
            " - layer4_input.weight grad norm: 0.4826543629169464\n",
            " - layer4_input.bias grad norm: 0.0005605045589618385\n",
            " - layer5.weight grad norm: 0.012551750987768173\n",
            " - layer5.bias grad norm: 0.005458091385662556\n",
            "Gradients at iteration 252:\n",
            " - layer1.weight grad norm: 0.5231255888938904\n",
            " - layer1.bias grad norm: 0.000610791496001184\n",
            " - layer2.weight grad norm: 0.0018219599733129144\n",
            " - layer2.bias grad norm: 0.000627299421466887\n",
            " - layer2_input.weight grad norm: 0.5336523056030273\n",
            " - layer2_input.bias grad norm: 0.000627299421466887\n",
            " - layer3.weight grad norm: 0.002569766715168953\n",
            " - layer3.bias grad norm: 0.000531185301952064\n",
            " - layer3_input.weight grad norm: 0.4606565833091736\n",
            " - layer3_input.bias grad norm: 0.000531185301952064\n",
            " - layer4.weight grad norm: 0.0030394147615879774\n",
            " - layer4.bias grad norm: 0.0005609954241663218\n",
            " - layer4_input.weight grad norm: 0.4787275493144989\n",
            " - layer4_input.bias grad norm: 0.0005609954241663218\n",
            " - layer5.weight grad norm: 0.011045671999454498\n",
            " - layer5.bias grad norm: 0.005175315774977207\n",
            "Gradients at iteration 253:\n",
            " - layer1.weight grad norm: 0.5092747807502747\n",
            " - layer1.bias grad norm: 0.0005941328126937151\n",
            " - layer2.weight grad norm: 0.0019384199986234307\n",
            " - layer2.bias grad norm: 0.000593101023696363\n",
            " - layer2_input.weight grad norm: 0.5090733170509338\n",
            " - layer2_input.bias grad norm: 0.000593101023696363\n",
            " - layer3.weight grad norm: 0.0026670601218938828\n",
            " - layer3.bias grad norm: 0.0005622560274787247\n",
            " - layer3_input.weight grad norm: 0.4812621772289276\n",
            " - layer3_input.bias grad norm: 0.0005622560274787247\n",
            " - layer4.weight grad norm: 0.003199557075276971\n",
            " - layer4.bias grad norm: 0.000585988920647651\n",
            " - layer4_input.weight grad norm: 0.4996495544910431\n",
            " - layer4_input.bias grad norm: 0.000585988920647651\n",
            " - layer5.weight grad norm: 0.012920273467898369\n",
            " - layer5.bias grad norm: 0.005478769075125456\n",
            "Gradients at iteration 254:\n",
            " - layer1.weight grad norm: 0.5040693879127502\n",
            " - layer1.bias grad norm: 0.0005859018419869244\n",
            " - layer2.weight grad norm: 0.0018225550884380937\n",
            " - layer2.bias grad norm: 0.0006012310041114688\n",
            " - layer2_input.weight grad norm: 0.5119518637657166\n",
            " - layer2_input.bias grad norm: 0.0006012310041114688\n",
            " - layer3.weight grad norm: 0.0024832007475197315\n",
            " - layer3.bias grad norm: 0.0005385107360780239\n",
            " - layer3_input.weight grad norm: 0.46779775619506836\n",
            " - layer3_input.bias grad norm: 0.0005385107360780239\n",
            " - layer4.weight grad norm: 0.0029886665288358927\n",
            " - layer4.bias grad norm: 0.0006039206637069583\n",
            " - layer4_input.weight grad norm: 0.5145866274833679\n",
            " - layer4_input.bias grad norm: 0.0006039206637069583\n",
            " - layer5.weight grad norm: 0.011768106371164322\n",
            " - layer5.bias grad norm: 0.005090571008622646\n",
            "Gradients at iteration 255:\n",
            " - layer1.weight grad norm: 0.5186048746109009\n",
            " - layer1.bias grad norm: 0.0005991743528284132\n",
            " - layer2.weight grad norm: 0.0019751833751797676\n",
            " - layer2.bias grad norm: 0.0005874448688700795\n",
            " - layer2_input.weight grad norm: 0.5097412467002869\n",
            " - layer2_input.bias grad norm: 0.0005874448688700795\n",
            " - layer3.weight grad norm: 0.0026796211022883654\n",
            " - layer3.bias grad norm: 0.0005669142119586468\n",
            " - layer3_input.weight grad norm: 0.49001944065093994\n",
            " - layer3_input.bias grad norm: 0.0005669142119586468\n",
            " - layer4.weight grad norm: 0.0032386949751526117\n",
            " - layer4.bias grad norm: 0.0005602699238806963\n",
            " - layer4_input.weight grad norm: 0.4805031716823578\n",
            " - layer4_input.bias grad norm: 0.0005602699238806963\n",
            " - layer5.weight grad norm: 0.01248940546065569\n",
            " - layer5.bias grad norm: 0.005532024893909693\n",
            "Gradients at iteration 256:\n",
            " - layer1.weight grad norm: 0.49653947353363037\n",
            " - layer1.bias grad norm: 0.0005771589931100607\n",
            " - layer2.weight grad norm: 0.0018003631848841906\n",
            " - layer2.bias grad norm: 0.0005595469265244901\n",
            " - layer2_input.weight grad norm: 0.4834023118019104\n",
            " - layer2_input.bias grad norm: 0.0005595469265244901\n",
            " - layer3.weight grad norm: 0.0025461993645876646\n",
            " - layer3.bias grad norm: 0.000576638150960207\n",
            " - layer3_input.weight grad norm: 0.4919319748878479\n",
            " - layer3_input.bias grad norm: 0.000576638150960207\n",
            " - layer4.weight grad norm: 0.003011001506820321\n",
            " - layer4.bias grad norm: 0.0006248550489544868\n",
            " - layer4_input.weight grad norm: 0.5268773436546326\n",
            " - layer4_input.bias grad norm: 0.0006248550489544868\n",
            " - layer5.weight grad norm: 0.011242561042308807\n",
            " - layer5.bias grad norm: 0.005148901604115963\n",
            "Gradients at iteration 257:\n",
            " - layer1.weight grad norm: 0.504736602306366\n",
            " - layer1.bias grad norm: 0.000585145375225693\n",
            " - layer2.weight grad norm: 0.0018667589174583554\n",
            " - layer2.bias grad norm: 0.0005786466063000262\n",
            " - layer2_input.weight grad norm: 0.4989105463027954\n",
            " - layer2_input.bias grad norm: 0.0005786466063000262\n",
            " - layer3.weight grad norm: 0.0025220431853085756\n",
            " - layer3.bias grad norm: 0.0005333563894964755\n",
            " - layer3_input.weight grad norm: 0.4643147587776184\n",
            " - layer3_input.bias grad norm: 0.0005333563894964755\n",
            " - layer4.weight grad norm: 0.003039170289412141\n",
            " - layer4.bias grad norm: 0.0006267346907407045\n",
            " - layer4_input.weight grad norm: 0.5296759009361267\n",
            " - layer4_input.bias grad norm: 0.0006267346907407045\n",
            " - layer5.weight grad norm: 0.01165031734853983\n",
            " - layer5.bias grad norm: 0.00522491242736578\n",
            "Gradients at iteration 258:\n",
            " - layer1.weight grad norm: 0.5011073350906372\n",
            " - layer1.bias grad norm: 0.0005706243100576103\n",
            " - layer2.weight grad norm: 0.001903835916891694\n",
            " - layer2.bias grad norm: 0.0006141815683804452\n",
            " - layer2_input.weight grad norm: 0.5280490517616272\n",
            " - layer2_input.bias grad norm: 0.0006141815683804452\n",
            " - layer3.weight grad norm: 0.0026567138265818357\n",
            " - layer3.bias grad norm: 0.0005454986821860075\n",
            " - layer3_input.weight grad norm: 0.4756440818309784\n",
            " - layer3_input.bias grad norm: 0.0005454986821860075\n",
            " - layer4.weight grad norm: 0.003179143648594618\n",
            " - layer4.bias grad norm: 0.0005715412553399801\n",
            " - layer4_input.weight grad norm: 0.4935753047466278\n",
            " - layer4_input.bias grad norm: 0.0005715412553399801\n",
            " - layer5.weight grad norm: 0.012207421474158764\n",
            " - layer5.bias grad norm: 0.00543641671538353\n",
            "Gradients at iteration 259:\n",
            " - layer1.weight grad norm: 0.5360547304153442\n",
            " - layer1.bias grad norm: 0.0006198652554303408\n",
            " - layer2.weight grad norm: 0.0018844170263037086\n",
            " - layer2.bias grad norm: 0.0006025336333550513\n",
            " - layer2_input.weight grad norm: 0.5204763412475586\n",
            " - layer2_input.bias grad norm: 0.0006025336333550513\n",
            " - layer3.weight grad norm: 0.0026682193856686354\n",
            " - layer3.bias grad norm: 0.0005377778434194624\n",
            " - layer3_input.weight grad norm: 0.4728512763977051\n",
            " - layer3_input.bias grad norm: 0.0005377778434194624\n",
            " - layer4.weight grad norm: 0.003112177364528179\n",
            " - layer4.bias grad norm: 0.0005266990629024804\n",
            " - layer4_input.weight grad norm: 0.4668371379375458\n",
            " - layer4_input.bias grad norm: 0.0005266990629024804\n",
            " - layer5.weight grad norm: 0.01315800379961729\n",
            " - layer5.bias grad norm: 0.005360499024391174\n",
            "Gradients at iteration 260:\n",
            " - layer1.weight grad norm: 0.557796835899353\n",
            " - layer1.bias grad norm: 0.0006460026488639414\n",
            " - layer2.weight grad norm: 0.001946243573911488\n",
            " - layer2.bias grad norm: 0.000572872580960393\n",
            " - layer2_input.weight grad norm: 0.5002736449241638\n",
            " - layer2_input.bias grad norm: 0.000572872580960393\n",
            " - layer3.weight grad norm: 0.0026983136776834726\n",
            " - layer3.bias grad norm: 0.0005514285294339061\n",
            " - layer3_input.weight grad norm: 0.4823870360851288\n",
            " - layer3_input.bias grad norm: 0.0005514285294339061\n",
            " - layer4.weight grad norm: 0.003233876544982195\n",
            " - layer4.bias grad norm: 0.0005175945698283613\n",
            " - layer4_input.weight grad norm: 0.4535335302352905\n",
            " - layer4_input.bias grad norm: 0.0005175945698283613\n",
            " - layer5.weight grad norm: 0.012049769051373005\n",
            " - layer5.bias grad norm: 0.005493546836078167\n",
            "Gradients at iteration 261:\n",
            " - layer1.weight grad norm: 0.5181008577346802\n",
            " - layer1.bias grad norm: 0.0005999524146318436\n",
            " - layer2.weight grad norm: 0.0018407386960461736\n",
            " - layer2.bias grad norm: 0.0005667402874678373\n",
            " - layer2_input.weight grad norm: 0.48849961161613464\n",
            " - layer2_input.bias grad norm: 0.0005667402874678373\n",
            " - layer3.weight grad norm: 0.0026008624117821455\n",
            " - layer3.bias grad norm: 0.0005924067227169871\n",
            " - layer3_input.weight grad norm: 0.5060515403747559\n",
            " - layer3_input.bias grad norm: 0.0005924067227169871\n",
            " - layer4.weight grad norm: 0.0030848511960357428\n",
            " - layer4.bias grad norm: 0.0005707821110263467\n",
            " - layer4_input.weight grad norm: 0.48647984862327576\n",
            " - layer4_input.bias grad norm: 0.0005707821110263467\n",
            " - layer5.weight grad norm: 0.011801269836723804\n",
            " - layer5.bias grad norm: 0.00524052232503891\n",
            "Gradients at iteration 262:\n",
            " - layer1.weight grad norm: 0.5477781891822815\n",
            " - layer1.bias grad norm: 0.0006371079361997545\n",
            " - layer2.weight grad norm: 0.0019384626066312194\n",
            " - layer2.bias grad norm: 0.0006054012919776142\n",
            " - layer2_input.weight grad norm: 0.522044837474823\n",
            " - layer2_input.bias grad norm: 0.0006054012919776142\n",
            " - layer3.weight grad norm: 0.0026421938091516495\n",
            " - layer3.bias grad norm: 0.0005194265395402908\n",
            " - layer3_input.weight grad norm: 0.45778846740722656\n",
            " - layer3_input.bias grad norm: 0.0005194265395402908\n",
            " - layer4.weight grad norm: 0.003171047894284129\n",
            " - layer4.bias grad norm: 0.0005367751000449061\n",
            " - layer4_input.weight grad norm: 0.4664941132068634\n",
            " - layer4_input.bias grad norm: 0.0005367751000449061\n",
            " - layer5.weight grad norm: 0.012968205846846104\n",
            " - layer5.bias grad norm: 0.005461318418383598\n",
            "Gradients at iteration 263:\n",
            " - layer1.weight grad norm: 0.5036773681640625\n",
            " - layer1.bias grad norm: 0.0005837834905833006\n",
            " - layer2.weight grad norm: 0.0018731759628280997\n",
            " - layer2.bias grad norm: 0.0005453831399790943\n",
            " - layer2_input.weight grad norm: 0.4744189977645874\n",
            " - layer2_input.bias grad norm: 0.0005453831399790943\n",
            " - layer3.weight grad norm: 0.0025783488526940346\n",
            " - layer3.bias grad norm: 0.0005751391290687025\n",
            " - layer3_input.weight grad norm: 0.4961359202861786\n",
            " - layer3_input.bias grad norm: 0.0005751391290687025\n",
            " - layer4.weight grad norm: 0.003034112509340048\n",
            " - layer4.bias grad norm: 0.0006172564462758601\n",
            " - layer4_input.weight grad norm: 0.5242938995361328\n",
            " - layer4_input.bias grad norm: 0.0006172564462758601\n",
            " - layer5.weight grad norm: 0.012307832017540932\n",
            " - layer5.bias grad norm: 0.005261465907096863\n",
            "Gradients at iteration 264:\n",
            " - layer1.weight grad norm: 0.5171992182731628\n",
            " - layer1.bias grad norm: 0.0006044509937055409\n",
            " - layer2.weight grad norm: 0.00187302241101861\n",
            " - layer2.bias grad norm: 0.0006594136939384043\n",
            " - layer2_input.weight grad norm: 0.5595678091049194\n",
            " - layer2_input.bias grad norm: 0.0006594136939384043\n",
            " - layer3.weight grad norm: 0.0026052265893667936\n",
            " - layer3.bias grad norm: 0.0005422324175015092\n",
            " - layer3_input.weight grad norm: 0.46568039059638977\n",
            " - layer3_input.bias grad norm: 0.0005422324175015092\n",
            " - layer4.weight grad norm: 0.0031438195146620274\n",
            " - layer4.bias grad norm: 0.0005183906177990139\n",
            " - layer4_input.weight grad norm: 0.4498029947280884\n",
            " - layer4_input.bias grad norm: 0.0005183906177990139\n",
            " - layer5.weight grad norm: 0.012516518123447895\n",
            " - layer5.bias grad norm: 0.005354293156415224\n",
            "Gradients at iteration 265:\n",
            " - layer1.weight grad norm: 0.4814315140247345\n",
            " - layer1.bias grad norm: 0.0005475636571645737\n",
            " - layer2.weight grad norm: 0.0019082492217421532\n",
            " - layer2.bias grad norm: 0.000629246118478477\n",
            " - layer2_input.weight grad norm: 0.5406970381736755\n",
            " - layer2_input.bias grad norm: 0.000629246118478477\n",
            " - layer3.weight grad norm: 0.002628531539812684\n",
            " - layer3.bias grad norm: 0.0005654898122884333\n",
            " - layer3_input.weight grad norm: 0.4903789758682251\n",
            " - layer3_input.bias grad norm: 0.0005654898122884333\n",
            " - layer4.weight grad norm: 0.0031493441201746464\n",
            " - layer4.bias grad norm: 0.0005619511939585209\n",
            " - layer4_input.weight grad norm: 0.4849860668182373\n",
            " - layer4_input.bias grad norm: 0.0005619511939585209\n",
            " - layer5.weight grad norm: 0.011650025844573975\n",
            " - layer5.bias grad norm: 0.00537475012242794\n",
            "Gradients at iteration 266:\n",
            " - layer1.weight grad norm: 0.485646516084671\n",
            " - layer1.bias grad norm: 0.0005577445845119655\n",
            " - layer2.weight grad norm: 0.0019517778418958187\n",
            " - layer2.bias grad norm: 0.0006359844701364636\n",
            " - layer2_input.weight grad norm: 0.5464470386505127\n",
            " - layer2_input.bias grad norm: 0.0006359844701364636\n",
            " - layer3.weight grad norm: 0.0026568034663796425\n",
            " - layer3.bias grad norm: 0.0006080690072849393\n",
            " - layer3_input.weight grad norm: 0.5232676863670349\n",
            " - layer3_input.bias grad norm: 0.0006080690072849393\n",
            " - layer4.weight grad norm: 0.003169629257172346\n",
            " - layer4.bias grad norm: 0.0004978570505045354\n",
            " - layer4_input.weight grad norm: 0.43762579560279846\n",
            " - layer4_input.bias grad norm: 0.0004978570505045354\n",
            " - layer5.weight grad norm: 0.012833156622946262\n",
            " - layer5.bias grad norm: 0.005451275501400232\n",
            "Gradients at iteration 267:\n",
            " - layer1.weight grad norm: 0.5562987327575684\n",
            " - layer1.bias grad norm: 0.0006529079983010888\n",
            " - layer2.weight grad norm: 0.001893289852887392\n",
            " - layer2.bias grad norm: 0.0005534249939955771\n",
            " - layer2_input.weight grad norm: 0.4806200861930847\n",
            " - layer2_input.bias grad norm: 0.0005534249939955771\n",
            " - layer3.weight grad norm: 0.00256779114715755\n",
            " - layer3.bias grad norm: 0.0005752005963586271\n",
            " - layer3_input.weight grad norm: 0.494490385055542\n",
            " - layer3_input.bias grad norm: 0.0005752005963586271\n",
            " - layer4.weight grad norm: 0.003096215659752488\n",
            " - layer4.bias grad norm: 0.0005365292890928686\n",
            " - layer4_input.weight grad norm: 0.463477224111557\n",
            " - layer4_input.bias grad norm: 0.0005365292890928686\n",
            " - layer5.weight grad norm: 0.012415384873747826\n",
            " - layer5.bias grad norm: 0.005302521400153637\n",
            "Gradients at iteration 268:\n",
            " - layer1.weight grad norm: 0.5270222425460815\n",
            " - layer1.bias grad norm: 0.0006188521510921419\n",
            " - layer2.weight grad norm: 0.0019065510714426637\n",
            " - layer2.bias grad norm: 0.000597019272390753\n",
            " - layer2_input.weight grad norm: 0.5117021799087524\n",
            " - layer2_input.bias grad norm: 0.000597019272390753\n",
            " - layer3.weight grad norm: 0.002586940536275506\n",
            " - layer3.bias grad norm: 0.0005910018808208406\n",
            " - layer3_input.weight grad norm: 0.507248044013977\n",
            " - layer3_input.bias grad norm: 0.0005910018808208406\n",
            " - layer4.weight grad norm: 0.003083716845139861\n",
            " - layer4.bias grad norm: 0.0005190776428207755\n",
            " - layer4_input.weight grad norm: 0.45045509934425354\n",
            " - layer4_input.bias grad norm: 0.0005190776428207755\n",
            " - layer5.weight grad norm: 0.0121386107057333\n",
            " - layer5.bias grad norm: 0.0053353034891188145\n",
            "Gradients at iteration 269:\n",
            " - layer1.weight grad norm: 0.516414999961853\n",
            " - layer1.bias grad norm: 0.0005998596316203475\n",
            " - layer2.weight grad norm: 0.0019231124315410852\n",
            " - layer2.bias grad norm: 0.0005948125617578626\n",
            " - layer2_input.weight grad norm: 0.5082271695137024\n",
            " - layer2_input.bias grad norm: 0.0005948125617578626\n",
            " - layer3.weight grad norm: 0.002642424311488867\n",
            " - layer3.bias grad norm: 0.0005389611469581723\n",
            " - layer3_input.weight grad norm: 0.46692511439323425\n",
            " - layer3_input.bias grad norm: 0.0005389611469581723\n",
            " - layer4.weight grad norm: 0.0031655977945774794\n",
            " - layer4.bias grad norm: 0.0005936939269304276\n",
            " - layer4_input.weight grad norm: 0.5067546963691711\n",
            " - layer4_input.bias grad norm: 0.0005936939269304276\n",
            " - layer5.weight grad norm: 0.01219736598432064\n",
            " - layer5.bias grad norm: 0.0054278080351650715\n",
            "Gradients at iteration 270:\n",
            " - layer1.weight grad norm: 0.5826923847198486\n",
            " - layer1.bias grad norm: 0.000690419808961451\n",
            " - layer2.weight grad norm: 0.001747995032928884\n",
            " - layer2.bias grad norm: 0.0005763833178207278\n",
            " - layer2_input.weight grad norm: 0.48957714438438416\n",
            " - layer2_input.bias grad norm: 0.0005763833178207278\n",
            " - layer3.weight grad norm: 0.0024137687869369984\n",
            " - layer3.bias grad norm: 0.0005463508423417807\n",
            " - layer3_input.weight grad norm: 0.46768176555633545\n",
            " - layer3_input.bias grad norm: 0.0005463508423417807\n",
            " - layer4.weight grad norm: 0.0029671061784029007\n",
            " - layer4.bias grad norm: 0.0005230065435171127\n",
            " - layer4_input.weight grad norm: 0.44932982325553894\n",
            " - layer4_input.bias grad norm: 0.0005230065435171127\n",
            " - layer5.weight grad norm: 0.010748101398348808\n",
            " - layer5.bias grad norm: 0.004992238245904446\n",
            "Gradients at iteration 271:\n",
            " - layer1.weight grad norm: 0.5364266633987427\n",
            " - layer1.bias grad norm: 0.0006209808052517474\n",
            " - layer2.weight grad norm: 0.001948278397321701\n",
            " - layer2.bias grad norm: 0.0005565117462538183\n",
            " - layer2_input.weight grad norm: 0.48899978399276733\n",
            " - layer2_input.bias grad norm: 0.0005565117462538183\n",
            " - layer3.weight grad norm: 0.0027066662441939116\n",
            " - layer3.bias grad norm: 0.0005897385417483747\n",
            " - layer3_input.weight grad norm: 0.5081819295883179\n",
            " - layer3_input.bias grad norm: 0.0005897385417483747\n",
            " - layer4.weight grad norm: 0.0032202592119574547\n",
            " - layer4.bias grad norm: 0.0005294661968946457\n",
            " - layer4_input.weight grad norm: 0.4633338153362274\n",
            " - layer4_input.bias grad norm: 0.0005294661968946457\n",
            " - layer5.weight grad norm: 0.012031328864395618\n",
            " - layer5.bias grad norm: 0.005486791022121906\n",
            "Gradients at iteration 272:\n",
            " - layer1.weight grad norm: 0.5186490416526794\n",
            " - layer1.bias grad norm: 0.0005995153915137053\n",
            " - layer2.weight grad norm: 0.001886181184090674\n",
            " - layer2.bias grad norm: 0.0005723645444959402\n",
            " - layer2_input.weight grad norm: 0.4977743625640869\n",
            " - layer2_input.bias grad norm: 0.0005723645444959402\n",
            " - layer3.weight grad norm: 0.002585530513897538\n",
            " - layer3.bias grad norm: 0.0005845042178407311\n",
            " - layer3_input.weight grad norm: 0.503395676612854\n",
            " - layer3_input.bias grad norm: 0.0005845042178407311\n",
            " - layer4.weight grad norm: 0.0031204025726765394\n",
            " - layer4.bias grad norm: 0.0005579777644015849\n",
            " - layer4_input.weight grad norm: 0.47916391491889954\n",
            " - layer4_input.bias grad norm: 0.0005579777644015849\n",
            " - layer5.weight grad norm: 0.012938370928168297\n",
            " - layer5.bias grad norm: 0.0053809876553714275\n",
            "Gradients at iteration 273:\n",
            " - layer1.weight grad norm: 0.5365560054779053\n",
            " - layer1.bias grad norm: 0.0006330878823064268\n",
            " - layer2.weight grad norm: 0.0018062219023704529\n",
            " - layer2.bias grad norm: 0.0006082183681428432\n",
            " - layer2_input.weight grad norm: 0.5204458832740784\n",
            " - layer2_input.bias grad norm: 0.0006082183681428432\n",
            " - layer3.weight grad norm: 0.002482749754562974\n",
            " - layer3.bias grad norm: 0.0005704161594621837\n",
            " - layer3_input.weight grad norm: 0.4906342923641205\n",
            " - layer3_input.bias grad norm: 0.0005704161594621837\n",
            " - layer4.weight grad norm: 0.0030026263557374477\n",
            " - layer4.bias grad norm: 0.0005125144380144775\n",
            " - layer4_input.weight grad norm: 0.4476180076599121\n",
            " - layer4_input.bias grad norm: 0.0005125144380144775\n",
            " - layer5.weight grad norm: 0.010649177245795727\n",
            " - layer5.bias grad norm: 0.005063640419393778\n",
            "Gradients at iteration 274:\n",
            " - layer1.weight grad norm: 0.5090058445930481\n",
            " - layer1.bias grad norm: 0.0005906500155106187\n",
            " - layer2.weight grad norm: 0.0019006920047104359\n",
            " - layer2.bias grad norm: 0.0005460360553115606\n",
            " - layer2_input.weight grad norm: 0.476260781288147\n",
            " - layer2_input.bias grad norm: 0.0005460360553115606\n",
            " - layer3.weight grad norm: 0.002612051321193576\n",
            " - layer3.bias grad norm: 0.0005930076586082578\n",
            " - layer3_input.weight grad norm: 0.5092872977256775\n",
            " - layer3_input.bias grad norm: 0.0005930076586082578\n",
            " - layer4.weight grad norm: 0.003108630422502756\n",
            " - layer4.bias grad norm: 0.0005930665647611022\n",
            " - layer4_input.weight grad norm: 0.504491925239563\n",
            " - layer4_input.bias grad norm: 0.0005930665647611022\n",
            " - layer5.weight grad norm: 0.012316886335611343\n",
            " - layer5.bias grad norm: 0.005372117273509502\n",
            "Gradients at iteration 275:\n",
            " - layer1.weight grad norm: 0.5033658146858215\n",
            " - layer1.bias grad norm: 0.0005760121275670826\n",
            " - layer2.weight grad norm: 0.0019202540861442685\n",
            " - layer2.bias grad norm: 0.0006546597578562796\n",
            " - layer2_input.weight grad norm: 0.5609049797058105\n",
            " - layer2_input.bias grad norm: 0.0006546597578562796\n",
            " - layer3.weight grad norm: 0.002680564997717738\n",
            " - layer3.bias grad norm: 0.0005483794375322759\n",
            " - layer3_input.weight grad norm: 0.4776771664619446\n",
            " - layer3_input.bias grad norm: 0.0005483794375322759\n",
            " - layer4.weight grad norm: 0.0031856379937380552\n",
            " - layer4.bias grad norm: 0.0005103443982079625\n",
            " - layer4_input.weight grad norm: 0.45127591490745544\n",
            " - layer4_input.bias grad norm: 0.0005103443982079625\n",
            " - layer5.weight grad norm: 0.011390257626771927\n",
            " - layer5.bias grad norm: 0.005476656369864941\n",
            "Gradients at iteration 276:\n",
            " - layer1.weight grad norm: 0.4851270914077759\n",
            " - layer1.bias grad norm: 0.0005512076895684004\n",
            " - layer2.weight grad norm: 0.001807404332794249\n",
            " - layer2.bias grad norm: 0.0006088528316468\n",
            " - layer2_input.weight grad norm: 0.5260847806930542\n",
            " - layer2_input.bias grad norm: 0.0006088528316468\n",
            " - layer3.weight grad norm: 0.0024851805064827204\n",
            " - layer3.bias grad norm: 0.0005308905965648592\n",
            " - layer3_input.weight grad norm: 0.46466633677482605\n",
            " - layer3_input.bias grad norm: 0.0005308905965648592\n",
            " - layer4.weight grad norm: 0.0030000265687704086\n",
            " - layer4.bias grad norm: 0.000607896305155009\n",
            " - layer4_input.weight grad norm: 0.5213245153427124\n",
            " - layer4_input.bias grad norm: 0.000607896305155009\n",
            " - layer5.weight grad norm: 0.012048952281475067\n",
            " - layer5.bias grad norm: 0.005145394708961248\n",
            "Gradients at iteration 277:\n",
            " - layer1.weight grad norm: 0.5043701529502869\n",
            " - layer1.bias grad norm: 0.0005873924237675965\n",
            " - layer2.weight grad norm: 0.0017846261616796255\n",
            " - layer2.bias grad norm: 0.0006084729102440178\n",
            " - layer2_input.weight grad norm: 0.5183598399162292\n",
            " - layer2_input.bias grad norm: 0.0006084729102440178\n",
            " - layer3.weight grad norm: 0.0024504971224814653\n",
            " - layer3.bias grad norm: 0.0005661007016897202\n",
            " - layer3_input.weight grad norm: 0.4872303903102875\n",
            " - layer3_input.bias grad norm: 0.0005661007016897202\n",
            " - layer4.weight grad norm: 0.0029131644405424595\n",
            " - layer4.bias grad norm: 0.0005717164021916687\n",
            " - layer4_input.weight grad norm: 0.4892239570617676\n",
            " - layer4_input.bias grad norm: 0.0005717164021916687\n",
            " - layer5.weight grad norm: 0.011601055040955544\n",
            " - layer5.bias grad norm: 0.005073575768619776\n",
            "Gradients at iteration 278:\n",
            " - layer1.weight grad norm: 0.5104610919952393\n",
            " - layer1.bias grad norm: 0.0005894940695725381\n",
            " - layer2.weight grad norm: 0.0019612638279795647\n",
            " - layer2.bias grad norm: 0.0005586566985584795\n",
            " - layer2_input.weight grad norm: 0.48924538493156433\n",
            " - layer2_input.bias grad norm: 0.0005586566985584795\n",
            " - layer3.weight grad norm: 0.002693935064598918\n",
            " - layer3.bias grad norm: 0.000596668804064393\n",
            " - layer3_input.weight grad norm: 0.5120446085929871\n",
            " - layer3_input.bias grad norm: 0.000596668804064393\n",
            " - layer4.weight grad norm: 0.003161438275128603\n",
            " - layer4.bias grad norm: 0.000563800276722759\n",
            " - layer4_input.weight grad norm: 0.4875178635120392\n",
            " - layer4_input.bias grad norm: 0.000563800276722759\n",
            " - layer5.weight grad norm: 0.012305838987231255\n",
            " - layer5.bias grad norm: 0.005490383133292198\n",
            "Gradients at iteration 279:\n",
            " - layer1.weight grad norm: 0.5094900727272034\n",
            " - layer1.bias grad norm: 0.0005861282115802169\n",
            " - layer2.weight grad norm: 0.00202366104349494\n",
            " - layer2.bias grad norm: 0.0005930873448960483\n",
            " - layer2_input.weight grad norm: 0.5128840804100037\n",
            " - layer2_input.bias grad norm: 0.0005930873448960483\n",
            " - layer3.weight grad norm: 0.0027916254475712776\n",
            " - layer3.bias grad norm: 0.0005162261077202857\n",
            " - layer3_input.weight grad norm: 0.45724621415138245\n",
            " - layer3_input.bias grad norm: 0.0005162261077202857\n",
            " - layer4.weight grad norm: 0.0032904597464948893\n",
            " - layer4.bias grad norm: 0.0006013289676047862\n",
            " - layer4_input.weight grad norm: 0.5177625417709351\n",
            " - layer4_input.bias grad norm: 0.0006013289676047862\n",
            " - layer5.weight grad norm: 0.012670976109802723\n",
            " - layer5.bias grad norm: 0.005657021887600422\n",
            "Gradients at iteration 280:\n",
            " - layer1.weight grad norm: 0.5125982761383057\n",
            " - layer1.bias grad norm: 0.0005949505721218884\n",
            " - layer2.weight grad norm: 0.001899614348076284\n",
            " - layer2.bias grad norm: 0.0005772144068032503\n",
            " - layer2_input.weight grad norm: 0.4965885579586029\n",
            " - layer2_input.bias grad norm: 0.0005772144068032503\n",
            " - layer3.weight grad norm: 0.002633706433698535\n",
            " - layer3.bias grad norm: 0.0006114619900472462\n",
            " - layer3_input.weight grad norm: 0.5190105438232422\n",
            " - layer3_input.bias grad norm: 0.0006114619900472462\n",
            " - layer4.weight grad norm: 0.00316787650808692\n",
            " - layer4.bias grad norm: 0.000540088105481118\n",
            " - layer4_input.weight grad norm: 0.4701864421367645\n",
            " - layer4_input.bias grad norm: 0.000540088105481118\n",
            " - layer5.weight grad norm: 0.011980248615145683\n",
            " - layer5.bias grad norm: 0.0053864214569330215\n",
            "Gradients at iteration 281:\n",
            " - layer1.weight grad norm: 0.5147083401679993\n",
            " - layer1.bias grad norm: 0.000605147797614336\n",
            " - layer2.weight grad norm: 0.0019382325699552894\n",
            " - layer2.bias grad norm: 0.0006101265316829085\n",
            " - layer2_input.weight grad norm: 0.5178655982017517\n",
            " - layer2_input.bias grad norm: 0.0006101265316829085\n",
            " - layer3.weight grad norm: 0.0026208413764834404\n",
            " - layer3.bias grad norm: 0.0005826232954859734\n",
            " - layer3_input.weight grad norm: 0.49557384848594666\n",
            " - layer3_input.bias grad norm: 0.0005826232954859734\n",
            " - layer4.weight grad norm: 0.0031007223296910524\n",
            " - layer4.bias grad norm: 0.0005463353008963168\n",
            " - layer4_input.weight grad norm: 0.47022369503974915\n",
            " - layer4_input.bias grad norm: 0.0005463353008963168\n",
            " - layer5.weight grad norm: 0.011640205979347229\n",
            " - layer5.bias grad norm: 0.005359272472560406\n",
            "Gradients at iteration 282:\n",
            " - layer1.weight grad norm: 0.5488669276237488\n",
            " - layer1.bias grad norm: 0.0006344830617308617\n",
            " - layer2.weight grad norm: 0.0018930966034531593\n",
            " - layer2.bias grad norm: 0.0005956770619377494\n",
            " - layer2_input.weight grad norm: 0.5163418650627136\n",
            " - layer2_input.bias grad norm: 0.0005956770619377494\n",
            " - layer3.weight grad norm: 0.002653111470863223\n",
            " - layer3.bias grad norm: 0.0005787985865026712\n",
            " - layer3_input.weight grad norm: 0.5008135437965393\n",
            " - layer3_input.bias grad norm: 0.0005787985865026712\n",
            " - layer4.weight grad norm: 0.003131360048428178\n",
            " - layer4.bias grad norm: 0.00048208670341409743\n",
            " - layer4_input.weight grad norm: 0.4255883991718292\n",
            " - layer4_input.bias grad norm: 0.00048208670341409743\n",
            " - layer5.weight grad norm: 0.011998619884252548\n",
            " - layer5.bias grad norm: 0.005454577971249819\n",
            "Gradients at iteration 283:\n",
            " - layer1.weight grad norm: 0.5021158456802368\n",
            " - layer1.bias grad norm: 0.0005767336115241051\n",
            " - layer2.weight grad norm: 0.001954421866685152\n",
            " - layer2.bias grad norm: 0.0006030133226886392\n",
            " - layer2_input.weight grad norm: 0.5175532698631287\n",
            " - layer2_input.bias grad norm: 0.0006030133226886392\n",
            " - layer3.weight grad norm: 0.002595259342342615\n",
            " - layer3.bias grad norm: 0.0005622809403575957\n",
            " - layer3_input.weight grad norm: 0.487826943397522\n",
            " - layer3_input.bias grad norm: 0.0005622809403575957\n",
            " - layer4.weight grad norm: 0.003170577110722661\n",
            " - layer4.bias grad norm: 0.0005731899873353541\n",
            " - layer4_input.weight grad norm: 0.4917602241039276\n",
            " - layer4_input.bias grad norm: 0.0005731899873353541\n",
            " - layer5.weight grad norm: 0.012746362946927547\n",
            " - layer5.bias grad norm: 0.005437846761196852\n",
            "Gradients at iteration 284:\n",
            " - layer1.weight grad norm: 0.503632664680481\n",
            " - layer1.bias grad norm: 0.0005802818923257291\n",
            " - layer2.weight grad norm: 0.0018592823762446642\n",
            " - layer2.bias grad norm: 0.000543326314073056\n",
            " - layer2_input.weight grad norm: 0.475538969039917\n",
            " - layer2_input.bias grad norm: 0.000543326314073056\n",
            " - layer3.weight grad norm: 0.002586934482678771\n",
            " - layer3.bias grad norm: 0.0006220283103175461\n",
            " - layer3_input.weight grad norm: 0.529902994632721\n",
            " - layer3_input.bias grad norm: 0.0006220283103175461\n",
            " - layer4.weight grad norm: 0.0030870409682393074\n",
            " - layer4.bias grad norm: 0.0005697442684322596\n",
            " - layer4_input.weight grad norm: 0.489126056432724\n",
            " - layer4_input.bias grad norm: 0.0005697442684322596\n",
            " - layer5.weight grad norm: 0.011195367202162743\n",
            " - layer5.bias grad norm: 0.005261938087642193\n",
            "Gradients at iteration 285:\n",
            " - layer1.weight grad norm: 0.5322189331054688\n",
            " - layer1.bias grad norm: 0.0006193105364218354\n",
            " - layer2.weight grad norm: 0.0018415917875245214\n",
            " - layer2.bias grad norm: 0.000582987442612648\n",
            " - layer2_input.weight grad norm: 0.5057867765426636\n",
            " - layer2_input.bias grad norm: 0.000582987442612648\n",
            " - layer3.weight grad norm: 0.0025416980497539043\n",
            " - layer3.bias grad norm: 0.0005103956209495664\n",
            " - layer3_input.weight grad norm: 0.4455976188182831\n",
            " - layer3_input.bias grad norm: 0.0005103956209495664\n",
            " - layer4.weight grad norm: 0.003065739758312702\n",
            " - layer4.bias grad norm: 0.0005983124719932675\n",
            " - layer4_input.weight grad norm: 0.5120152235031128\n",
            " - layer4_input.bias grad norm: 0.0005983124719932675\n",
            " - layer5.weight grad norm: 0.012515705078840256\n",
            " - layer5.bias grad norm: 0.005272997543215752\n",
            "Gradients at iteration 286:\n",
            " - layer1.weight grad norm: 0.5122826099395752\n",
            " - layer1.bias grad norm: 0.0005864945123903453\n",
            " - layer2.weight grad norm: 0.0019220422254875302\n",
            " - layer2.bias grad norm: 0.0006401670398190618\n",
            " - layer2_input.weight grad norm: 0.552484929561615\n",
            " - layer2_input.bias grad norm: 0.0006401670398190618\n",
            " - layer3.weight grad norm: 0.0026702468749135733\n",
            " - layer3.bias grad norm: 0.0005407644785009325\n",
            " - layer3_input.weight grad norm: 0.47317925095558167\n",
            " - layer3_input.bias grad norm: 0.0005407644785009325\n",
            " - layer4.weight grad norm: 0.0032409573905169964\n",
            " - layer4.bias grad norm: 0.0005202877218835056\n",
            " - layer4_input.weight grad norm: 0.45632725954055786\n",
            " - layer4_input.bias grad norm: 0.0005202877218835056\n",
            " - layer5.weight grad norm: 0.011824097484350204\n",
            " - layer5.bias grad norm: 0.00549723207950592\n",
            "Gradients at iteration 287:\n",
            " - layer1.weight grad norm: 0.5340483784675598\n",
            " - layer1.bias grad norm: 0.000618084566667676\n",
            " - layer2.weight grad norm: 0.001976398751139641\n",
            " - layer2.bias grad norm: 0.0005819820216856897\n",
            " - layer2_input.weight grad norm: 0.5038632154464722\n",
            " - layer2_input.bias grad norm: 0.0005819820216856897\n",
            " - layer3.weight grad norm: 0.002685704967007041\n",
            " - layer3.bias grad norm: 0.0005240435712039471\n",
            " - layer3_input.weight grad norm: 0.45888715982437134\n",
            " - layer3_input.bias grad norm: 0.0005240435712039471\n",
            " - layer4.weight grad norm: 0.0032356809824705124\n",
            " - layer4.bias grad norm: 0.0005773209850303829\n",
            " - layer4_input.weight grad norm: 0.5001400709152222\n",
            " - layer4_input.bias grad norm: 0.0005773209850303829\n",
            " - layer5.weight grad norm: 0.01191585324704647\n",
            " - layer5.bias grad norm: 0.005554747302085161\n",
            "Gradients at iteration 288:\n",
            " - layer1.weight grad norm: 0.5146896243095398\n",
            " - layer1.bias grad norm: 0.0005861248937435448\n",
            " - layer2.weight grad norm: 0.001933627761900425\n",
            " - layer2.bias grad norm: 0.0005942500429227948\n",
            " - layer2_input.weight grad norm: 0.5192756652832031\n",
            " - layer2_input.bias grad norm: 0.0005942500429227948\n",
            " - layer3.weight grad norm: 0.0026875610928982496\n",
            " - layer3.bias grad norm: 0.0005694369319826365\n",
            " - layer3_input.weight grad norm: 0.49808868765830994\n",
            " - layer3_input.bias grad norm: 0.0005694369319826365\n",
            " - layer4.weight grad norm: 0.0031490824185311794\n",
            " - layer4.bias grad norm: 0.0005327065009623766\n",
            " - layer4_input.weight grad norm: 0.46600839495658875\n",
            " - layer4_input.bias grad norm: 0.0005327065009623766\n",
            " - layer5.weight grad norm: 0.01175366435199976\n",
            " - layer5.bias grad norm: 0.005460895597934723\n",
            "Gradients at iteration 289:\n",
            " - layer1.weight grad norm: 0.5129134058952332\n",
            " - layer1.bias grad norm: 0.0005919827963225543\n",
            " - layer2.weight grad norm: 0.001891648513264954\n",
            " - layer2.bias grad norm: 0.0005790502764284611\n",
            " - layer2_input.weight grad norm: 0.49982666969299316\n",
            " - layer2_input.bias grad norm: 0.0005790502764284611\n",
            " - layer3.weight grad norm: 0.0025900916662067175\n",
            " - layer3.bias grad norm: 0.000588742783293128\n",
            " - layer3_input.weight grad norm: 0.5093958377838135\n",
            " - layer3_input.bias grad norm: 0.000588742783293128\n",
            " - layer4.weight grad norm: 0.0031050292309373617\n",
            " - layer4.bias grad norm: 0.0005536422831937671\n",
            " - layer4_input.weight grad norm: 0.47689247131347656\n",
            " - layer4_input.bias grad norm: 0.0005536422831937671\n",
            " - layer5.weight grad norm: 0.011490484699606895\n",
            " - layer5.bias grad norm: 0.005316406488418579\n",
            "Gradients at iteration 290:\n",
            " - layer1.weight grad norm: 0.4897107481956482\n",
            " - layer1.bias grad norm: 0.0005673599080182612\n",
            " - layer2.weight grad norm: 0.0018746588611975312\n",
            " - layer2.bias grad norm: 0.0005997473490424454\n",
            " - layer2_input.weight grad norm: 0.5148504376411438\n",
            " - layer2_input.bias grad norm: 0.0005997473490424454\n",
            " - layer3.weight grad norm: 0.002589997835457325\n",
            " - layer3.bias grad norm: 0.0005872796755284071\n",
            " - layer3_input.weight grad norm: 0.5028929114341736\n",
            " - layer3_input.bias grad norm: 0.0005872796755284071\n",
            " - layer4.weight grad norm: 0.003075655782595277\n",
            " - layer4.bias grad norm: 0.0005715599982067943\n",
            " - layer4_input.weight grad norm: 0.49196097254753113\n",
            " - layer4_input.bias grad norm: 0.0005715599982067943\n",
            " - layer5.weight grad norm: 0.011631633155047894\n",
            " - layer5.bias grad norm: 0.005313850473612547\n",
            "Gradients at iteration 291:\n",
            " - layer1.weight grad norm: 0.5083007216453552\n",
            " - layer1.bias grad norm: 0.0005882849218323827\n",
            " - layer2.weight grad norm: 0.0019134916365146637\n",
            " - layer2.bias grad norm: 0.0005984518793411553\n",
            " - layer2_input.weight grad norm: 0.5142472386360168\n",
            " - layer2_input.bias grad norm: 0.0005984518793411553\n",
            " - layer3.weight grad norm: 0.002687671920284629\n",
            " - layer3.bias grad norm: 0.000557314429897815\n",
            " - layer3_input.weight grad norm: 0.48075973987579346\n",
            " - layer3_input.bias grad norm: 0.000557314429897815\n",
            " - layer4.weight grad norm: 0.003091613994911313\n",
            " - layer4.bias grad norm: 0.000573736266233027\n",
            " - layer4_input.weight grad norm: 0.4958367943763733\n",
            " - layer4_input.bias grad norm: 0.000573736266233027\n",
            " - layer5.weight grad norm: 0.012001547031104565\n",
            " - layer5.bias grad norm: 0.005408007651567459\n",
            "Gradients at iteration 292:\n",
            " - layer1.weight grad norm: 0.5072178840637207\n",
            " - layer1.bias grad norm: 0.0005846688291057944\n",
            " - layer2.weight grad norm: 0.0018973188707605004\n",
            " - layer2.bias grad norm: 0.0005313990986905992\n",
            " - layer2_input.weight grad norm: 0.4692656993865967\n",
            " - layer2_input.bias grad norm: 0.0005313990986905992\n",
            " - layer3.weight grad norm: 0.0026267780922353268\n",
            " - layer3.bias grad norm: 0.0005493051139637828\n",
            " - layer3_input.weight grad norm: 0.48041996359825134\n",
            " - layer3_input.bias grad norm: 0.0005493051139637828\n",
            " - layer4.weight grad norm: 0.003159973071888089\n",
            " - layer4.bias grad norm: 0.000631446426268667\n",
            " - layer4_input.weight grad norm: 0.539923906326294\n",
            " - layer4_input.bias grad norm: 0.000631446426268667\n",
            " - layer5.weight grad norm: 0.01209911610931158\n",
            " - layer5.bias grad norm: 0.005396653898060322\n",
            "Gradients at iteration 293:\n",
            " - layer1.weight grad norm: 0.47244715690612793\n",
            " - layer1.bias grad norm: 0.0005430435412563384\n",
            " - layer2.weight grad norm: 0.0018408347386866808\n",
            " - layer2.bias grad norm: 0.0006153781432658434\n",
            " - layer2_input.weight grad norm: 0.5284745693206787\n",
            " - layer2_input.bias grad norm: 0.0006153781432658434\n",
            " - layer3.weight grad norm: 0.002527813892811537\n",
            " - layer3.bias grad norm: 0.000592975236941129\n",
            " - layer3_input.weight grad norm: 0.5077332258224487\n",
            " - layer3_input.bias grad norm: 0.000592975236941129\n",
            " - layer4.weight grad norm: 0.003031162777915597\n",
            " - layer4.bias grad norm: 0.0005710689583793283\n",
            " - layer4_input.weight grad norm: 0.4894185960292816\n",
            " - layer4_input.bias grad norm: 0.0005710689583793283\n",
            " - layer5.weight grad norm: 0.011690220795571804\n",
            " - layer5.bias grad norm: 0.00516753364354372\n",
            "Gradients at iteration 294:\n",
            " - layer1.weight grad norm: 0.5243645310401917\n",
            " - layer1.bias grad norm: 0.0006114504649303854\n",
            " - layer2.weight grad norm: 0.0018815032672137022\n",
            " - layer2.bias grad norm: 0.0005987841868773103\n",
            " - layer2_input.weight grad norm: 0.5128326416015625\n",
            " - layer2_input.bias grad norm: 0.0005987841868773103\n",
            " - layer3.weight grad norm: 0.002614441094920039\n",
            " - layer3.bias grad norm: 0.0005745566450059414\n",
            " - layer3_input.weight grad norm: 0.4913836121559143\n",
            " - layer3_input.bias grad norm: 0.0005745566450059414\n",
            " - layer4.weight grad norm: 0.0030533194076269865\n",
            " - layer4.bias grad norm: 0.0005478384555317461\n",
            " - layer4_input.weight grad norm: 0.46947023272514343\n",
            " - layer4_input.bias grad norm: 0.0005478384555317461\n",
            " - layer5.weight grad norm: 0.011581487953662872\n",
            " - layer5.bias grad norm: 0.00530477799475193\n",
            "Gradients at iteration 295:\n",
            " - layer1.weight grad norm: 0.5087295174598694\n",
            " - layer1.bias grad norm: 0.0005797565099783242\n",
            " - layer2.weight grad norm: 0.0019092721631750464\n",
            " - layer2.bias grad norm: 0.0005973962252028286\n",
            " - layer2_input.weight grad norm: 0.5265873074531555\n",
            " - layer2_input.bias grad norm: 0.0005973962252028286\n",
            " - layer3.weight grad norm: 0.0026333339046686888\n",
            " - layer3.bias grad norm: 0.0005266686785034835\n",
            " - layer3_input.weight grad norm: 0.46619340777397156\n",
            " - layer3_input.bias grad norm: 0.0005266686785034835\n",
            " - layer4.weight grad norm: 0.0030811303295195103\n",
            " - layer4.bias grad norm: 0.0005688833189196885\n",
            " - layer4_input.weight grad norm: 0.4963257312774658\n",
            " - layer4_input.bias grad norm: 0.0005688833189196885\n",
            " - layer5.weight grad norm: 0.013166721910238266\n",
            " - layer5.bias grad norm: 0.005377389490604401\n",
            "Gradients at iteration 296:\n",
            " - layer1.weight grad norm: 0.5087648630142212\n",
            " - layer1.bias grad norm: 0.0005805699620395899\n",
            " - layer2.weight grad norm: 0.001964343013241887\n",
            " - layer2.bias grad norm: 0.0005338328192010522\n",
            " - layer2_input.weight grad norm: 0.4735596477985382\n",
            " - layer2_input.bias grad norm: 0.0005338328192010522\n",
            " - layer3.weight grad norm: 0.002714641159400344\n",
            " - layer3.bias grad norm: 0.0006226219120435417\n",
            " - layer3_input.weight grad norm: 0.5361414551734924\n",
            " - layer3_input.bias grad norm: 0.0006226219120435417\n",
            " - layer4.weight grad norm: 0.0032828182447701693\n",
            " - layer4.bias grad norm: 0.0005511341150850058\n",
            " - layer4_input.weight grad norm: 0.47878339886665344\n",
            " - layer4_input.bias grad norm: 0.0005511341150850058\n",
            " - layer5.weight grad norm: 0.01277067419141531\n",
            " - layer5.bias grad norm: 0.005560619290918112\n",
            "Gradients at iteration 297:\n",
            " - layer1.weight grad norm: 0.5216659903526306\n",
            " - layer1.bias grad norm: 0.0006136114243417978\n",
            " - layer2.weight grad norm: 0.001826732070185244\n",
            " - layer2.bias grad norm: 0.0006011290824972093\n",
            " - layer2_input.weight grad norm: 0.511891782283783\n",
            " - layer2_input.bias grad norm: 0.0006011290824972093\n",
            " - layer3.weight grad norm: 0.0025085443630814552\n",
            " - layer3.bias grad norm: 0.0005942845018580556\n",
            " - layer3_input.weight grad norm: 0.503706693649292\n",
            " - layer3_input.bias grad norm: 0.0005942845018580556\n",
            " - layer4.weight grad norm: 0.003016549861058593\n",
            " - layer4.bias grad norm: 0.0005347803817130625\n",
            " - layer4_input.weight grad norm: 0.4603387117385864\n",
            " - layer4_input.bias grad norm: 0.0005347803817130625\n",
            " - layer5.weight grad norm: 0.012314747087657452\n",
            " - layer5.bias grad norm: 0.005136142484843731\n",
            "Gradients at iteration 298:\n",
            " - layer1.weight grad norm: 0.502483606338501\n",
            " - layer1.bias grad norm: 0.0005825085681863129\n",
            " - layer2.weight grad norm: 0.0018312856554985046\n",
            " - layer2.bias grad norm: 0.0006265260744839907\n",
            " - layer2_input.weight grad norm: 0.5352914333343506\n",
            " - layer2_input.bias grad norm: 0.0006265260744839907\n",
            " - layer3.weight grad norm: 0.0025776810944080353\n",
            " - layer3.bias grad norm: 0.0005903707933612168\n",
            " - layer3_input.weight grad norm: 0.508033037185669\n",
            " - layer3_input.bias grad norm: 0.0005903707933612168\n",
            " - layer4.weight grad norm: 0.003034670138731599\n",
            " - layer4.bias grad norm: 0.0005157621926628053\n",
            " - layer4_input.weight grad norm: 0.45020589232444763\n",
            " - layer4_input.bias grad norm: 0.0005157621926628053\n",
            " - layer5.weight grad norm: 0.011916812509298325\n",
            " - layer5.bias grad norm: 0.0051766992546617985\n",
            "Gradients at iteration 299:\n",
            " - layer1.weight grad norm: 0.5382019877433777\n",
            " - layer1.bias grad norm: 0.0006304476992227137\n",
            " - layer2.weight grad norm: 0.001920531620271504\n",
            " - layer2.bias grad norm: 0.000592248747125268\n",
            " - layer2_input.weight grad norm: 0.5113181471824646\n",
            " - layer2_input.bias grad norm: 0.000592248747125268\n",
            " - layer3.weight grad norm: 0.0025601021479815245\n",
            " - layer3.bias grad norm: 0.0005296249291859567\n",
            " - layer3_input.weight grad norm: 0.4590401351451874\n",
            " - layer3_input.bias grad norm: 0.0005296249291859567\n",
            " - layer4.weight grad norm: 0.0030709486454725266\n",
            " - layer4.bias grad norm: 0.0005707809468731284\n",
            " - layer4_input.weight grad norm: 0.48782768845558167\n",
            " - layer4_input.bias grad norm: 0.0005707809468731284\n",
            " - layer5.weight grad norm: 0.012188650667667389\n",
            " - layer5.bias grad norm: 0.005296410992741585\n",
            "Gradients at iteration 300:\n",
            " - layer1.weight grad norm: 0.5167171955108643\n",
            " - layer1.bias grad norm: 0.0005947772297076881\n",
            " - layer2.weight grad norm: 0.001947954879142344\n",
            " - layer2.bias grad norm: 0.0006182525539770722\n",
            " - layer2_input.weight grad norm: 0.5314512252807617\n",
            " - layer2_input.bias grad norm: 0.0006182525539770722\n",
            " - layer3.weight grad norm: 0.002684536622837186\n",
            " - layer3.bias grad norm: 0.0005522941355593503\n",
            " - layer3_input.weight grad norm: 0.48170438408851624\n",
            " - layer3_input.bias grad norm: 0.0005522941355593503\n",
            " - layer4.weight grad norm: 0.0032021065708249807\n",
            " - layer4.bias grad norm: 0.0005375640466809273\n",
            " - layer4_input.weight grad norm: 0.46726107597351074\n",
            " - layer4_input.bias grad norm: 0.0005375640466809273\n",
            " - layer5.weight grad norm: 0.011717022396624088\n",
            " - layer5.bias grad norm: 0.0054786233231425285\n",
            "It: 300, Loss: 5.278e+13, Y0: 0.063, Time: 1.61, Learning Rate: 1.000e-03\n",
            "Gradients at iteration 301:\n",
            " - layer1.weight grad norm: 0.5082994699478149\n",
            " - layer1.bias grad norm: 0.0005845657433383167\n",
            " - layer2.weight grad norm: 0.0019245707662776113\n",
            " - layer2.bias grad norm: 0.0005598595598712564\n",
            " - layer2_input.weight grad norm: 0.4876507520675659\n",
            " - layer2_input.bias grad norm: 0.0005598595598712564\n",
            " - layer3.weight grad norm: 0.0026369672268629074\n",
            " - layer3.bias grad norm: 0.0005731842247769237\n",
            " - layer3_input.weight grad norm: 0.49612724781036377\n",
            " - layer3_input.bias grad norm: 0.0005731842247769237\n",
            " - layer4.weight grad norm: 0.003205258399248123\n",
            " - layer4.bias grad norm: 0.0005906747537665069\n",
            " - layer4_input.weight grad norm: 0.5074036717414856\n",
            " - layer4_input.bias grad norm: 0.0005906747537665069\n",
            " - layer5.weight grad norm: 0.013225221075117588\n",
            " - layer5.bias grad norm: 0.00542272487655282\n",
            "Gradients at iteration 302:\n",
            " - layer1.weight grad norm: 0.5212563276290894\n",
            " - layer1.bias grad norm: 0.0006131533300504088\n",
            " - layer2.weight grad norm: 0.0018466789042577147\n",
            " - layer2.bias grad norm: 0.0006054125260561705\n",
            " - layer2_input.weight grad norm: 0.5169695019721985\n",
            " - layer2_input.bias grad norm: 0.0006054125260561705\n",
            " - layer3.weight grad norm: 0.0025952314026653767\n",
            " - layer3.bias grad norm: 0.0005267269443720579\n",
            " - layer3_input.weight grad norm: 0.45465755462646484\n",
            " - layer3_input.bias grad norm: 0.0005267269443720579\n",
            " - layer4.weight grad norm: 0.003065062453970313\n",
            " - layer4.bias grad norm: 0.0005914875655435026\n",
            " - layer4_input.weight grad norm: 0.5041073560714722\n",
            " - layer4_input.bias grad norm: 0.0005914875655435026\n",
            " - layer5.weight grad norm: 0.012118630111217499\n",
            " - layer5.bias grad norm: 0.005280505400151014\n",
            "Gradients at iteration 303:\n",
            " - layer1.weight grad norm: 0.515729546546936\n",
            " - layer1.bias grad norm: 0.0005957764224149287\n",
            " - layer2.weight grad norm: 0.002021052176132798\n",
            " - layer2.bias grad norm: 0.0005931776249781251\n",
            " - layer2_input.weight grad norm: 0.5123857855796814\n",
            " - layer2_input.bias grad norm: 0.0005931776249781251\n",
            " - layer3.weight grad norm: 0.002753553679212928\n",
            " - layer3.bias grad norm: 0.0005927082384005189\n",
            " - layer3_input.weight grad norm: 0.5133447051048279\n",
            " - layer3_input.bias grad norm: 0.0005927082384005189\n",
            " - layer4.weight grad norm: 0.003259173361584544\n",
            " - layer4.bias grad norm: 0.0005208989605307579\n",
            " - layer4_input.weight grad norm: 0.4557774066925049\n",
            " - layer4_input.bias grad norm: 0.0005208989605307579\n",
            " - layer5.weight grad norm: 0.013092104345560074\n",
            " - layer5.bias grad norm: 0.005657253321260214\n",
            "Gradients at iteration 304:\n",
            " - layer1.weight grad norm: 0.48323139548301697\n",
            " - layer1.bias grad norm: 0.0005490388139151037\n",
            " - layer2.weight grad norm: 0.0018818009411916137\n",
            " - layer2.bias grad norm: 0.0005853521870449185\n",
            " - layer2_input.weight grad norm: 0.508953869342804\n",
            " - layer2_input.bias grad norm: 0.0005853521870449185\n",
            " - layer3.weight grad norm: 0.002580259693786502\n",
            " - layer3.bias grad norm: 0.000598766200710088\n",
            " - layer3_input.weight grad norm: 0.5139001607894897\n",
            " - layer3_input.bias grad norm: 0.000598766200710088\n",
            " - layer4.weight grad norm: 0.00313008320517838\n",
            " - layer4.bias grad norm: 0.0005660004680976272\n",
            " - layer4_input.weight grad norm: 0.4930901825428009\n",
            " - layer4_input.bias grad norm: 0.0005660004680976272\n",
            " - layer5.weight grad norm: 0.013098535127937794\n",
            " - layer5.bias grad norm: 0.005303291138261557\n",
            "Gradients at iteration 305:\n",
            " - layer1.weight grad norm: 0.5116487741470337\n",
            " - layer1.bias grad norm: 0.0005924879224039614\n",
            " - layer2.weight grad norm: 0.0018987542716786265\n",
            " - layer2.bias grad norm: 0.0005918770330026746\n",
            " - layer2_input.weight grad norm: 0.5100629925727844\n",
            " - layer2_input.bias grad norm: 0.0005918770330026746\n",
            " - layer3.weight grad norm: 0.002613555872812867\n",
            " - layer3.bias grad norm: 0.0006098888115957379\n",
            " - layer3_input.weight grad norm: 0.5221750140190125\n",
            " - layer3_input.bias grad norm: 0.0006098888115957379\n",
            " - layer4.weight grad norm: 0.0031865134369581938\n",
            " - layer4.bias grad norm: 0.0005254175048321486\n",
            " - layer4_input.weight grad norm: 0.4529685080051422\n",
            " - layer4_input.bias grad norm: 0.0005254175048321486\n",
            " - layer5.weight grad norm: 0.012312168255448341\n",
            " - layer5.bias grad norm: 0.005425027571618557\n",
            "Gradients at iteration 306:\n",
            " - layer1.weight grad norm: 0.5402142405509949\n",
            " - layer1.bias grad norm: 0.0006294565391726792\n",
            " - layer2.weight grad norm: 0.0019340717699378729\n",
            " - layer2.bias grad norm: 0.000564664020203054\n",
            " - layer2_input.weight grad norm: 0.48913487792015076\n",
            " - layer2_input.bias grad norm: 0.000564664020203054\n",
            " - layer3.weight grad norm: 0.0026340477634221315\n",
            " - layer3.bias grad norm: 0.0005696634179912508\n",
            " - layer3_input.weight grad norm: 0.49172666668891907\n",
            " - layer3_input.bias grad norm: 0.0005696634179912508\n",
            " - layer4.weight grad norm: 0.0031144199892878532\n",
            " - layer4.bias grad norm: 0.0005517626996152103\n",
            " - layer4_input.weight grad norm: 0.476351261138916\n",
            " - layer4_input.bias grad norm: 0.0005517626996152103\n",
            " - layer5.weight grad norm: 0.012569766491651535\n",
            " - layer5.bias grad norm: 0.005410879384726286\n",
            "Gradients at iteration 307:\n",
            " - layer1.weight grad norm: 0.5615425705909729\n",
            " - layer1.bias grad norm: 0.0006650259019806981\n",
            " - layer2.weight grad norm: 0.0018745259149000049\n",
            " - layer2.bias grad norm: 0.0006405232707038522\n",
            " - layer2_input.weight grad norm: 0.542844295501709\n",
            " - layer2_input.bias grad norm: 0.0006405232707038522\n",
            " - layer3.weight grad norm: 0.002597066340968013\n",
            " - layer3.bias grad norm: 0.0005171722150407732\n",
            " - layer3_input.weight grad norm: 0.45058590173721313\n",
            " - layer3_input.bias grad norm: 0.0005171722150407732\n",
            " - layer4.weight grad norm: 0.0030554328113794327\n",
            " - layer4.bias grad norm: 0.0004971318994648755\n",
            " - layer4_input.weight grad norm: 0.43214181065559387\n",
            " - layer4_input.bias grad norm: 0.0004971318994648755\n",
            " - layer5.weight grad norm: 0.012891709804534912\n",
            " - layer5.bias grad norm: 0.005273804534226656\n",
            "Gradients at iteration 308:\n",
            " - layer1.weight grad norm: 0.5104327201843262\n",
            " - layer1.bias grad norm: 0.0005877989460714161\n",
            " - layer2.weight grad norm: 0.00200297380797565\n",
            " - layer2.bias grad norm: 0.0006204828387126327\n",
            " - layer2_input.weight grad norm: 0.531364917755127\n",
            " - layer2_input.bias grad norm: 0.0006204828387126327\n",
            " - layer3.weight grad norm: 0.00274266442283988\n",
            " - layer3.bias grad norm: 0.0005321821663528681\n",
            " - layer3_input.weight grad norm: 0.4634716808795929\n",
            " - layer3_input.bias grad norm: 0.0005321821663528681\n",
            " - layer4.weight grad norm: 0.003307323670014739\n",
            " - layer4.bias grad norm: 0.000573091849219054\n",
            " - layer4_input.weight grad norm: 0.49202388525009155\n",
            " - layer4_input.bias grad norm: 0.000573091849219054\n",
            " - layer5.weight grad norm: 0.01266271248459816\n",
            " - layer5.bias grad norm: 0.005573905538767576\n",
            "Gradients at iteration 309:\n",
            " - layer1.weight grad norm: 0.5461307168006897\n",
            " - layer1.bias grad norm: 0.0006282811518758535\n",
            " - layer2.weight grad norm: 0.001919672591611743\n",
            " - layer2.bias grad norm: 0.0005814381875097752\n",
            " - layer2_input.weight grad norm: 0.5077944993972778\n",
            " - layer2_input.bias grad norm: 0.0005814381875097752\n",
            " - layer3.weight grad norm: 0.002654322888702154\n",
            " - layer3.bias grad norm: 0.0005731640267185867\n",
            " - layer3_input.weight grad norm: 0.4972805380821228\n",
            " - layer3_input.bias grad norm: 0.0005731640267185867\n",
            " - layer4.weight grad norm: 0.0031716928351670504\n",
            " - layer4.bias grad norm: 0.0004981138044968247\n",
            " - layer4_input.weight grad norm: 0.4431825578212738\n",
            " - layer4_input.bias grad norm: 0.0004981138044968247\n",
            " - layer5.weight grad norm: 0.011586886830627918\n",
            " - layer5.bias grad norm: 0.005465845577418804\n",
            "Gradients at iteration 310:\n",
            " - layer1.weight grad norm: 0.5192713141441345\n",
            " - layer1.bias grad norm: 0.0006021775770932436\n",
            " - layer2.weight grad norm: 0.001890888437628746\n",
            " - layer2.bias grad norm: 0.0006108751404099166\n",
            " - layer2_input.weight grad norm: 0.5248588919639587\n",
            " - layer2_input.bias grad norm: 0.0006108751404099166\n",
            " - layer3.weight grad norm: 0.002647843910381198\n",
            " - layer3.bias grad norm: 0.0005925596342422068\n",
            " - layer3_input.weight grad norm: 0.5110068917274475\n",
            " - layer3_input.bias grad norm: 0.0005925596342422068\n",
            " - layer4.weight grad norm: 0.0030910675413906574\n",
            " - layer4.bias grad norm: 0.0005020644748583436\n",
            " - layer4_input.weight grad norm: 0.4399318993091583\n",
            " - layer4_input.bias grad norm: 0.0005020644748583436\n",
            " - layer5.weight grad norm: 0.012703542597591877\n",
            " - layer5.bias grad norm: 0.0053235325030982494\n",
            "Gradients at iteration 311:\n",
            " - layer1.weight grad norm: 0.5121400952339172\n",
            " - layer1.bias grad norm: 0.0005996925174258649\n",
            " - layer2.weight grad norm: 0.0018829237669706345\n",
            " - layer2.bias grad norm: 0.0006178258918225765\n",
            " - layer2_input.weight grad norm: 0.5267408490180969\n",
            " - layer2_input.bias grad norm: 0.0006178258918225765\n",
            " - layer3.weight grad norm: 0.0025650816969573498\n",
            " - layer3.bias grad norm: 0.000568523071706295\n",
            " - layer3_input.weight grad norm: 0.4903750717639923\n",
            " - layer3_input.bias grad norm: 0.000568523071706295\n",
            " - layer4.weight grad norm: 0.003041687188670039\n",
            " - layer4.bias grad norm: 0.0005487782182171941\n",
            " - layer4_input.weight grad norm: 0.4685952067375183\n",
            " - layer4_input.bias grad norm: 0.0005487782182171941\n",
            " - layer5.weight grad norm: 0.01257247757166624\n",
            " - layer5.bias grad norm: 0.005256107542663813\n",
            "Gradients at iteration 312:\n",
            " - layer1.weight grad norm: 0.5126504302024841\n",
            " - layer1.bias grad norm: 0.0005930872866883874\n",
            " - layer2.weight grad norm: 0.001851845532655716\n",
            " - layer2.bias grad norm: 0.000584710156545043\n",
            " - layer2_input.weight grad norm: 0.5014359951019287\n",
            " - layer2_input.bias grad norm: 0.000584710156545043\n",
            " - layer3.weight grad norm: 0.0025883326306939125\n",
            " - layer3.bias grad norm: 0.0005714052240364254\n",
            " - layer3_input.weight grad norm: 0.4945690631866455\n",
            " - layer3_input.bias grad norm: 0.0005714052240364254\n",
            " - layer4.weight grad norm: 0.003067394020035863\n",
            " - layer4.bias grad norm: 0.0005716737359762192\n",
            " - layer4_input.weight grad norm: 0.4908726215362549\n",
            " - layer4_input.bias grad norm: 0.0005716737359762192\n",
            " - layer5.weight grad norm: 0.012146394699811935\n",
            " - layer5.bias grad norm: 0.005266654770821333\n",
            "Gradients at iteration 313:\n",
            " - layer1.weight grad norm: 0.48442432284355164\n",
            " - layer1.bias grad norm: 0.0005520130507647991\n",
            " - layer2.weight grad norm: 0.0019502131035551429\n",
            " - layer2.bias grad norm: 0.0005369820282794535\n",
            " - layer2_input.weight grad norm: 0.4728289544582367\n",
            " - layer2_input.bias grad norm: 0.0005369820282794535\n",
            " - layer3.weight grad norm: 0.00267890770919621\n",
            " - layer3.bias grad norm: 0.0006357523379847407\n",
            " - layer3_input.weight grad norm: 0.5403788089752197\n",
            " - layer3_input.bias grad norm: 0.0006357523379847407\n",
            " - layer4.weight grad norm: 0.0031753736548125744\n",
            " - layer4.bias grad norm: 0.0005803104722872376\n",
            " - layer4_input.weight grad norm: 0.49955734610557556\n",
            " - layer4_input.bias grad norm: 0.0005803104722872376\n",
            " - layer5.weight grad norm: 0.012080696411430836\n",
            " - layer5.bias grad norm: 0.0054527767933905125\n",
            "Gradients at iteration 314:\n",
            " - layer1.weight grad norm: 0.5063683986663818\n",
            " - layer1.bias grad norm: 0.0005791529547423124\n",
            " - layer2.weight grad norm: 0.0018582487246021628\n",
            " - layer2.bias grad norm: 0.0005754513549618423\n",
            " - layer2_input.weight grad norm: 0.5020067095756531\n",
            " - layer2_input.bias grad norm: 0.0005754513549618423\n",
            " - layer3.weight grad norm: 0.00253083067946136\n",
            " - layer3.bias grad norm: 0.0005788885755464435\n",
            " - layer3_input.weight grad norm: 0.4992140829563141\n",
            " - layer3_input.bias grad norm: 0.0005788885755464435\n",
            " - layer4.weight grad norm: 0.003047687467187643\n",
            " - layer4.bias grad norm: 0.0005689066019840539\n",
            " - layer4_input.weight grad norm: 0.49212634563446045\n",
            " - layer4_input.bias grad norm: 0.0005689066019840539\n",
            " - layer5.weight grad norm: 0.011342167854309082\n",
            " - layer5.bias grad norm: 0.005209770984947681\n",
            "Gradients at iteration 315:\n",
            " - layer1.weight grad norm: 0.5093400478363037\n",
            " - layer1.bias grad norm: 0.0005909789470024407\n",
            " - layer2.weight grad norm: 0.001813058159314096\n",
            " - layer2.bias grad norm: 0.0005482104606926441\n",
            " - layer2_input.weight grad norm: 0.47950175404548645\n",
            " - layer2_input.bias grad norm: 0.0005482104606926441\n",
            " - layer3.weight grad norm: 0.0025366295594722033\n",
            " - layer3.bias grad norm: 0.0005770214484073222\n",
            " - layer3_input.weight grad norm: 0.49777767062187195\n",
            " - layer3_input.bias grad norm: 0.0005770214484073222\n",
            " - layer4.weight grad norm: 0.0030543056782335043\n",
            " - layer4.bias grad norm: 0.0006012402009218931\n",
            " - layer4_input.weight grad norm: 0.5125163197517395\n",
            " - layer4_input.bias grad norm: 0.0006012402009218931\n",
            " - layer5.weight grad norm: 0.012138638645410538\n",
            " - layer5.bias grad norm: 0.0051526520401239395\n",
            "Gradients at iteration 316:\n",
            " - layer1.weight grad norm: 0.5112736225128174\n",
            " - layer1.bias grad norm: 0.0005950573249720037\n",
            " - layer2.weight grad norm: 0.0019147151615470648\n",
            " - layer2.bias grad norm: 0.0005867786821909249\n",
            " - layer2_input.weight grad norm: 0.5038545727729797\n",
            " - layer2_input.bias grad norm: 0.0005867786821909249\n",
            " - layer3.weight grad norm: 0.0026227927301079035\n",
            " - layer3.bias grad norm: 0.0006210615392774343\n",
            " - layer3_input.weight grad norm: 0.5266686081886292\n",
            " - layer3_input.bias grad norm: 0.0006210615392774343\n",
            " - layer4.weight grad norm: 0.003199064638465643\n",
            " - layer4.bias grad norm: 0.0005233559641055763\n",
            " - layer4_input.weight grad norm: 0.45514237880706787\n",
            " - layer4_input.bias grad norm: 0.0005233559641055763\n",
            " - layer5.weight grad norm: 0.01198007632046938\n",
            " - layer5.bias grad norm: 0.0053766570053994656\n",
            "Gradients at iteration 317:\n",
            " - layer1.weight grad norm: 0.5185408592224121\n",
            " - layer1.bias grad norm: 0.0005973245715722442\n",
            " - layer2.weight grad norm: 0.0019048009999096394\n",
            " - layer2.bias grad norm: 0.0005550933419726789\n",
            " - layer2_input.weight grad norm: 0.4853414297103882\n",
            " - layer2_input.bias grad norm: 0.0005550933419726789\n",
            " - layer3.weight grad norm: 0.0026783589273691177\n",
            " - layer3.bias grad norm: 0.0005257033044472337\n",
            " - layer3_input.weight grad norm: 0.4602581858634949\n",
            " - layer3_input.bias grad norm: 0.0005257033044472337\n",
            " - layer4.weight grad norm: 0.0031415470875799656\n",
            " - layer4.bias grad norm: 0.0006229642312973738\n",
            " - layer4_input.weight grad norm: 0.532477080821991\n",
            " - layer4_input.bias grad norm: 0.0006229642312973738\n",
            " - layer5.weight grad norm: 0.01171829178929329\n",
            " - layer5.bias grad norm: 0.005429849959909916\n",
            "Gradients at iteration 318:\n",
            " - layer1.weight grad norm: 0.5459643006324768\n",
            " - layer1.bias grad norm: 0.0006479414296336472\n",
            " - layer2.weight grad norm: 0.0018209025729447603\n",
            " - layer2.bias grad norm: 0.0005910023464821279\n",
            " - layer2_input.weight grad norm: 0.5027234554290771\n",
            " - layer2_input.bias grad norm: 0.0005910023464821279\n",
            " - layer3.weight grad norm: 0.0024763725232332945\n",
            " - layer3.bias grad norm: 0.0005232642870396376\n",
            " - layer3_input.weight grad norm: 0.45323073863983154\n",
            " - layer3_input.bias grad norm: 0.0005232642870396376\n",
            " - layer4.weight grad norm: 0.002967570209875703\n",
            " - layer4.bias grad norm: 0.000580323045141995\n",
            " - layer4_input.weight grad norm: 0.4935716986656189\n",
            " - layer4_input.bias grad norm: 0.000580323045141995\n",
            " - layer5.weight grad norm: 0.010680667124688625\n",
            " - layer5.bias grad norm: 0.00512507651001215\n",
            "Gradients at iteration 319:\n",
            " - layer1.weight grad norm: 0.49218785762786865\n",
            " - layer1.bias grad norm: 0.0005655494751408696\n",
            " - layer2.weight grad norm: 0.0018050663638859987\n",
            " - layer2.bias grad norm: 0.0006240346119739115\n",
            " - layer2_input.weight grad norm: 0.5336076021194458\n",
            " - layer2_input.bias grad norm: 0.0006240346119739115\n",
            " - layer3.weight grad norm: 0.002468072110787034\n",
            " - layer3.bias grad norm: 0.0005656522116623819\n",
            " - layer3_input.weight grad norm: 0.487544447183609\n",
            " - layer3_input.bias grad norm: 0.0005656522116623819\n",
            " - layer4.weight grad norm: 0.0029435993637889624\n",
            " - layer4.bias grad norm: 0.0005629367660731077\n",
            " - layer4_input.weight grad norm: 0.4849160611629486\n",
            " - layer4_input.bias grad norm: 0.0005629367660731077\n",
            " - layer5.weight grad norm: 0.01117858849465847\n",
            " - layer5.bias grad norm: 0.005065766163170338\n",
            "Gradients at iteration 320:\n",
            " - layer1.weight grad norm: 0.5131645798683167\n",
            " - layer1.bias grad norm: 0.0005955246742814779\n",
            " - layer2.weight grad norm: 0.0018408488249406219\n",
            " - layer2.bias grad norm: 0.0006210901192389429\n",
            " - layer2_input.weight grad norm: 0.5298300981521606\n",
            " - layer2_input.bias grad norm: 0.0006210901192389429\n",
            " - layer3.weight grad norm: 0.002601384650915861\n",
            " - layer3.bias grad norm: 0.0006038093124516308\n",
            " - layer3_input.weight grad norm: 0.513955295085907\n",
            " - layer3_input.bias grad norm: 0.0006038093124516308\n",
            " - layer4.weight grad norm: 0.00310564786195755\n",
            " - layer4.bias grad norm: 0.0005013222107663751\n",
            " - layer4_input.weight grad norm: 0.4377249777317047\n",
            " - layer4_input.bias grad norm: 0.0005013222107663751\n",
            " - layer5.weight grad norm: 0.011770447716116905\n",
            " - layer5.bias grad norm: 0.005321168806403875\n",
            "Gradients at iteration 321:\n",
            " - layer1.weight grad norm: 0.5139740109443665\n",
            " - layer1.bias grad norm: 0.0005899009411223233\n",
            " - layer2.weight grad norm: 0.0018911358201876283\n",
            " - layer2.bias grad norm: 0.0006233882158994675\n",
            " - layer2_input.weight grad norm: 0.5365210771560669\n",
            " - layer2_input.bias grad norm: 0.0006233882158994675\n",
            " - layer3.weight grad norm: 0.0026460266672074795\n",
            " - layer3.bias grad norm: 0.0005482733249664307\n",
            " - layer3_input.weight grad norm: 0.4806012809276581\n",
            " - layer3_input.bias grad norm: 0.0005482733249664307\n",
            " - layer4.weight grad norm: 0.0031586848199367523\n",
            " - layer4.bias grad norm: 0.0005396788474172354\n",
            " - layer4_input.weight grad norm: 0.4656282365322113\n",
            " - layer4_input.bias grad norm: 0.0005396788474172354\n",
            " - layer5.weight grad norm: 0.011698378250002861\n",
            " - layer5.bias grad norm: 0.005378736183047295\n",
            "Gradients at iteration 322:\n",
            " - layer1.weight grad norm: 0.4912400245666504\n",
            " - layer1.bias grad norm: 0.0005648262449540198\n",
            " - layer2.weight grad norm: 0.0018697278574109077\n",
            " - layer2.bias grad norm: 0.0006466993363574147\n",
            " - layer2_input.weight grad norm: 0.5494958162307739\n",
            " - layer2_input.bias grad norm: 0.0006466993363574147\n",
            " - layer3.weight grad norm: 0.0026050845626741648\n",
            " - layer3.bias grad norm: 0.0005790796712972224\n",
            " - layer3_input.weight grad norm: 0.49636709690093994\n",
            " - layer3_input.bias grad norm: 0.0005790796712972224\n",
            " - layer4.weight grad norm: 0.003112269565463066\n",
            " - layer4.bias grad norm: 0.0005307301180437207\n",
            " - layer4_input.weight grad norm: 0.4584527313709259\n",
            " - layer4_input.bias grad norm: 0.0005307301180437207\n",
            " - layer5.weight grad norm: 0.01132869627326727\n",
            " - layer5.bias grad norm: 0.00526778306812048\n",
            "Gradients at iteration 323:\n",
            " - layer1.weight grad norm: 0.5003425478935242\n",
            " - layer1.bias grad norm: 0.0005774425226263702\n",
            " - layer2.weight grad norm: 0.001852935180068016\n",
            " - layer2.bias grad norm: 0.0005751934950239956\n",
            " - layer2_input.weight grad norm: 0.49405619502067566\n",
            " - layer2_input.bias grad norm: 0.0005751934950239956\n",
            " - layer3.weight grad norm: 0.002615481149405241\n",
            " - layer3.bias grad norm: 0.0006072917021811008\n",
            " - layer3_input.weight grad norm: 0.516189694404602\n",
            " - layer3_input.bias grad norm: 0.0006072917021811008\n",
            " - layer4.weight grad norm: 0.003138602478429675\n",
            " - layer4.bias grad norm: 0.000575023703277111\n",
            " - layer4_input.weight grad norm: 0.4888022541999817\n",
            " - layer4_input.bias grad norm: 0.000575023703277111\n",
            " - layer5.weight grad norm: 0.011624924838542938\n",
            " - layer5.bias grad norm: 0.005343349650502205\n",
            "Gradients at iteration 324:\n",
            " - layer1.weight grad norm: 0.5134404897689819\n",
            " - layer1.bias grad norm: 0.0005907901795580983\n",
            " - layer2.weight grad norm: 0.001861008582636714\n",
            " - layer2.bias grad norm: 0.0005846094572916627\n",
            " - layer2_input.weight grad norm: 0.5038806796073914\n",
            " - layer2_input.bias grad norm: 0.0005846094572916627\n",
            " - layer3.weight grad norm: 0.002593225333839655\n",
            " - layer3.bias grad norm: 0.0005936469533480704\n",
            " - layer3_input.weight grad norm: 0.5096011757850647\n",
            " - layer3_input.bias grad norm: 0.0005936469533480704\n",
            " - layer4.weight grad norm: 0.0030817415099591017\n",
            " - layer4.bias grad norm: 0.0005461114342324436\n",
            " - layer4_input.weight grad norm: 0.47180041670799255\n",
            " - layer4_input.bias grad norm: 0.0005461114342324436\n",
            " - layer5.weight grad norm: 0.012014832347631454\n",
            " - layer5.bias grad norm: 0.005268854089081287\n",
            "Gradients at iteration 325:\n",
            " - layer1.weight grad norm: 0.5120947360992432\n",
            " - layer1.bias grad norm: 0.0005866624996997416\n",
            " - layer2.weight grad norm: 0.0018869265913963318\n",
            " - layer2.bias grad norm: 0.0006043359753675759\n",
            " - layer2_input.weight grad norm: 0.5244678854942322\n",
            " - layer2_input.bias grad norm: 0.0006043359753675759\n",
            " - layer3.weight grad norm: 0.0026044186670333147\n",
            " - layer3.bias grad norm: 0.0005838710349053144\n",
            " - layer3_input.weight grad norm: 0.5060814023017883\n",
            " - layer3_input.bias grad norm: 0.0005838710349053144\n",
            " - layer4.weight grad norm: 0.003082932671532035\n",
            " - layer4.bias grad norm: 0.0005150517099536955\n",
            " - layer4_input.weight grad norm: 0.4542927145957947\n",
            " - layer4_input.bias grad norm: 0.0005150517099536955\n",
            " - layer5.weight grad norm: 0.011936071328818798\n",
            " - layer5.bias grad norm: 0.005252072587609291\n",
            "Gradients at iteration 326:\n",
            " - layer1.weight grad norm: 0.5132765173912048\n",
            " - layer1.bias grad norm: 0.0005962519207969308\n",
            " - layer2.weight grad norm: 0.0019450377440080047\n",
            " - layer2.bias grad norm: 0.0006483009201474488\n",
            " - layer2_input.weight grad norm: 0.5467099547386169\n",
            " - layer2_input.bias grad norm: 0.0006483009201474488\n",
            " - layer3.weight grad norm: 0.0026932042092084885\n",
            " - layer3.bias grad norm: 0.0005502841668203473\n",
            " - layer3_input.weight grad norm: 0.47638264298439026\n",
            " - layer3_input.bias grad norm: 0.0005502841668203473\n",
            " - layer4.weight grad norm: 0.0031738393008708954\n",
            " - layer4.bias grad norm: 0.0005268211243674159\n",
            " - layer4_input.weight grad norm: 0.45879989862442017\n",
            " - layer4_input.bias grad norm: 0.0005268211243674159\n",
            " - layer5.weight grad norm: 0.012816714122891426\n",
            " - layer5.bias grad norm: 0.0054776789620518684\n",
            "Gradients at iteration 327:\n",
            " - layer1.weight grad norm: 0.5481944680213928\n",
            " - layer1.bias grad norm: 0.0006435852847062051\n",
            " - layer2.weight grad norm: 0.0018577156588435173\n",
            " - layer2.bias grad norm: 0.0005598336574621499\n",
            " - layer2_input.weight grad norm: 0.4837130904197693\n",
            " - layer2_input.bias grad norm: 0.0005598336574621499\n",
            " - layer3.weight grad norm: 0.002560144755989313\n",
            " - layer3.bias grad norm: 0.0005540098063647747\n",
            " - layer3_input.weight grad norm: 0.4773179590702057\n",
            " - layer3_input.bias grad norm: 0.0005540098063647747\n",
            " - layer4.weight grad norm: 0.003070787526667118\n",
            " - layer4.bias grad norm: 0.0005669047241099179\n",
            " - layer4_input.weight grad norm: 0.48730260133743286\n",
            " - layer4_input.bias grad norm: 0.0005669047241099179\n",
            " - layer5.weight grad norm: 0.012618782930076122\n",
            " - layer5.bias grad norm: 0.0052163624204695225\n",
            "Gradients at iteration 328:\n",
            " - layer1.weight grad norm: 0.49362459778785706\n",
            " - layer1.bias grad norm: 0.0005604938487522304\n",
            " - layer2.weight grad norm: 0.0019962729420512915\n",
            " - layer2.bias grad norm: 0.000613091338891536\n",
            " - layer2_input.weight grad norm: 0.523632287979126\n",
            " - layer2_input.bias grad norm: 0.000613091338891536\n",
            " - layer3.weight grad norm: 0.002730028936639428\n",
            " - layer3.bias grad norm: 0.0005767060210928321\n",
            " - layer3_input.weight grad norm: 0.5012717843055725\n",
            " - layer3_input.bias grad norm: 0.0005767060210928321\n",
            " - layer4.weight grad norm: 0.0033318214118480682\n",
            " - layer4.bias grad norm: 0.0005470715113915503\n",
            " - layer4_input.weight grad norm: 0.48022568225860596\n",
            " - layer4_input.bias grad norm: 0.0005470715113915503\n",
            " - layer5.weight grad norm: 0.014049499295651913\n",
            " - layer5.bias grad norm: 0.005611311178654432\n",
            "Gradients at iteration 329:\n",
            " - layer1.weight grad norm: 0.5664011240005493\n",
            " - layer1.bias grad norm: 0.0006607586983591318\n",
            " - layer2.weight grad norm: 0.001919290516525507\n",
            " - layer2.bias grad norm: 0.000601663370616734\n",
            " - layer2_input.weight grad norm: 0.5163466334342957\n",
            " - layer2_input.bias grad norm: 0.000601663370616734\n",
            " - layer3.weight grad norm: 0.0026546346489340067\n",
            " - layer3.bias grad norm: 0.0005196641432121396\n",
            " - layer3_input.weight grad norm: 0.4557301104068756\n",
            " - layer3_input.bias grad norm: 0.0005196641432121396\n",
            " - layer4.weight grad norm: 0.003160600084811449\n",
            " - layer4.bias grad norm: 0.0005154572427272797\n",
            " - layer4_input.weight grad norm: 0.45239800214767456\n",
            " - layer4_input.bias grad norm: 0.0005154572427272797\n",
            " - layer5.weight grad norm: 0.013044784776866436\n",
            " - layer5.bias grad norm: 0.00538446381688118\n",
            "Gradients at iteration 330:\n",
            " - layer1.weight grad norm: 0.5300007462501526\n",
            " - layer1.bias grad norm: 0.0006129087414592505\n",
            " - layer2.weight grad norm: 0.0019318517297506332\n",
            " - layer2.bias grad norm: 0.0005507140303961933\n",
            " - layer2_input.weight grad norm: 0.47807565331459045\n",
            " - layer2_input.bias grad norm: 0.0005507140303961933\n",
            " - layer3.weight grad norm: 0.0026524378918111324\n",
            " - layer3.bias grad norm: 0.0005986311007291079\n",
            " - layer3_input.weight grad norm: 0.5139905214309692\n",
            " - layer3_input.bias grad norm: 0.0005986311007291079\n",
            " - layer4.weight grad norm: 0.003143046284094453\n",
            " - layer4.bias grad norm: 0.0005521088023670018\n",
            " - layer4_input.weight grad norm: 0.4755496084690094\n",
            " - layer4_input.bias grad norm: 0.0005521088023670018\n",
            " - layer5.weight grad norm: 0.012525452300906181\n",
            " - layer5.bias grad norm: 0.0054236287251114845\n",
            "Gradients at iteration 331:\n",
            " - layer1.weight grad norm: 0.48228904604911804\n",
            " - layer1.bias grad norm: 0.0005512981442734599\n",
            " - layer2.weight grad norm: 0.0018351094331592321\n",
            " - layer2.bias grad norm: 0.0005978544359095395\n",
            " - layer2_input.weight grad norm: 0.5145745277404785\n",
            " - layer2_input.bias grad norm: 0.0005978544359095395\n",
            " - layer3.weight grad norm: 0.0025448608212172985\n",
            " - layer3.bias grad norm: 0.0006055989069864154\n",
            " - layer3_input.weight grad norm: 0.5187650918960571\n",
            " - layer3_input.bias grad norm: 0.0006055989069864154\n",
            " - layer4.weight grad norm: 0.0030105316545814276\n",
            " - layer4.bias grad norm: 0.0005619645235128701\n",
            " - layer4_input.weight grad norm: 0.4830038547515869\n",
            " - layer4_input.bias grad norm: 0.0005619645235128701\n",
            " - layer5.weight grad norm: 0.012328946962952614\n",
            " - layer5.bias grad norm: 0.005195039790123701\n",
            "Gradients at iteration 332:\n",
            " - layer1.weight grad norm: 0.5151604413986206\n",
            " - layer1.bias grad norm: 0.0005893739871680737\n",
            " - layer2.weight grad norm: 0.0019404409686103463\n",
            " - layer2.bias grad norm: 0.0005484945140779018\n",
            " - layer2_input.weight grad norm: 0.4842853546142578\n",
            " - layer2_input.bias grad norm: 0.0005484945140779018\n",
            " - layer3.weight grad norm: 0.0026445414405316114\n",
            " - layer3.bias grad norm: 0.0005807026755064726\n",
            " - layer3_input.weight grad norm: 0.5037934184074402\n",
            " - layer3_input.bias grad norm: 0.0005807026755064726\n",
            " - layer4.weight grad norm: 0.003170482348650694\n",
            " - layer4.bias grad norm: 0.0005726348608732224\n",
            " - layer4_input.weight grad norm: 0.49606260657310486\n",
            " - layer4_input.bias grad norm: 0.0005726348608732224\n",
            " - layer5.weight grad norm: 0.011779638938605785\n",
            " - layer5.bias grad norm: 0.005441417917609215\n",
            "Gradients at iteration 333:\n",
            " - layer1.weight grad norm: 0.5182088017463684\n",
            " - layer1.bias grad norm: 0.0006025336333550513\n",
            " - layer2.weight grad norm: 0.0019980059005320072\n",
            " - layer2.bias grad norm: 0.0006086676148697734\n",
            " - layer2_input.weight grad norm: 0.5221090912818909\n",
            " - layer2_input.bias grad norm: 0.0006086676148697734\n",
            " - layer3.weight grad norm: 0.0027706120163202286\n",
            " - layer3.bias grad norm: 0.0005996169638819993\n",
            " - layer3_input.weight grad norm: 0.5124791264533997\n",
            " - layer3_input.bias grad norm: 0.0005996169638819993\n",
            " - layer4.weight grad norm: 0.0032711648382246494\n",
            " - layer4.bias grad norm: 0.0005111439968459308\n",
            " - layer4_input.weight grad norm: 0.4427415132522583\n",
            " - layer4_input.bias grad norm: 0.0005111439968459308\n",
            " - layer5.weight grad norm: 0.012268693186342716\n",
            " - layer5.bias grad norm: 0.0056276340037584305\n",
            "Gradients at iteration 334:\n",
            " - layer1.weight grad norm: 0.5150247812271118\n",
            " - layer1.bias grad norm: 0.0005893700872547925\n",
            " - layer2.weight grad norm: 0.0018763385014608502\n",
            " - layer2.bias grad norm: 0.0005581028526648879\n",
            " - layer2_input.weight grad norm: 0.4883762300014496\n",
            " - layer2_input.bias grad norm: 0.0005581028526648879\n",
            " - layer3.weight grad norm: 0.0026039283256977797\n",
            " - layer3.bias grad norm: 0.0006072412361390889\n",
            " - layer3_input.weight grad norm: 0.522715151309967\n",
            " - layer3_input.bias grad norm: 0.0006072412361390889\n",
            " - layer4.weight grad norm: 0.0030532628297805786\n",
            " - layer4.bias grad norm: 0.0005445871502161026\n",
            " - layer4_input.weight grad norm: 0.472032755613327\n",
            " - layer4_input.bias grad norm: 0.0005445871502161026\n",
            " - layer5.weight grad norm: 0.011947516351938248\n",
            " - layer5.bias grad norm: 0.0052545201033353806\n",
            "Gradients at iteration 335:\n",
            " - layer1.weight grad norm: 0.5069113373756409\n",
            " - layer1.bias grad norm: 0.000575578713323921\n",
            " - layer2.weight grad norm: 0.0018922764575108886\n",
            " - layer2.bias grad norm: 0.0005959030240774155\n",
            " - layer2_input.weight grad norm: 0.5198497772216797\n",
            " - layer2_input.bias grad norm: 0.0005959030240774155\n",
            " - layer3.weight grad norm: 0.0025883023627102375\n",
            " - layer3.bias grad norm: 0.0005565991159528494\n",
            " - layer3_input.weight grad norm: 0.4860718548297882\n",
            " - layer3_input.bias grad norm: 0.0005565991159528494\n",
            " - layer4.weight grad norm: 0.003156225197017193\n",
            " - layer4.bias grad norm: 0.0005540500860661268\n",
            " - layer4_input.weight grad norm: 0.48616185784339905\n",
            " - layer4_input.bias grad norm: 0.0005540500860661268\n",
            " - layer5.weight grad norm: 0.011261912994086742\n",
            " - layer5.bias grad norm: 0.0053450074046850204\n",
            "Gradients at iteration 336:\n",
            " - layer1.weight grad norm: 0.5348257422447205\n",
            " - layer1.bias grad norm: 0.0006234515458345413\n",
            " - layer2.weight grad norm: 0.001924285665154457\n",
            " - layer2.bias grad norm: 0.000542050926014781\n",
            " - layer2_input.weight grad norm: 0.4687666594982147\n",
            " - layer2_input.bias grad norm: 0.000542050926014781\n",
            " - layer3.weight grad norm: 0.002630152041092515\n",
            " - layer3.bias grad norm: 0.0006253538304008543\n",
            " - layer3_input.weight grad norm: 0.5294636487960815\n",
            " - layer3_input.bias grad norm: 0.0006253538304008543\n",
            " - layer4.weight grad norm: 0.003129594260826707\n",
            " - layer4.bias grad norm: 0.0005362668307498097\n",
            " - layer4_input.weight grad norm: 0.46224871277809143\n",
            " - layer4_input.bias grad norm: 0.0005362668307498097\n",
            " - layer5.weight grad norm: 0.012737645767629147\n",
            " - layer5.bias grad norm: 0.005354616791009903\n",
            "Gradients at iteration 337:\n",
            " - layer1.weight grad norm: 0.5276710987091064\n",
            " - layer1.bias grad norm: 0.0006126558873802423\n",
            " - layer2.weight grad norm: 0.0018831243505701423\n",
            " - layer2.bias grad norm: 0.0005755355814471841\n",
            " - layer2_input.weight grad norm: 0.49941855669021606\n",
            " - layer2_input.bias grad norm: 0.0005755355814471841\n",
            " - layer3.weight grad norm: 0.0026136713568121195\n",
            " - layer3.bias grad norm: 0.0005856677889823914\n",
            " - layer3_input.weight grad norm: 0.506011426448822\n",
            " - layer3_input.bias grad norm: 0.0005856677889823914\n",
            " - layer4.weight grad norm: 0.0030712333973497152\n",
            " - layer4.bias grad norm: 0.0005333400913514197\n",
            " - layer4_input.weight grad norm: 0.46464502811431885\n",
            " - layer4_input.bias grad norm: 0.0005333400913514197\n",
            " - layer5.weight grad norm: 0.012312213890254498\n",
            " - layer5.bias grad norm: 0.005286083556711674\n",
            "Gradients at iteration 338:\n",
            " - layer1.weight grad norm: 0.5288345813751221\n",
            " - layer1.bias grad norm: 0.000612199364695698\n",
            " - layer2.weight grad norm: 0.0019057102035731077\n",
            " - layer2.bias grad norm: 0.0005654027918353677\n",
            " - layer2_input.weight grad norm: 0.49363380670547485\n",
            " - layer2_input.bias grad norm: 0.0005654027918353677\n",
            " - layer3.weight grad norm: 0.0026187982875853777\n",
            " - layer3.bias grad norm: 0.0006118830060586333\n",
            " - layer3_input.weight grad norm: 0.5259912610054016\n",
            " - layer3_input.bias grad norm: 0.0006118830060586333\n",
            " - layer4.weight grad norm: 0.003137037856504321\n",
            " - layer4.bias grad norm: 0.0005039095995016396\n",
            " - layer4_input.weight grad norm: 0.4469936192035675\n",
            " - layer4_input.bias grad norm: 0.0005039095995016396\n",
            " - layer5.weight grad norm: 0.011752861551940441\n",
            " - layer5.bias grad norm: 0.0053610047325491905\n",
            "Gradients at iteration 339:\n",
            " - layer1.weight grad norm: 0.5505915284156799\n",
            " - layer1.bias grad norm: 0.0006467746570706367\n",
            " - layer2.weight grad norm: 0.0018755107885226607\n",
            " - layer2.bias grad norm: 0.0005836280179210007\n",
            " - layer2_input.weight grad norm: 0.5011549592018127\n",
            " - layer2_input.bias grad norm: 0.0005836280179210007\n",
            " - layer3.weight grad norm: 0.0025541263166815042\n",
            " - layer3.bias grad norm: 0.0005704591167159379\n",
            " - layer3_input.weight grad norm: 0.4911390542984009\n",
            " - layer3_input.bias grad norm: 0.0005704591167159379\n",
            " - layer4.weight grad norm: 0.0031252545304596424\n",
            " - layer4.bias grad norm: 0.0005206127534620464\n",
            " - layer4_input.weight grad norm: 0.4519961178302765\n",
            " - layer4_input.bias grad norm: 0.0005206127534620464\n",
            " - layer5.weight grad norm: 0.011184941045939922\n",
            " - layer5.bias grad norm: 0.00524382758885622\n",
            "Gradients at iteration 340:\n",
            " - layer1.weight grad norm: 0.49231329560279846\n",
            " - layer1.bias grad norm: 0.0005708904936909676\n",
            " - layer2.weight grad norm: 0.0018474968383088708\n",
            " - layer2.bias grad norm: 0.0005974005907773972\n",
            " - layer2_input.weight grad norm: 0.5077226161956787\n",
            " - layer2_input.bias grad norm: 0.0005974005907773972\n",
            " - layer3.weight grad norm: 0.002530325436964631\n",
            " - layer3.bias grad norm: 0.0005788778071291745\n",
            " - layer3_input.weight grad norm: 0.49917736649513245\n",
            " - layer3_input.bias grad norm: 0.0005788778071291745\n",
            " - layer4.weight grad norm: 0.003017502138391137\n",
            " - layer4.bias grad norm: 0.000588102440815419\n",
            " - layer4_input.weight grad norm: 0.5004719495773315\n",
            " - layer4_input.bias grad norm: 0.000588102440815419\n",
            " - layer5.weight grad norm: 0.012129588983952999\n",
            " - layer5.bias grad norm: 0.005161188077181578\n",
            "Gradients at iteration 341:\n",
            " - layer1.weight grad norm: 0.5267279744148254\n",
            " - layer1.bias grad norm: 0.00061585265211761\n",
            " - layer2.weight grad norm: 0.0018324912525713444\n",
            " - layer2.bias grad norm: 0.0005794977187179029\n",
            " - layer2_input.weight grad norm: 0.4996284246444702\n",
            " - layer2_input.bias grad norm: 0.0005794977187179029\n",
            " - layer3.weight grad norm: 0.0025364055763930082\n",
            " - layer3.bias grad norm: 0.0006010195938870311\n",
            " - layer3_input.weight grad norm: 0.513290286064148\n",
            " - layer3_input.bias grad norm: 0.0006010195938870311\n",
            " - layer4.weight grad norm: 0.0030651765409857035\n",
            " - layer4.bias grad norm: 0.0005285611259751022\n",
            " - layer4_input.weight grad norm: 0.4574712812900543\n",
            " - layer4_input.bias grad norm: 0.0005285611259751022\n",
            " - layer5.weight grad norm: 0.011571265757083893\n",
            " - layer5.bias grad norm: 0.005181354936212301\n",
            "Gradients at iteration 342:\n",
            " - layer1.weight grad norm: 0.4846450686454773\n",
            " - layer1.bias grad norm: 0.0005544955492950976\n",
            " - layer2.weight grad norm: 0.001890762010589242\n",
            " - layer2.bias grad norm: 0.0006284244009293616\n",
            " - layer2_input.weight grad norm: 0.5394424796104431\n",
            " - layer2_input.bias grad norm: 0.0006284244009293616\n",
            " - layer3.weight grad norm: 0.002622428350150585\n",
            " - layer3.bias grad norm: 0.0005806174594908953\n",
            " - layer3_input.weight grad norm: 0.5051557421684265\n",
            " - layer3_input.bias grad norm: 0.0005806174594908953\n",
            " - layer4.weight grad norm: 0.0031263285782188177\n",
            " - layer4.bias grad norm: 0.0005379984504543245\n",
            " - layer4_input.weight grad norm: 0.46769511699676514\n",
            " - layer4_input.bias grad norm: 0.0005379984504543245\n",
            " - layer5.weight grad norm: 0.01219765655696392\n",
            " - layer5.bias grad norm: 0.005333650857210159\n",
            "Gradients at iteration 343:\n",
            " - layer1.weight grad norm: 0.530579149723053\n",
            " - layer1.bias grad norm: 0.000615343393292278\n",
            " - layer2.weight grad norm: 0.0018653188599273562\n",
            " - layer2.bias grad norm: 0.0005339788040146232\n",
            " - layer2_input.weight grad norm: 0.46780431270599365\n",
            " - layer2_input.bias grad norm: 0.0005339788040146232\n",
            " - layer3.weight grad norm: 0.002561445813626051\n",
            " - layer3.bias grad norm: 0.0005997330881655216\n",
            " - layer3_input.weight grad norm: 0.5151047110557556\n",
            " - layer3_input.bias grad norm: 0.0005997330881655216\n",
            " - layer4.weight grad norm: 0.0031285022851079702\n",
            " - layer4.bias grad norm: 0.0005610299413092434\n",
            " - layer4_input.weight grad norm: 0.4838402569293976\n",
            " - layer4_input.bias grad norm: 0.0005610299413092434\n",
            " - layer5.weight grad norm: 0.012670166790485382\n",
            " - layer5.bias grad norm: 0.005292273126542568\n",
            "Gradients at iteration 344:\n",
            " - layer1.weight grad norm: 0.5288005471229553\n",
            " - layer1.bias grad norm: 0.0006160440389066935\n",
            " - layer2.weight grad norm: 0.0018636212917044759\n",
            " - layer2.bias grad norm: 0.0006080548046156764\n",
            " - layer2_input.weight grad norm: 0.5232317447662354\n",
            " - layer2_input.bias grad norm: 0.0006080548046156764\n",
            " - layer3.weight grad norm: 0.0025775835383683443\n",
            " - layer3.bias grad norm: 0.0005961346323601902\n",
            " - layer3_input.weight grad norm: 0.5090767741203308\n",
            " - layer3_input.bias grad norm: 0.0005961346323601902\n",
            " - layer4.weight grad norm: 0.003063984215259552\n",
            " - layer4.bias grad norm: 0.0004934552707709372\n",
            " - layer4_input.weight grad norm: 0.4327433109283447\n",
            " - layer4_input.bias grad norm: 0.0004934552707709372\n",
            " - layer5.weight grad norm: 0.011114031076431274\n",
            " - layer5.bias grad norm: 0.005217458121478558\n",
            "Gradients at iteration 345:\n",
            " - layer1.weight grad norm: 0.5411774516105652\n",
            " - layer1.bias grad norm: 0.0006239775684662163\n",
            " - layer2.weight grad norm: 0.0019388752989470959\n",
            " - layer2.bias grad norm: 0.0006010883371345699\n",
            " - layer2_input.weight grad norm: 0.5143932104110718\n",
            " - layer2_input.bias grad norm: 0.0006010883371345699\n",
            " - layer3.weight grad norm: 0.0026441628579050303\n",
            " - layer3.bias grad norm: 0.0005562566802836955\n",
            " - layer3_input.weight grad norm: 0.48513656854629517\n",
            " - layer3_input.bias grad norm: 0.0005562566802836955\n",
            " - layer4.weight grad norm: 0.003145650727674365\n",
            " - layer4.bias grad norm: 0.000521208974532783\n",
            " - layer4_input.weight grad norm: 0.4549322724342346\n",
            " - layer4_input.bias grad norm: 0.000521208974532783\n",
            " - layer5.weight grad norm: 0.012370223179459572\n",
            " - layer5.bias grad norm: 0.005455006379634142\n",
            "Gradients at iteration 346:\n",
            " - layer1.weight grad norm: 0.5314212441444397\n",
            " - layer1.bias grad norm: 0.0006266544805839658\n",
            " - layer2.weight grad norm: 0.001885053119622171\n",
            " - layer2.bias grad norm: 0.0006033189129084349\n",
            " - layer2_input.weight grad norm: 0.5136224031448364\n",
            " - layer2_input.bias grad norm: 0.0006033189129084349\n",
            " - layer3.weight grad norm: 0.002599243773147464\n",
            " - layer3.bias grad norm: 0.0005770051502622664\n",
            " - layer3_input.weight grad norm: 0.4942949414253235\n",
            " - layer3_input.bias grad norm: 0.0005770051502622664\n",
            " - layer4.weight grad norm: 0.0031302773859351873\n",
            " - layer4.bias grad norm: 0.0005318570765666664\n",
            " - layer4_input.weight grad norm: 0.45744165778160095\n",
            " - layer4_input.bias grad norm: 0.0005318570765666664\n",
            " - layer5.weight grad norm: 0.012359533458948135\n",
            " - layer5.bias grad norm: 0.0052979690954089165\n",
            "Gradients at iteration 347:\n",
            " - layer1.weight grad norm: 0.5578516125679016\n",
            " - layer1.bias grad norm: 0.0006491783424280584\n",
            " - layer2.weight grad norm: 0.0018488677451387048\n",
            " - layer2.bias grad norm: 0.0005708510288968682\n",
            " - layer2_input.weight grad norm: 0.4953451454639435\n",
            " - layer2_input.bias grad norm: 0.0005708510288968682\n",
            " - layer3.weight grad norm: 0.002594973659142852\n",
            " - layer3.bias grad norm: 0.0005599015858024359\n",
            " - layer3_input.weight grad norm: 0.4882471561431885\n",
            " - layer3_input.bias grad norm: 0.0005599015858024359\n",
            " - layer4.weight grad norm: 0.003087275195866823\n",
            " - layer4.bias grad norm: 0.0005217087455093861\n",
            " - layer4_input.weight grad norm: 0.4526347517967224\n",
            " - layer4_input.bias grad norm: 0.0005217087455093861\n",
            " - layer5.weight grad norm: 0.011023438535630703\n",
            " - layer5.bias grad norm: 0.005270455498248339\n",
            "Gradients at iteration 348:\n",
            " - layer1.weight grad norm: 0.5277333855628967\n",
            " - layer1.bias grad norm: 0.00061563536291942\n",
            " - layer2.weight grad norm: 0.0018997093429788947\n",
            " - layer2.bias grad norm: 0.000615301076322794\n",
            " - layer2_input.weight grad norm: 0.5239354968070984\n",
            " - layer2_input.bias grad norm: 0.000615301076322794\n",
            " - layer3.weight grad norm: 0.0026503498665988445\n",
            " - layer3.bias grad norm: 0.0005211698007769883\n",
            " - layer3_input.weight grad norm: 0.4579169452190399\n",
            " - layer3_input.bias grad norm: 0.0005211698007769883\n",
            " - layer4.weight grad norm: 0.0031436304561793804\n",
            " - layer4.bias grad norm: 0.0005662879557348788\n",
            " - layer4_input.weight grad norm: 0.4869488775730133\n",
            " - layer4_input.bias grad norm: 0.0005662879557348788\n",
            " - layer5.weight grad norm: 0.011404366232454777\n",
            " - layer5.bias grad norm: 0.005389928352087736\n",
            "Gradients at iteration 349:\n",
            " - layer1.weight grad norm: 0.49674177169799805\n",
            " - layer1.bias grad norm: 0.0005752845318056643\n",
            " - layer2.weight grad norm: 0.001942967064678669\n",
            " - layer2.bias grad norm: 0.0005958068650215864\n",
            " - layer2_input.weight grad norm: 0.5062984824180603\n",
            " - layer2_input.bias grad norm: 0.0005958068650215864\n",
            " - layer3.weight grad norm: 0.0026370857376605272\n",
            " - layer3.bias grad norm: 0.0005218895967118442\n",
            " - layer3_input.weight grad norm: 0.45223909616470337\n",
            " - layer3_input.bias grad norm: 0.0005218895967118442\n",
            " - layer4.weight grad norm: 0.0031386923510581255\n",
            " - layer4.bias grad norm: 0.000642681960016489\n",
            " - layer4_input.weight grad norm: 0.5405403971672058\n",
            " - layer4_input.bias grad norm: 0.000642681960016489\n",
            " - layer5.weight grad norm: 0.012381999753415585\n",
            " - layer5.bias grad norm: 0.005399946589022875\n",
            "Gradients at iteration 350:\n",
            " - layer1.weight grad norm: 0.5261093974113464\n",
            " - layer1.bias grad norm: 0.0006083436892367899\n",
            " - layer2.weight grad norm: 0.0018880990101024508\n",
            " - layer2.bias grad norm: 0.0005784576642327011\n",
            " - layer2_input.weight grad norm: 0.49967142939567566\n",
            " - layer2_input.bias grad norm: 0.0005784576642327011\n",
            " - layer3.weight grad norm: 0.0026058785151690245\n",
            " - layer3.bias grad norm: 0.0006215653265826404\n",
            " - layer3_input.weight grad norm: 0.5265049338340759\n",
            " - layer3_input.bias grad norm: 0.0006215653265826404\n",
            " - layer4.weight grad norm: 0.003172673285007477\n",
            " - layer4.bias grad norm: 0.0005028094747103751\n",
            " - layer4_input.weight grad norm: 0.4428520202636719\n",
            " - layer4_input.bias grad norm: 0.0005028094747103751\n",
            " - layer5.weight grad norm: 0.01266134437173605\n",
            " - layer5.bias grad norm: 0.005388172343373299\n",
            "Gradients at iteration 351:\n",
            " - layer1.weight grad norm: 0.5319172143936157\n",
            " - layer1.bias grad norm: 0.0006281025125645101\n",
            " - layer2.weight grad norm: 0.001860479125753045\n",
            " - layer2.bias grad norm: 0.000582906708586961\n",
            " - layer2_input.weight grad norm: 0.5004761219024658\n",
            " - layer2_input.bias grad norm: 0.000582906708586961\n",
            " - layer3.weight grad norm: 0.0025646432768553495\n",
            " - layer3.bias grad norm: 0.0005533299408853054\n",
            " - layer3_input.weight grad norm: 0.4751421809196472\n",
            " - layer3_input.bias grad norm: 0.0005533299408853054\n",
            " - layer4.weight grad norm: 0.0031401992309838533\n",
            " - layer4.bias grad norm: 0.0005709982942789793\n",
            " - layer4_input.weight grad norm: 0.4905397891998291\n",
            " - layer4_input.bias grad norm: 0.0005709982942789793\n",
            " - layer5.weight grad norm: 0.01219144370406866\n",
            " - layer5.bias grad norm: 0.005253496114164591\n",
            "Gradients at iteration 352:\n",
            " - layer1.weight grad norm: 0.4975082278251648\n",
            " - layer1.bias grad norm: 0.0005796666955575347\n",
            " - layer2.weight grad norm: 0.0018609891412779689\n",
            " - layer2.bias grad norm: 0.0006044332403689623\n",
            " - layer2_input.weight grad norm: 0.5110597014427185\n",
            " - layer2_input.bias grad norm: 0.0006044332403689623\n",
            " - layer3.weight grad norm: 0.002548694610595703\n",
            " - layer3.bias grad norm: 0.0005946489400230348\n",
            " - layer3_input.weight grad norm: 0.5033460259437561\n",
            " - layer3_input.bias grad norm: 0.0005946489400230348\n",
            " - layer4.weight grad norm: 0.0030008796602487564\n",
            " - layer4.bias grad norm: 0.0005713858990930021\n",
            " - layer4_input.weight grad norm: 0.4875948429107666\n",
            " - layer4_input.bias grad norm: 0.0005713858990930021\n",
            " - layer5.weight grad norm: 0.012208321131765842\n",
            " - layer5.bias grad norm: 0.0052097262814641\n",
            "Gradients at iteration 353:\n",
            " - layer1.weight grad norm: 0.5240769386291504\n",
            " - layer1.bias grad norm: 0.0005999658023938537\n",
            " - layer2.weight grad norm: 0.0019411942921578884\n",
            " - layer2.bias grad norm: 0.0005869644810445607\n",
            " - layer2_input.weight grad norm: 0.5067780613899231\n",
            " - layer2_input.bias grad norm: 0.0005869644810445607\n",
            " - layer3.weight grad norm: 0.002659511985257268\n",
            " - layer3.bias grad norm: 0.0005427461583167315\n",
            " - layer3_input.weight grad norm: 0.47387969493865967\n",
            " - layer3_input.bias grad norm: 0.0005427461583167315\n",
            " - layer4.weight grad norm: 0.0031646350398659706\n",
            " - layer4.bias grad norm: 0.0005714926519431174\n",
            " - layer4_input.weight grad norm: 0.4937059283256531\n",
            " - layer4_input.bias grad norm: 0.0005714926519431174\n",
            " - layer5.weight grad norm: 0.012636585161089897\n",
            " - layer5.bias grad norm: 0.0053932019509375095\n",
            "Gradients at iteration 354:\n",
            " - layer1.weight grad norm: 0.568250298500061\n",
            " - layer1.bias grad norm: 0.0006752698100171983\n",
            " - layer2.weight grad norm: 0.0018732831813395023\n",
            " - layer2.bias grad norm: 0.0005614310503005981\n",
            " - layer2_input.weight grad norm: 0.48491281270980835\n",
            " - layer2_input.bias grad norm: 0.0005614310503005981\n",
            " - layer3.weight grad norm: 0.002579658292233944\n",
            " - layer3.bias grad norm: 0.0005568526103161275\n",
            " - layer3_input.weight grad norm: 0.478223592042923\n",
            " - layer3_input.bias grad norm: 0.0005568526103161275\n",
            " - layer4.weight grad norm: 0.0030410215258598328\n",
            " - layer4.bias grad norm: 0.0005361308576539159\n",
            " - layer4_input.weight grad norm: 0.46158385276794434\n",
            " - layer4_input.bias grad norm: 0.0005361308576539159\n",
            " - layer5.weight grad norm: 0.01200750283896923\n",
            " - layer5.bias grad norm: 0.00527827488258481\n",
            "Gradients at iteration 355:\n",
            " - layer1.weight grad norm: 0.513043999671936\n",
            " - layer1.bias grad norm: 0.0005946597666479647\n",
            " - layer2.weight grad norm: 0.0018772127805277705\n",
            " - layer2.bias grad norm: 0.0005841418169438839\n",
            " - layer2_input.weight grad norm: 0.5024186968803406\n",
            " - layer2_input.bias grad norm: 0.0005841418169438839\n",
            " - layer3.weight grad norm: 0.002620047889649868\n",
            " - layer3.bias grad norm: 0.0005999673740006983\n",
            " - layer3_input.weight grad norm: 0.5106139183044434\n",
            " - layer3_input.bias grad norm: 0.0005999673740006983\n",
            " - layer4.weight grad norm: 0.0031244601123034954\n",
            " - layer4.bias grad norm: 0.0005521668354049325\n",
            " - layer4_input.weight grad norm: 0.4726656675338745\n",
            " - layer4_input.bias grad norm: 0.0005521668354049325\n",
            " - layer5.weight grad norm: 0.013093276880681515\n",
            " - layer5.bias grad norm: 0.00529080955311656\n",
            "Gradients at iteration 356:\n",
            " - layer1.weight grad norm: 0.5193063616752625\n",
            " - layer1.bias grad norm: 0.0006047108909115195\n",
            " - layer2.weight grad norm: 0.0019161246018484235\n",
            " - layer2.bias grad norm: 0.000554214115254581\n",
            " - layer2_input.weight grad norm: 0.48324209451675415\n",
            " - layer2_input.bias grad norm: 0.000554214115254581\n",
            " - layer3.weight grad norm: 0.0025952509604394436\n",
            " - layer3.bias grad norm: 0.0005884651327505708\n",
            " - layer3_input.weight grad norm: 0.5044441819190979\n",
            " - layer3_input.bias grad norm: 0.0005884651327505708\n",
            " - layer4.weight grad norm: 0.003140462329611182\n",
            " - layer4.bias grad norm: 0.0005728168762288988\n",
            " - layer4_input.weight grad norm: 0.4920693635940552\n",
            " - layer4_input.bias grad norm: 0.0005728168762288988\n",
            " - layer5.weight grad norm: 0.012265654280781746\n",
            " - layer5.bias grad norm: 0.0053554498590528965\n",
            "Gradients at iteration 357:\n",
            " - layer1.weight grad norm: 0.5015949606895447\n",
            " - layer1.bias grad norm: 0.0005697028827853501\n",
            " - layer2.weight grad norm: 0.0019124188693240285\n",
            " - layer2.bias grad norm: 0.0005510652554221451\n",
            " - layer2_input.weight grad norm: 0.4807242453098297\n",
            " - layer2_input.bias grad norm: 0.0005510652554221451\n",
            " - layer3.weight grad norm: 0.0026429572608321905\n",
            " - layer3.bias grad norm: 0.000598644430283457\n",
            " - layer3_input.weight grad norm: 0.5135601162910461\n",
            " - layer3_input.bias grad norm: 0.000598644430283457\n",
            " - layer4.weight grad norm: 0.003154083387926221\n",
            " - layer4.bias grad norm: 0.0005835636984556913\n",
            " - layer4_input.weight grad norm: 0.5033653378486633\n",
            " - layer4_input.bias grad norm: 0.0005835636984556913\n",
            " - layer5.weight grad norm: 0.011569580994546413\n",
            " - layer5.bias grad norm: 0.005408931989222765\n",
            "Gradients at iteration 358:\n",
            " - layer1.weight grad norm: 0.49108952283859253\n",
            " - layer1.bias grad norm: 0.0005655664135701954\n",
            " - layer2.weight grad norm: 0.0018631109269335866\n",
            " - layer2.bias grad norm: 0.0006047997740097344\n",
            " - layer2_input.weight grad norm: 0.5204721093177795\n",
            " - layer2_input.bias grad norm: 0.0006047997740097344\n",
            " - layer3.weight grad norm: 0.002533825347200036\n",
            " - layer3.bias grad norm: 0.0005658328300341964\n",
            " - layer3_input.weight grad norm: 0.48749950528144836\n",
            " - layer3_input.bias grad norm: 0.0005658328300341964\n",
            " - layer4.weight grad norm: 0.003088788129389286\n",
            " - layer4.bias grad norm: 0.0005811358569189906\n",
            " - layer4_input.weight grad norm: 0.5000904202461243\n",
            " - layer4_input.bias grad norm: 0.0005811358569189906\n",
            " - layer5.weight grad norm: 0.011986798606812954\n",
            " - layer5.bias grad norm: 0.0052953739650547504\n",
            "Gradients at iteration 359:\n",
            " - layer1.weight grad norm: 0.5633853077888489\n",
            " - layer1.bias grad norm: 0.0006610608543269336\n",
            " - layer2.weight grad norm: 0.0018926335033029318\n",
            " - layer2.bias grad norm: 0.0005877080839127302\n",
            " - layer2_input.weight grad norm: 0.5059396028518677\n",
            " - layer2_input.bias grad norm: 0.0005877080839127302\n",
            " - layer3.weight grad norm: 0.0025915212463587523\n",
            " - layer3.bias grad norm: 0.0005570192588493228\n",
            " - layer3_input.weight grad norm: 0.4798396825790405\n",
            " - layer3_input.bias grad norm: 0.0005570192588493228\n",
            " - layer4.weight grad norm: 0.003105527488514781\n",
            " - layer4.bias grad norm: 0.0005055744550190866\n",
            " - layer4_input.weight grad norm: 0.44292154908180237\n",
            " - layer4_input.bias grad norm: 0.0005055744550190866\n",
            " - layer5.weight grad norm: 0.012094443663954735\n",
            " - layer5.bias grad norm: 0.005290701985359192\n",
            "Gradients at iteration 360:\n",
            " - layer1.weight grad norm: 0.5697251558303833\n",
            " - layer1.bias grad norm: 0.0006750599131919444\n",
            " - layer2.weight grad norm: 0.0017965864390134811\n",
            " - layer2.bias grad norm: 0.0005773809389211237\n",
            " - layer2_input.weight grad norm: 0.49799424409866333\n",
            " - layer2_input.bias grad norm: 0.0005773809389211237\n",
            " - layer3.weight grad norm: 0.002468843013048172\n",
            " - layer3.bias grad norm: 0.0005169751239009202\n",
            " - layer3_input.weight grad norm: 0.4489345848560333\n",
            " - layer3_input.bias grad norm: 0.0005169751239009202\n",
            " - layer4.weight grad norm: 0.00297875446267426\n",
            " - layer4.bias grad norm: 0.000556983461137861\n",
            " - layer4_input.weight grad norm: 0.47504720091819763\n",
            " - layer4_input.bias grad norm: 0.000556983461137861\n",
            " - layer5.weight grad norm: 0.012496626004576683\n",
            " - layer5.bias grad norm: 0.005115231033414602\n",
            "Gradients at iteration 361:\n",
            " - layer1.weight grad norm: 0.49764484167099\n",
            " - layer1.bias grad norm: 0.000574639649130404\n",
            " - layer2.weight grad norm: 0.0018853713991120458\n",
            " - layer2.bias grad norm: 0.000657807569950819\n",
            " - layer2_input.weight grad norm: 0.5566787719726562\n",
            " - layer2_input.bias grad norm: 0.000657807569950819\n",
            " - layer3.weight grad norm: 0.002612320240586996\n",
            " - layer3.bias grad norm: 0.0005704232025891542\n",
            " - layer3_input.weight grad norm: 0.4915814697742462\n",
            " - layer3_input.bias grad norm: 0.0005704232025891542\n",
            " - layer4.weight grad norm: 0.0031320243142545223\n",
            " - layer4.bias grad norm: 0.0005144011811353266\n",
            " - layer4_input.weight grad norm: 0.4478960335254669\n",
            " - layer4_input.bias grad norm: 0.0005144011811353266\n",
            " - layer5.weight grad norm: 0.012021462433040142\n",
            " - layer5.bias grad norm: 0.005302948411554098\n",
            "Gradients at iteration 362:\n",
            " - layer1.weight grad norm: 0.5528680086135864\n",
            " - layer1.bias grad norm: 0.0006470826338045299\n",
            " - layer2.weight grad norm: 0.001835531322285533\n",
            " - layer2.bias grad norm: 0.0005557682015933096\n",
            " - layer2_input.weight grad norm: 0.48219406604766846\n",
            " - layer2_input.bias grad norm: 0.0005557682015933096\n",
            " - layer3.weight grad norm: 0.002487360266968608\n",
            " - layer3.bias grad norm: 0.0005851387977600098\n",
            " - layer3_input.weight grad norm: 0.5003301501274109\n",
            " - layer3_input.bias grad norm: 0.0005851387977600098\n",
            " - layer4.weight grad norm: 0.002998140873387456\n",
            " - layer4.bias grad norm: 0.0005268112872727215\n",
            " - layer4_input.weight grad norm: 0.459684282541275\n",
            " - layer4_input.bias grad norm: 0.0005268112872727215\n",
            " - layer5.weight grad norm: 0.011786239221692085\n",
            " - layer5.bias grad norm: 0.00513198459520936\n",
            "Gradients at iteration 363:\n",
            " - layer1.weight grad norm: 0.5563978552818298\n",
            " - layer1.bias grad norm: 0.0006532488041557372\n",
            " - layer2.weight grad norm: 0.0018516220152378082\n",
            " - layer2.bias grad norm: 0.0005557943950407207\n",
            " - layer2_input.weight grad norm: 0.4807125926017761\n",
            " - layer2_input.bias grad norm: 0.0005557943950407207\n",
            " - layer3.weight grad norm: 0.002551057143136859\n",
            " - layer3.bias grad norm: 0.0005876905051991343\n",
            " - layer3_input.weight grad norm: 0.5055341720581055\n",
            " - layer3_input.bias grad norm: 0.0005876905051991343\n",
            " - layer4.weight grad norm: 0.003046983852982521\n",
            " - layer4.bias grad norm: 0.0005222080508247018\n",
            " - layer4_input.weight grad norm: 0.45121729373931885\n",
            " - layer4_input.bias grad norm: 0.0005222080508247018\n",
            " - layer5.weight grad norm: 0.011223927140235901\n",
            " - layer5.bias grad norm: 0.00525336479768157\n",
            "Gradients at iteration 364:\n",
            " - layer1.weight grad norm: 0.5739148259162903\n",
            " - layer1.bias grad norm: 0.0006818927358835936\n",
            " - layer2.weight grad norm: 0.00180890504270792\n",
            " - layer2.bias grad norm: 0.0005777844926342368\n",
            " - layer2_input.weight grad norm: 0.49729856848716736\n",
            " - layer2_input.bias grad norm: 0.0005777844926342368\n",
            " - layer3.weight grad norm: 0.0024878554977476597\n",
            " - layer3.bias grad norm: 0.0005296471645124257\n",
            " - layer3_input.weight grad norm: 0.46228843927383423\n",
            " - layer3_input.bias grad norm: 0.0005296471645124257\n",
            " - layer4.weight grad norm: 0.002941377693787217\n",
            " - layer4.bias grad norm: 0.0005305098020471632\n",
            " - layer4_input.weight grad norm: 0.45763614773750305\n",
            " - layer4_input.bias grad norm: 0.0005305098020471632\n",
            " - layer5.weight grad norm: 0.011338740587234497\n",
            " - layer5.bias grad norm: 0.005060989409685135\n",
            "Gradients at iteration 365:\n",
            " - layer1.weight grad norm: 0.5369826555252075\n",
            " - layer1.bias grad norm: 0.0006363799911923707\n",
            " - layer2.weight grad norm: 0.001910544466227293\n",
            " - layer2.bias grad norm: 0.0006247685523703694\n",
            " - layer2_input.weight grad norm: 0.5343970060348511\n",
            " - layer2_input.bias grad norm: 0.0006247685523703694\n",
            " - layer3.weight grad norm: 0.0026329942047595978\n",
            " - layer3.bias grad norm: 0.0005634031840600073\n",
            " - layer3_input.weight grad norm: 0.48554137349128723\n",
            " - layer3_input.bias grad norm: 0.0005634031840600073\n",
            " - layer4.weight grad norm: 0.003152097575366497\n",
            " - layer4.bias grad norm: 0.0004955456824973226\n",
            " - layer4_input.weight grad norm: 0.4360181391239166\n",
            " - layer4_input.bias grad norm: 0.0004955456824973226\n",
            " - layer5.weight grad norm: 0.01245331671088934\n",
            " - layer5.bias grad norm: 0.005409163888543844\n",
            "Gradients at iteration 366:\n",
            " - layer1.weight grad norm: 0.4916198253631592\n",
            " - layer1.bias grad norm: 0.000565460417419672\n",
            " - layer2.weight grad norm: 0.0018430091440677643\n",
            " - layer2.bias grad norm: 0.0005774622550234199\n",
            " - layer2_input.weight grad norm: 0.5008096694946289\n",
            " - layer2_input.bias grad norm: 0.0005774622550234199\n",
            " - layer3.weight grad norm: 0.002528504701331258\n",
            " - layer3.bias grad norm: 0.0005778807681053877\n",
            " - layer3_input.weight grad norm: 0.5005075931549072\n",
            " - layer3_input.bias grad norm: 0.0005778807681053877\n",
            " - layer4.weight grad norm: 0.0030412948690354824\n",
            " - layer4.bias grad norm: 0.0005927880411036313\n",
            " - layer4_input.weight grad norm: 0.5067446827888489\n",
            " - layer4_input.bias grad norm: 0.0005927880411036313\n",
            " - layer5.weight grad norm: 0.012373859994113445\n",
            " - layer5.bias grad norm: 0.005206665955483913\n",
            "Gradients at iteration 367:\n",
            " - layer1.weight grad norm: 0.5282959342002869\n",
            " - layer1.bias grad norm: 0.000605266191996634\n",
            " - layer2.weight grad norm: 0.0019241744885221124\n",
            " - layer2.bias grad norm: 0.0005928473547101021\n",
            " - layer2_input.weight grad norm: 0.5126487016677856\n",
            " - layer2_input.bias grad norm: 0.0005928473547101021\n",
            " - layer3.weight grad norm: 0.00258760922588408\n",
            " - layer3.bias grad norm: 0.0005386157426983118\n",
            " - layer3_input.weight grad norm: 0.46652060747146606\n",
            " - layer3_input.bias grad norm: 0.0005386157426983118\n",
            " - layer4.weight grad norm: 0.0031074548605829477\n",
            " - layer4.bias grad norm: 0.0005647466168738902\n",
            " - layer4_input.weight grad norm: 0.49015435576438904\n",
            " - layer4_input.bias grad norm: 0.0005647466168738902\n",
            " - layer5.weight grad norm: 0.012265539728105068\n",
            " - layer5.bias grad norm: 0.005407858639955521\n",
            "Gradients at iteration 368:\n",
            " - layer1.weight grad norm: 0.4779098629951477\n",
            " - layer1.bias grad norm: 0.0005424149567261338\n",
            " - layer2.weight grad norm: 0.0018601054325699806\n",
            " - layer2.bias grad norm: 0.0006100386381149292\n",
            " - layer2_input.weight grad norm: 0.5236908197402954\n",
            " - layer2_input.bias grad norm: 0.0006100386381149292\n",
            " - layer3.weight grad norm: 0.0025723171420395374\n",
            " - layer3.bias grad norm: 0.000621986051555723\n",
            " - layer3_input.weight grad norm: 0.5312745571136475\n",
            " - layer3_input.bias grad norm: 0.000621986051555723\n",
            " - layer4.weight grad norm: 0.0030509824864566326\n",
            " - layer4.bias grad norm: 0.0005340971983969212\n",
            " - layer4_input.weight grad norm: 0.46359366178512573\n",
            " - layer4_input.bias grad norm: 0.0005340971983969212\n",
            " - layer5.weight grad norm: 0.011344772763550282\n",
            " - layer5.bias grad norm: 0.0052704582922160625\n",
            "Gradients at iteration 369:\n",
            " - layer1.weight grad norm: 0.47889840602874756\n",
            " - layer1.bias grad norm: 0.0005421913228929043\n",
            " - layer2.weight grad norm: 0.0019202419789507985\n",
            " - layer2.bias grad norm: 0.0006356255034916103\n",
            " - layer2_input.weight grad norm: 0.5453230738639832\n",
            " - layer2_input.bias grad norm: 0.0006356255034916103\n",
            " - layer3.weight grad norm: 0.002636759774759412\n",
            " - layer3.bias grad norm: 0.0005612620152533054\n",
            " - layer3_input.weight grad norm: 0.483664870262146\n",
            " - layer3_input.bias grad norm: 0.0005612620152533054\n",
            " - layer4.weight grad norm: 0.0031784726306796074\n",
            " - layer4.bias grad norm: 0.0005678862216882408\n",
            " - layer4_input.weight grad norm: 0.48904335498809814\n",
            " - layer4_input.bias grad norm: 0.0005678862216882408\n",
            " - layer5.weight grad norm: 0.011465602554380894\n",
            " - layer5.bias grad norm: 0.005414674989879131\n",
            "Gradients at iteration 370:\n",
            " - layer1.weight grad norm: 0.5090708136558533\n",
            " - layer1.bias grad norm: 0.0005901413969695568\n",
            " - layer2.weight grad norm: 0.0018460601568222046\n",
            " - layer2.bias grad norm: 0.0005719249020330608\n",
            " - layer2_input.weight grad norm: 0.49626168608665466\n",
            " - layer2_input.bias grad norm: 0.0005719249020330608\n",
            " - layer3.weight grad norm: 0.0025194890331476927\n",
            " - layer3.bias grad norm: 0.0005999153363518417\n",
            " - layer3_input.weight grad norm: 0.5174514651298523\n",
            " - layer3_input.bias grad norm: 0.0005999153363518417\n",
            " - layer4.weight grad norm: 0.0030938908457756042\n",
            " - layer4.bias grad norm: 0.0005507977912202477\n",
            " - layer4_input.weight grad norm: 0.4760792851448059\n",
            " - layer4_input.bias grad norm: 0.0005507977912202477\n",
            " - layer5.weight grad norm: 0.010707945562899113\n",
            " - layer5.bias grad norm: 0.005240027327090502\n",
            "Gradients at iteration 371:\n",
            " - layer1.weight grad norm: 0.5054863691329956\n",
            " - layer1.bias grad norm: 0.0005794705357402563\n",
            " - layer2.weight grad norm: 0.0018872483633458614\n",
            " - layer2.bias grad norm: 0.0005723316571675241\n",
            " - layer2_input.weight grad norm: 0.49784156680107117\n",
            " - layer2_input.bias grad norm: 0.0005723316571675241\n",
            " - layer3.weight grad norm: 0.0026146708987653255\n",
            " - layer3.bias grad norm: 0.0005715860170312226\n",
            " - layer3_input.weight grad norm: 0.49571678042411804\n",
            " - layer3_input.bias grad norm: 0.0005715860170312226\n",
            " - layer4.weight grad norm: 0.00311639835126698\n",
            " - layer4.bias grad norm: 0.0005819201469421387\n",
            " - layer4_input.weight grad norm: 0.5006912350654602\n",
            " - layer4_input.bias grad norm: 0.0005819201469421387\n",
            " - layer5.weight grad norm: 0.01263332087546587\n",
            " - layer5.bias grad norm: 0.005322560202330351\n",
            "Gradients at iteration 372:\n",
            " - layer1.weight grad norm: 0.5277602076530457\n",
            " - layer1.bias grad norm: 0.0006129289395175874\n",
            " - layer2.weight grad norm: 0.001976399216800928\n",
            " - layer2.bias grad norm: 0.0005937934038229287\n",
            " - layer2_input.weight grad norm: 0.5083169937133789\n",
            " - layer2_input.bias grad norm: 0.0005937934038229287\n",
            " - layer3.weight grad norm: 0.0026762934867292643\n",
            " - layer3.bias grad norm: 0.0005914421053603292\n",
            " - layer3_input.weight grad norm: 0.5062702298164368\n",
            " - layer3_input.bias grad norm: 0.0005914421053603292\n",
            " - layer4.weight grad norm: 0.0032011184375733137\n",
            " - layer4.bias grad norm: 0.0005235651624388993\n",
            " - layer4_input.weight grad norm: 0.4544719159603119\n",
            " - layer4_input.bias grad norm: 0.0005235651624388993\n",
            " - layer5.weight grad norm: 0.013207664713263512\n",
            " - layer5.bias grad norm: 0.005541947670280933\n",
            "Gradients at iteration 373:\n",
            " - layer1.weight grad norm: 0.5169671773910522\n",
            " - layer1.bias grad norm: 0.0006027704803273082\n",
            " - layer2.weight grad norm: 0.001724344096146524\n",
            " - layer2.bias grad norm: 0.0006185509846545756\n",
            " - layer2_input.weight grad norm: 0.5225276350975037\n",
            " - layer2_input.bias grad norm: 0.0006185509846545756\n",
            " - layer3.weight grad norm: 0.0024122465401887894\n",
            " - layer3.bias grad norm: 0.0006010336801409721\n",
            " - layer3_input.weight grad norm: 0.5078865885734558\n",
            " - layer3_input.bias grad norm: 0.0006010336801409721\n",
            " - layer4.weight grad norm: 0.002886653644964099\n",
            " - layer4.bias grad norm: 0.0005234708660282195\n",
            " - layer4_input.weight grad norm: 0.44898873567581177\n",
            " - layer4_input.bias grad norm: 0.0005234708660282195\n",
            " - layer5.weight grad norm: 0.01123704295605421\n",
            " - layer5.bias grad norm: 0.004937186371535063\n",
            "Gradients at iteration 374:\n",
            " - layer1.weight grad norm: 0.5480255484580994\n",
            " - layer1.bias grad norm: 0.0006406888132914901\n",
            " - layer2.weight grad norm: 0.0019172652391716838\n",
            " - layer2.bias grad norm: 0.0005735179875046015\n",
            " - layer2_input.weight grad norm: 0.4938676655292511\n",
            " - layer2_input.bias grad norm: 0.0005735179875046015\n",
            " - layer3.weight grad norm: 0.002645801519975066\n",
            " - layer3.bias grad norm: 0.0005737445899285376\n",
            " - layer3_input.weight grad norm: 0.49513307213783264\n",
            " - layer3_input.bias grad norm: 0.0005737445899285376\n",
            " - layer4.weight grad norm: 0.0031226968858391047\n",
            " - layer4.bias grad norm: 0.0005261533660814166\n",
            " - layer4_input.weight grad norm: 0.45869019627571106\n",
            " - layer4_input.bias grad norm: 0.0005261533660814166\n",
            " - layer5.weight grad norm: 0.012549284845590591\n",
            " - layer5.bias grad norm: 0.005401411093771458\n",
            "Gradients at iteration 375:\n",
            " - layer1.weight grad norm: 0.5533854365348816\n",
            " - layer1.bias grad norm: 0.0006472432287409902\n",
            " - layer2.weight grad norm: 0.0018330501625314355\n",
            " - layer2.bias grad norm: 0.0005441540270112455\n",
            " - layer2_input.weight grad norm: 0.4750736355781555\n",
            " - layer2_input.bias grad norm: 0.0005441540270112455\n",
            " - layer3.weight grad norm: 0.002499781083315611\n",
            " - layer3.bias grad norm: 0.0005595807451754808\n",
            " - layer3_input.weight grad norm: 0.4850441515445709\n",
            " - layer3_input.bias grad norm: 0.0005595807451754808\n",
            " - layer4.weight grad norm: 0.002998276613652706\n",
            " - layer4.bias grad norm: 0.0005606454215012491\n",
            " - layer4_input.weight grad norm: 0.482295960187912\n",
            " - layer4_input.bias grad norm: 0.0005606454215012491\n",
            " - layer5.weight grad norm: 0.012030570767819881\n",
            " - layer5.bias grad norm: 0.005163572728633881\n",
            "Gradients at iteration 376:\n",
            " - layer1.weight grad norm: 0.5460944175720215\n",
            " - layer1.bias grad norm: 0.0006427133339457214\n",
            " - layer2.weight grad norm: 0.0018764017149806023\n",
            " - layer2.bias grad norm: 0.0005842839018441737\n",
            " - layer2_input.weight grad norm: 0.5004656314849854\n",
            " - layer2_input.bias grad norm: 0.0005842839018441737\n",
            " - layer3.weight grad norm: 0.002571562770754099\n",
            " - layer3.bias grad norm: 0.0005807040724903345\n",
            " - layer3_input.weight grad norm: 0.49634790420532227\n",
            " - layer3_input.bias grad norm: 0.0005807040724903345\n",
            " - layer4.weight grad norm: 0.003073351923376322\n",
            " - layer4.bias grad norm: 0.0005200873711146414\n",
            " - layer4_input.weight grad norm: 0.4525041878223419\n",
            " - layer4_input.bias grad norm: 0.0005200873711146414\n",
            " - layer5.weight grad norm: 0.011991412378847599\n",
            " - layer5.bias grad norm: 0.005276733543723822\n",
            "Gradients at iteration 377:\n",
            " - layer1.weight grad norm: 0.4681297242641449\n",
            " - layer1.bias grad norm: 0.0005255290307104588\n",
            " - layer2.weight grad norm: 0.0018829365726560354\n",
            " - layer2.bias grad norm: 0.0006113792769610882\n",
            " - layer2_input.weight grad norm: 0.5273637175559998\n",
            " - layer2_input.bias grad norm: 0.0006113792769610882\n",
            " - layer3.weight grad norm: 0.0026268502697348595\n",
            " - layer3.bias grad norm: 0.0006024972535669804\n",
            " - layer3_input.weight grad norm: 0.5189999938011169\n",
            " - layer3_input.bias grad norm: 0.0006024972535669804\n",
            " - layer4.weight grad norm: 0.0030715966131538153\n",
            " - layer4.bias grad norm: 0.0005577564588747919\n",
            " - layer4_input.weight grad norm: 0.4829009771347046\n",
            " - layer4_input.bias grad norm: 0.0005577564588747919\n",
            " - layer5.weight grad norm: 0.011699693277478218\n",
            " - layer5.bias grad norm: 0.005339924246072769\n",
            "Gradients at iteration 378:\n",
            " - layer1.weight grad norm: 0.5029424428939819\n",
            " - layer1.bias grad norm: 0.0005839294171892107\n",
            " - layer2.weight grad norm: 0.0018745982088148594\n",
            " - layer2.bias grad norm: 0.0006089743110351264\n",
            " - layer2_input.weight grad norm: 0.5183094143867493\n",
            " - layer2_input.bias grad norm: 0.0006089743110351264\n",
            " - layer3.weight grad norm: 0.002566329902037978\n",
            " - layer3.bias grad norm: 0.0005881922552362084\n",
            " - layer3_input.weight grad norm: 0.5031489133834839\n",
            " - layer3_input.bias grad norm: 0.0005881922552362084\n",
            " - layer4.weight grad norm: 0.003083755960687995\n",
            " - layer4.bias grad norm: 0.0005556198884733021\n",
            " - layer4_input.weight grad norm: 0.47439083456993103\n",
            " - layer4_input.bias grad norm: 0.0005556198884733021\n",
            " - layer5.weight grad norm: 0.01220344752073288\n",
            " - layer5.bias grad norm: 0.0052774581126868725\n",
            "Gradients at iteration 379:\n",
            " - layer1.weight grad norm: 0.5424453020095825\n",
            " - layer1.bias grad norm: 0.0006388755864463747\n",
            " - layer2.weight grad norm: 0.001762861735187471\n",
            " - layer2.bias grad norm: 0.0005786812398582697\n",
            " - layer2_input.weight grad norm: 0.49404361844062805\n",
            " - layer2_input.bias grad norm: 0.0005786812398582697\n",
            " - layer3.weight grad norm: 0.002462724456563592\n",
            " - layer3.bias grad norm: 0.0005869397427886724\n",
            " - layer3_input.weight grad norm: 0.501772403717041\n",
            " - layer3_input.bias grad norm: 0.0005869397427886724\n",
            " - layer4.weight grad norm: 0.0029702179599553347\n",
            " - layer4.bias grad norm: 0.0005289462860673666\n",
            " - layer4_input.weight grad norm: 0.45793935656547546\n",
            " - layer4_input.bias grad norm: 0.0005289462860673666\n",
            " - layer5.weight grad norm: 0.011988360434770584\n",
            " - layer5.bias grad norm: 0.005093835294246674\n",
            "Gradients at iteration 380:\n",
            " - layer1.weight grad norm: 0.5403738021850586\n",
            " - layer1.bias grad norm: 0.0006346038426272571\n",
            " - layer2.weight grad norm: 0.0018410973716527224\n",
            " - layer2.bias grad norm: 0.0005472109187394381\n",
            " - layer2_input.weight grad norm: 0.47584962844848633\n",
            " - layer2_input.bias grad norm: 0.0005472109187394381\n",
            " - layer3.weight grad norm: 0.0025840112939476967\n",
            " - layer3.bias grad norm: 0.0005946924793533981\n",
            " - layer3_input.weight grad norm: 0.5057163834571838\n",
            " - layer3_input.bias grad norm: 0.0005946924793533981\n",
            " - layer4.weight grad norm: 0.003066191915422678\n",
            " - layer4.bias grad norm: 0.0005525420419871807\n",
            " - layer4_input.weight grad norm: 0.4749920666217804\n",
            " - layer4_input.bias grad norm: 0.0005525420419871807\n",
            " - layer5.weight grad norm: 0.012093400582671165\n",
            " - layer5.bias grad norm: 0.005347044672816992\n",
            "Gradients at iteration 381:\n",
            " - layer1.weight grad norm: 0.5380585789680481\n",
            " - layer1.bias grad norm: 0.0006260161171667278\n",
            " - layer2.weight grad norm: 0.0018844269216060638\n",
            " - layer2.bias grad norm: 0.0006218184716999531\n",
            " - layer2_input.weight grad norm: 0.5329370498657227\n",
            " - layer2_input.bias grad norm: 0.0006218184716999531\n",
            " - layer3.weight grad norm: 0.0025813686661422253\n",
            " - layer3.bias grad norm: 0.0005390056176111102\n",
            " - layer3_input.weight grad norm: 0.4703523516654968\n",
            " - layer3_input.bias grad norm: 0.0005390056176111102\n",
            " - layer4.weight grad norm: 0.0031440581660717726\n",
            " - layer4.bias grad norm: 0.0005161198787391186\n",
            " - layer4_input.weight grad norm: 0.4528144896030426\n",
            " - layer4_input.bias grad norm: 0.0005161198787391186\n",
            " - layer5.weight grad norm: 0.01215526182204485\n",
            " - layer5.bias grad norm: 0.005346672143787146\n",
            "Gradients at iteration 382:\n",
            " - layer1.weight grad norm: 0.5317548513412476\n",
            " - layer1.bias grad norm: 0.0006207280675880611\n",
            " - layer2.weight grad norm: 0.0018445022869855165\n",
            " - layer2.bias grad norm: 0.0006006342591717839\n",
            " - layer2_input.weight grad norm: 0.512906014919281\n",
            " - layer2_input.bias grad norm: 0.0006006342591717839\n",
            " - layer3.weight grad norm: 0.002560554537922144\n",
            " - layer3.bias grad norm: 0.0005194320692680776\n",
            " - layer3_input.weight grad norm: 0.4509263038635254\n",
            " - layer3_input.bias grad norm: 0.0005194320692680776\n",
            " - layer4.weight grad norm: 0.003043511649593711\n",
            " - layer4.bias grad norm: 0.0005850898451171815\n",
            " - layer4_input.weight grad norm: 0.500612199306488\n",
            " - layer4_input.bias grad norm: 0.0005850898451171815\n",
            " - layer5.weight grad norm: 0.012978640384972095\n",
            " - layer5.bias grad norm: 0.005214950535446405\n",
            "Gradients at iteration 383:\n",
            " - layer1.weight grad norm: 0.5493367910385132\n",
            " - layer1.bias grad norm: 0.0006442142766900361\n",
            " - layer2.weight grad norm: 0.001983467023819685\n",
            " - layer2.bias grad norm: 0.0006005066679790616\n",
            " - layer2_input.weight grad norm: 0.515877366065979\n",
            " - layer2_input.bias grad norm: 0.0006005066679790616\n",
            " - layer3.weight grad norm: 0.002730059437453747\n",
            " - layer3.bias grad norm: 0.0005112075596116483\n",
            " - layer3_input.weight grad norm: 0.44989681243896484\n",
            " - layer3_input.bias grad norm: 0.0005112075596116483\n",
            " - layer4.weight grad norm: 0.003200748935341835\n",
            " - layer4.bias grad norm: 0.0005555429961532354\n",
            " - layer4_input.weight grad norm: 0.47905147075653076\n",
            " - layer4_input.bias grad norm: 0.0005555429961532354\n",
            " - layer5.weight grad norm: 0.012126834131777287\n",
            " - layer5.bias grad norm: 0.005575147457420826\n",
            "Gradients at iteration 384:\n",
            " - layer1.weight grad norm: 0.4985666573047638\n",
            " - layer1.bias grad norm: 0.000569680763874203\n",
            " - layer2.weight grad norm: 0.001955244690179825\n",
            " - layer2.bias grad norm: 0.0005770972347818315\n",
            " - layer2_input.weight grad norm: 0.4991053640842438\n",
            " - layer2_input.bias grad norm: 0.0005770972347818315\n",
            " - layer3.weight grad norm: 0.0027342934627085924\n",
            " - layer3.bias grad norm: 0.0006320851971395314\n",
            " - layer3_input.weight grad norm: 0.5379157066345215\n",
            " - layer3_input.bias grad norm: 0.0006320851971395314\n",
            " - layer4.weight grad norm: 0.003183775581419468\n",
            " - layer4.bias grad norm: 0.0005327780381776392\n",
            " - layer4_input.weight grad norm: 0.46128636598587036\n",
            " - layer4_input.bias grad norm: 0.0005327780381776392\n",
            " - layer5.weight grad norm: 0.011495151557028294\n",
            " - layer5.bias grad norm: 0.0055410247296094894\n",
            "Gradients at iteration 385:\n",
            " - layer1.weight grad norm: 0.5208999514579773\n",
            " - layer1.bias grad norm: 0.0006095348508097231\n",
            " - layer2.weight grad norm: 0.0018227545078843832\n",
            " - layer2.bias grad norm: 0.0005800787475891411\n",
            " - layer2_input.weight grad norm: 0.49867767095565796\n",
            " - layer2_input.bias grad norm: 0.0005800787475891411\n",
            " - layer3.weight grad norm: 0.0025783837772905827\n",
            " - layer3.bias grad norm: 0.0005401875241659582\n",
            " - layer3_input.weight grad norm: 0.4694339334964752\n",
            " - layer3_input.bias grad norm: 0.0005401875241659582\n",
            " - layer4.weight grad norm: 0.003062662435695529\n",
            " - layer4.bias grad norm: 0.0006018408457748592\n",
            " - layer4_input.weight grad norm: 0.509337306022644\n",
            " - layer4_input.bias grad norm: 0.0006018408457748592\n",
            " - layer5.weight grad norm: 0.011904548853635788\n",
            " - layer5.bias grad norm: 0.005263080354779959\n",
            "Gradients at iteration 386:\n",
            " - layer1.weight grad norm: 0.4881318509578705\n",
            " - layer1.bias grad norm: 0.000563992012757808\n",
            " - layer2.weight grad norm: 0.0018803292186930776\n",
            " - layer2.bias grad norm: 0.0006184824742376804\n",
            " - layer2_input.weight grad norm: 0.5283450484275818\n",
            " - layer2_input.bias grad norm: 0.0006184824742376804\n",
            " - layer3.weight grad norm: 0.0026194127276539803\n",
            " - layer3.bias grad norm: 0.0005822019884362817\n",
            " - layer3_input.weight grad norm: 0.4962138235569\n",
            " - layer3_input.bias grad norm: 0.0005822019884362817\n",
            " - layer4.weight grad norm: 0.00308305979706347\n",
            " - layer4.bias grad norm: 0.0005690362304449081\n",
            " - layer4_input.weight grad norm: 0.48595941066741943\n",
            " - layer4_input.bias grad norm: 0.0005690362304449081\n",
            " - layer5.weight grad norm: 0.011964539997279644\n",
            " - layer5.bias grad norm: 0.005362521857023239\n",
            "Gradients at iteration 387:\n",
            " - layer1.weight grad norm: 0.5651863813400269\n",
            " - layer1.bias grad norm: 0.0006597093306481838\n",
            " - layer2.weight grad norm: 0.0018442092696204782\n",
            " - layer2.bias grad norm: 0.0005398637731559575\n",
            " - layer2_input.weight grad norm: 0.47336140275001526\n",
            " - layer2_input.bias grad norm: 0.0005398637731559575\n",
            " - layer3.weight grad norm: 0.0025255405344069004\n",
            " - layer3.bias grad norm: 0.0005502780550159514\n",
            " - layer3_input.weight grad norm: 0.4816124439239502\n",
            " - layer3_input.bias grad norm: 0.0005502780550159514\n",
            " - layer4.weight grad norm: 0.0030553273390978575\n",
            " - layer4.bias grad norm: 0.000544239766895771\n",
            " - layer4_input.weight grad norm: 0.47366052865982056\n",
            " - layer4_input.bias grad norm: 0.000544239766895771\n",
            " - layer5.weight grad norm: 0.011841648258268833\n",
            " - layer5.bias grad norm: 0.005192437209188938\n",
            "Gradients at iteration 388:\n",
            " - layer1.weight grad norm: 0.49618467688560486\n",
            " - layer1.bias grad norm: 0.0005690626567229629\n",
            " - layer2.weight grad norm: 0.001832940150052309\n",
            " - layer2.bias grad norm: 0.0005763821536675096\n",
            " - layer2_input.weight grad norm: 0.4999209940433502\n",
            " - layer2_input.bias grad norm: 0.0005763821536675096\n",
            " - layer3.weight grad norm: 0.0025673259515315294\n",
            " - layer3.bias grad norm: 0.0005995302926748991\n",
            " - layer3_input.weight grad norm: 0.5139530897140503\n",
            " - layer3_input.bias grad norm: 0.0005995302926748991\n",
            " - layer4.weight grad norm: 0.003017751732841134\n",
            " - layer4.bias grad norm: 0.0005641576135531068\n",
            " - layer4_input.weight grad norm: 0.4894484579563141\n",
            " - layer4_input.bias grad norm: 0.0005641576135531068\n",
            " - layer5.weight grad norm: 0.01112170796841383\n",
            " - layer5.bias grad norm: 0.005201071035116911\n",
            "Gradients at iteration 389:\n",
            " - layer1.weight grad norm: 0.5384178757667542\n",
            " - layer1.bias grad norm: 0.000616624834947288\n",
            " - layer2.weight grad norm: 0.001961525995284319\n",
            " - layer2.bias grad norm: 0.0005413906183093786\n",
            " - layer2_input.weight grad norm: 0.4799164831638336\n",
            " - layer2_input.bias grad norm: 0.0005413906183093786\n",
            " - layer3.weight grad norm: 0.002678606892004609\n",
            " - layer3.bias grad norm: 0.0005771077703684568\n",
            " - layer3_input.weight grad norm: 0.5034220218658447\n",
            " - layer3_input.bias grad norm: 0.0005771077703684568\n",
            " - layer4.weight grad norm: 0.0031890005338937044\n",
            " - layer4.bias grad norm: 0.0005407363059930503\n",
            " - layer4_input.weight grad norm: 0.47554460167884827\n",
            " - layer4_input.bias grad norm: 0.0005407363059930503\n",
            " - layer5.weight grad norm: 0.012494699098169804\n",
            " - layer5.bias grad norm: 0.005520624574273825\n",
            "Gradients at iteration 390:\n",
            " - layer1.weight grad norm: 0.4922897517681122\n",
            " - layer1.bias grad norm: 0.0005642367177642882\n",
            " - layer2.weight grad norm: 0.0019042553612962365\n",
            " - layer2.bias grad norm: 0.0005725800874643028\n",
            " - layer2_input.weight grad norm: 0.49660542607307434\n",
            " - layer2_input.bias grad norm: 0.0005725800874643028\n",
            " - layer3.weight grad norm: 0.002560995751991868\n",
            " - layer3.bias grad norm: 0.0006289720768108964\n",
            " - layer3_input.weight grad norm: 0.5341737270355225\n",
            " - layer3_input.bias grad norm: 0.0006289720768108964\n",
            " - layer4.weight grad norm: 0.0031167820561677217\n",
            " - layer4.bias grad norm: 0.0005499172257259488\n",
            " - layer4_input.weight grad norm: 0.47486037015914917\n",
            " - layer4_input.bias grad norm: 0.0005499172257259488\n",
            " - layer5.weight grad norm: 0.012207571417093277\n",
            " - layer5.bias grad norm: 0.00533017423003912\n",
            "Gradients at iteration 391:\n",
            " - layer1.weight grad norm: 0.49589860439300537\n",
            " - layer1.bias grad norm: 0.0005701854242943227\n",
            " - layer2.weight grad norm: 0.0018879759591072798\n",
            " - layer2.bias grad norm: 0.0005498239770531654\n",
            " - layer2_input.weight grad norm: 0.4844369888305664\n",
            " - layer2_input.bias grad norm: 0.0005498239770531654\n",
            " - layer3.weight grad norm: 0.0026282102335244417\n",
            " - layer3.bias grad norm: 0.0006059763254597783\n",
            " - layer3_input.weight grad norm: 0.5228788256645203\n",
            " - layer3_input.bias grad norm: 0.0006059763254597783\n",
            " - layer4.weight grad norm: 0.003116062842309475\n",
            " - layer4.bias grad norm: 0.0005725544178858399\n",
            " - layer4_input.weight grad norm: 0.49580562114715576\n",
            " - layer4_input.bias grad norm: 0.0005725544178858399\n",
            " - layer5.weight grad norm: 0.01135239191353321\n",
            " - layer5.bias grad norm: 0.0053374553099274635\n",
            "Gradients at iteration 392:\n",
            " - layer1.weight grad norm: 0.48921748995780945\n",
            " - layer1.bias grad norm: 0.0005606833728961647\n",
            " - layer2.weight grad norm: 0.001856616116128862\n",
            " - layer2.bias grad norm: 0.0006321102846413851\n",
            " - layer2_input.weight grad norm: 0.5378598570823669\n",
            " - layer2_input.bias grad norm: 0.0006321102846413851\n",
            " - layer3.weight grad norm: 0.0025099979247897863\n",
            " - layer3.bias grad norm: 0.0006150443223305047\n",
            " - layer3_input.weight grad norm: 0.5247451066970825\n",
            " - layer3_input.bias grad norm: 0.0006150443223305047\n",
            " - layer4.weight grad norm: 0.0029990060720592737\n",
            " - layer4.bias grad norm: 0.0005055969231761992\n",
            " - layer4_input.weight grad norm: 0.4424893856048584\n",
            " - layer4_input.bias grad norm: 0.0005055969231761992\n",
            " - layer5.weight grad norm: 0.013066454790532589\n",
            " - layer5.bias grad norm: 0.005191920790821314\n",
            "Gradients at iteration 393:\n",
            " - layer1.weight grad norm: 0.49521559476852417\n",
            " - layer1.bias grad norm: 0.0005708468961529434\n",
            " - layer2.weight grad norm: 0.0018797626253217459\n",
            " - layer2.bias grad norm: 0.0005759429768659174\n",
            " - layer2_input.weight grad norm: 0.49927836656570435\n",
            " - layer2_input.bias grad norm: 0.0005759429768659174\n",
            " - layer3.weight grad norm: 0.0025656346697360277\n",
            " - layer3.bias grad norm: 0.0005678754532709718\n",
            " - layer3_input.weight grad norm: 0.4893663823604584\n",
            " - layer3_input.bias grad norm: 0.0005678754532709718\n",
            " - layer4.weight grad norm: 0.003079775720834732\n",
            " - layer4.bias grad norm: 0.0006070419331081212\n",
            " - layer4_input.weight grad norm: 0.5155249834060669\n",
            " - layer4_input.bias grad norm: 0.0006070419331081212\n",
            " - layer5.weight grad norm: 0.013685342855751514\n",
            " - layer5.bias grad norm: 0.005283453036099672\n",
            "Gradients at iteration 394:\n",
            " - layer1.weight grad norm: 0.5359073281288147\n",
            " - layer1.bias grad norm: 0.0006273816688917577\n",
            " - layer2.weight grad norm: 0.001758500817231834\n",
            " - layer2.bias grad norm: 0.0005797712947241962\n",
            " - layer2_input.weight grad norm: 0.49669891595840454\n",
            " - layer2_input.bias grad norm: 0.0005797712947241962\n",
            " - layer3.weight grad norm: 0.002434391528367996\n",
            " - layer3.bias grad norm: 0.0005822608945891261\n",
            " - layer3_input.weight grad norm: 0.4984671473503113\n",
            " - layer3_input.bias grad norm: 0.0005822608945891261\n",
            " - layer4.weight grad norm: 0.0028893069829791784\n",
            " - layer4.bias grad norm: 0.0005461865803226829\n",
            " - layer4_input.weight grad norm: 0.4663105607032776\n",
            " - layer4_input.bias grad norm: 0.0005461865803226829\n",
            " - layer5.weight grad norm: 0.01159570924937725\n",
            " - layer5.bias grad norm: 0.004943561740219593\n",
            "Gradients at iteration 395:\n",
            " - layer1.weight grad norm: 0.5616369843482971\n",
            " - layer1.bias grad norm: 0.0006510264938697219\n",
            " - layer2.weight grad norm: 0.0018470122013241053\n",
            " - layer2.bias grad norm: 0.0006085428176447749\n",
            " - layer2_input.weight grad norm: 0.526621401309967\n",
            " - layer2_input.bias grad norm: 0.0006085428176447749\n",
            " - layer3.weight grad norm: 0.0025286099407821894\n",
            " - layer3.bias grad norm: 0.0004857413296122104\n",
            " - layer3_input.weight grad norm: 0.43018338084220886\n",
            " - layer3_input.bias grad norm: 0.0004857413296122104\n",
            " - layer4.weight grad norm: 0.0029697823338210583\n",
            " - layer4.bias grad norm: 0.0005445160786621273\n",
            " - layer4_input.weight grad norm: 0.4711483418941498\n",
            " - layer4_input.bias grad norm: 0.0005445160786621273\n",
            " - layer5.weight grad norm: 0.012136601842939854\n",
            " - layer5.bias grad norm: 0.005208808463066816\n",
            "Gradients at iteration 396:\n",
            " - layer1.weight grad norm: 0.5612244009971619\n",
            " - layer1.bias grad norm: 0.000660338148009032\n",
            " - layer2.weight grad norm: 0.0017427493585273623\n",
            " - layer2.bias grad norm: 0.0005932070780545473\n",
            " - layer2_input.weight grad norm: 0.5080763101577759\n",
            " - layer2_input.bias grad norm: 0.0005932070780545473\n",
            " - layer3.weight grad norm: 0.0024276075419038534\n",
            " - layer3.bias grad norm: 0.0005271754343993962\n",
            " - layer3_input.weight grad norm: 0.46046701073646545\n",
            " - layer3_input.bias grad norm: 0.0005271754343993962\n",
            " - layer4.weight grad norm: 0.0028626120183616877\n",
            " - layer4.bias grad norm: 0.0005382701056078076\n",
            " - layer4_input.weight grad norm: 0.46333736181259155\n",
            " - layer4_input.bias grad norm: 0.0005382701056078076\n",
            " - layer5.weight grad norm: 0.01139567606151104\n",
            " - layer5.bias grad norm: 0.005011203698813915\n",
            "Gradients at iteration 397:\n",
            " - layer1.weight grad norm: 0.5078338384628296\n",
            " - layer1.bias grad norm: 0.0005874548223800957\n",
            " - layer2.weight grad norm: 0.001818825607188046\n",
            " - layer2.bias grad norm: 0.0005979415727779269\n",
            " - layer2_input.weight grad norm: 0.516440749168396\n",
            " - layer2_input.bias grad norm: 0.0005979415727779269\n",
            " - layer3.weight grad norm: 0.0025166876148432493\n",
            " - layer3.bias grad norm: 0.0005592938396148384\n",
            " - layer3_input.weight grad norm: 0.4837396442890167\n",
            " - layer3_input.bias grad norm: 0.0005592938396148384\n",
            " - layer4.weight grad norm: 0.0029377322643995285\n",
            " - layer4.bias grad norm: 0.0005715092411264777\n",
            " - layer4_input.weight grad norm: 0.49113729596138\n",
            " - layer4_input.bias grad norm: 0.0005715092411264777\n",
            " - layer5.weight grad norm: 0.011274912394583225\n",
            " - layer5.bias grad norm: 0.00510710384696722\n",
            "Gradients at iteration 398:\n",
            " - layer1.weight grad norm: 0.5061546564102173\n",
            " - layer1.bias grad norm: 0.0005769051495008171\n",
            " - layer2.weight grad norm: 0.0020131655037403107\n",
            " - layer2.bias grad norm: 0.0005839337827637792\n",
            " - layer2_input.weight grad norm: 0.5075257420539856\n",
            " - layer2_input.bias grad norm: 0.0005839337827637792\n",
            " - layer3.weight grad norm: 0.0027630096301436424\n",
            " - layer3.bias grad norm: 0.0006078057922422886\n",
            " - layer3_input.weight grad norm: 0.5222262144088745\n",
            " - layer3_input.bias grad norm: 0.0006078057922422886\n",
            " - layer4.weight grad norm: 0.0033010595943778753\n",
            " - layer4.bias grad norm: 0.0005285031511448324\n",
            " - layer4_input.weight grad norm: 0.4618334174156189\n",
            " - layer4_input.bias grad norm: 0.0005285031511448324\n",
            " - layer5.weight grad norm: 0.01254276093095541\n",
            " - layer5.bias grad norm: 0.005705204792320728\n",
            "Gradients at iteration 399:\n",
            " - layer1.weight grad norm: 0.5165904760360718\n",
            " - layer1.bias grad norm: 0.0005961183342151344\n",
            " - layer2.weight grad norm: 0.0018883132142946124\n",
            " - layer2.bias grad norm: 0.0006001045112498105\n",
            " - layer2_input.weight grad norm: 0.5193636417388916\n",
            " - layer2_input.bias grad norm: 0.0006001045112498105\n",
            " - layer3.weight grad norm: 0.002604772336781025\n",
            " - layer3.bias grad norm: 0.0005562410224229097\n",
            " - layer3_input.weight grad norm: 0.4856097102165222\n",
            " - layer3_input.bias grad norm: 0.0005562410224229097\n",
            " - layer4.weight grad norm: 0.0031094097066670656\n",
            " - layer4.bias grad norm: 0.0005471262848004699\n",
            " - layer4_input.weight grad norm: 0.47684064507484436\n",
            " - layer4_input.bias grad norm: 0.0005471262848004699\n",
            " - layer5.weight grad norm: 0.012293387204408646\n",
            " - layer5.bias grad norm: 0.005334924440830946\n",
            "Gradients at iteration 400:\n",
            " - layer1.weight grad norm: 0.49729952216148376\n",
            " - layer1.bias grad norm: 0.00056836026487872\n",
            " - layer2.weight grad norm: 0.0019224073039367795\n",
            " - layer2.bias grad norm: 0.0005691309343092144\n",
            " - layer2_input.weight grad norm: 0.497272253036499\n",
            " - layer2_input.bias grad norm: 0.0005691309343092144\n",
            " - layer3.weight grad norm: 0.00265472917817533\n",
            " - layer3.bias grad norm: 0.0006032236269675195\n",
            " - layer3_input.weight grad norm: 0.5189251899719238\n",
            " - layer3_input.bias grad norm: 0.0006032236269675195\n",
            " - layer4.weight grad norm: 0.0031126057729125023\n",
            " - layer4.bias grad norm: 0.0005636988207697868\n",
            " - layer4_input.weight grad norm: 0.4857326149940491\n",
            " - layer4_input.bias grad norm: 0.0005636988207697868\n",
            " - layer5.weight grad norm: 0.011897945776581764\n",
            " - layer5.bias grad norm: 0.00543040269985795\n",
            "It: 400, Loss: 5.379e+13, Y0: 3.495, Time: 1.65, Learning Rate: 1.000e-03\n",
            "Gradients at iteration 401:\n",
            " - layer1.weight grad norm: 0.5294485688209534\n",
            " - layer1.bias grad norm: 0.0006089042290113866\n",
            " - layer2.weight grad norm: 0.001883212011307478\n",
            " - layer2.bias grad norm: 0.0005621208692900836\n",
            " - layer2_input.weight grad norm: 0.488250732421875\n",
            " - layer2_input.bias grad norm: 0.0005621208692900836\n",
            " - layer3.weight grad norm: 0.002623592270538211\n",
            " - layer3.bias grad norm: 0.0005458428058773279\n",
            " - layer3_input.weight grad norm: 0.4770318269729614\n",
            " - layer3_input.bias grad norm: 0.0005458428058773279\n",
            " - layer4.weight grad norm: 0.003087038639932871\n",
            " - layer4.bias grad norm: 0.0005863150581717491\n",
            " - layer4_input.weight grad norm: 0.5035287141799927\n",
            " - layer4_input.bias grad norm: 0.0005863150581717491\n",
            " - layer5.weight grad norm: 0.012007897719740868\n",
            " - layer5.bias grad norm: 0.005327753722667694\n",
            "Gradients at iteration 402:\n",
            " - layer1.weight grad norm: 0.5167868137359619\n",
            " - layer1.bias grad norm: 0.000599764462094754\n",
            " - layer2.weight grad norm: 0.0018611598061397672\n",
            " - layer2.bias grad norm: 0.0006003220332786441\n",
            " - layer2_input.weight grad norm: 0.5146573185920715\n",
            " - layer2_input.bias grad norm: 0.0006003220332786441\n",
            " - layer3.weight grad norm: 0.0025454170536249876\n",
            " - layer3.bias grad norm: 0.000570397824048996\n",
            " - layer3_input.weight grad norm: 0.49018701910972595\n",
            " - layer3_input.bias grad norm: 0.000570397824048996\n",
            " - layer4.weight grad norm: 0.0029793037101626396\n",
            " - layer4.bias grad norm: 0.0005578367854468524\n",
            " - layer4_input.weight grad norm: 0.47706979513168335\n",
            " - layer4_input.bias grad norm: 0.0005578367854468524\n",
            " - layer5.weight grad norm: 0.011506072245538235\n",
            " - layer5.bias grad norm: 0.005169687792658806\n",
            "Gradients at iteration 403:\n",
            " - layer1.weight grad norm: 0.5474001169204712\n",
            " - layer1.bias grad norm: 0.0006403910228982568\n",
            " - layer2.weight grad norm: 0.0019226763397455215\n",
            " - layer2.bias grad norm: 0.000592519121710211\n",
            " - layer2_input.weight grad norm: 0.5132150053977966\n",
            " - layer2_input.bias grad norm: 0.000592519121710211\n",
            " - layer3.weight grad norm: 0.0026054223999381065\n",
            " - layer3.bias grad norm: 0.0005665457574650645\n",
            " - layer3_input.weight grad norm: 0.48494407534599304\n",
            " - layer3_input.bias grad norm: 0.0005665457574650645\n",
            " - layer4.weight grad norm: 0.0031085184309631586\n",
            " - layer4.bias grad norm: 0.0005102694849483669\n",
            " - layer4_input.weight grad norm: 0.4489823281764984\n",
            " - layer4_input.bias grad norm: 0.0005102694849483669\n",
            " - layer5.weight grad norm: 0.012493428774178028\n",
            " - layer5.bias grad norm: 0.005395880900323391\n",
            "Gradients at iteration 404:\n",
            " - layer1.weight grad norm: 0.505014181137085\n",
            " - layer1.bias grad norm: 0.0005791735602542758\n",
            " - layer2.weight grad norm: 0.0019295542733743787\n",
            " - layer2.bias grad norm: 0.0005400039372034371\n",
            " - layer2_input.weight grad norm: 0.4759192168712616\n",
            " - layer2_input.bias grad norm: 0.0005400039372034371\n",
            " - layer3.weight grad norm: 0.0026638975832611322\n",
            " - layer3.bias grad norm: 0.0006197067559696734\n",
            " - layer3_input.weight grad norm: 0.533688485622406\n",
            " - layer3_input.bias grad norm: 0.0006197067559696734\n",
            " - layer4.weight grad norm: 0.003199314698576927\n",
            " - layer4.bias grad norm: 0.0005555207026191056\n",
            " - layer4_input.weight grad norm: 0.48312684893608093\n",
            " - layer4_input.bias grad norm: 0.0005555207026191056\n",
            " - layer5.weight grad norm: 0.013174552470445633\n",
            " - layer5.bias grad norm: 0.005460057873278856\n",
            "Gradients at iteration 405:\n",
            " - layer1.weight grad norm: 0.5119045376777649\n",
            " - layer1.bias grad norm: 0.0005988088087178767\n",
            " - layer2.weight grad norm: 0.0018315964844077826\n",
            " - layer2.bias grad norm: 0.0005929499748162925\n",
            " - layer2_input.weight grad norm: 0.5073683261871338\n",
            " - layer2_input.bias grad norm: 0.0005929499748162925\n",
            " - layer3.weight grad norm: 0.0025083180516958237\n",
            " - layer3.bias grad norm: 0.0006298120133578777\n",
            " - layer3_input.weight grad norm: 0.5352729558944702\n",
            " - layer3_input.bias grad norm: 0.0006298120133578777\n",
            " - layer4.weight grad norm: 0.0030487817712128162\n",
            " - layer4.bias grad norm: 0.0005064889555796981\n",
            " - layer4_input.weight grad norm: 0.4402496814727783\n",
            " - layer4_input.bias grad norm: 0.0005064889555796981\n",
            " - layer5.weight grad norm: 0.012068279087543488\n",
            " - layer5.bias grad norm: 0.005223022308200598\n",
            "Gradients at iteration 406:\n",
            " - layer1.weight grad norm: 0.5506324768066406\n",
            " - layer1.bias grad norm: 0.0006522753392346203\n",
            " - layer2.weight grad norm: 0.0017564045265316963\n",
            " - layer2.bias grad norm: 0.0005975090898573399\n",
            " - layer2_input.weight grad norm: 0.5108489394187927\n",
            " - layer2_input.bias grad norm: 0.0005975090898573399\n",
            " - layer3.weight grad norm: 0.0024077268317341805\n",
            " - layer3.bias grad norm: 0.0005592923844233155\n",
            " - layer3_input.weight grad norm: 0.47897791862487793\n",
            " - layer3_input.bias grad norm: 0.0005592923844233155\n",
            " - layer4.weight grad norm: 0.002900181571021676\n",
            " - layer4.bias grad norm: 0.0005252553382888436\n",
            " - layer4_input.weight grad norm: 0.45412537455558777\n",
            " - layer4_input.bias grad norm: 0.0005252553382888436\n",
            " - layer5.weight grad norm: 0.011951934546232224\n",
            " - layer5.bias grad norm: 0.005014545284211636\n",
            "Gradients at iteration 407:\n",
            " - layer1.weight grad norm: 0.4746023416519165\n",
            " - layer1.bias grad norm: 0.0005423001712188125\n",
            " - layer2.weight grad norm: 0.001914206426590681\n",
            " - layer2.bias grad norm: 0.0006272068712860346\n",
            " - layer2_input.weight grad norm: 0.5358245372772217\n",
            " - layer2_input.bias grad norm: 0.0006272068712860346\n",
            " - layer3.weight grad norm: 0.0026541855186223984\n",
            " - layer3.bias grad norm: 0.000568143732380122\n",
            " - layer3_input.weight grad norm: 0.49366575479507446\n",
            " - layer3_input.bias grad norm: 0.000568143732380122\n",
            " - layer4.weight grad norm: 0.003174995072185993\n",
            " - layer4.bias grad norm: 0.0005790303112007678\n",
            " - layer4_input.weight grad norm: 0.49370986223220825\n",
            " - layer4_input.bias grad norm: 0.0005790303112007678\n",
            " - layer5.weight grad norm: 0.011671026237308979\n",
            " - layer5.bias grad norm: 0.00545576773583889\n",
            "Gradients at iteration 408:\n",
            " - layer1.weight grad norm: 0.5148146152496338\n",
            " - layer1.bias grad norm: 0.0005919916438870132\n",
            " - layer2.weight grad norm: 0.0018792094197124243\n",
            " - layer2.bias grad norm: 0.0005676904111169279\n",
            " - layer2_input.weight grad norm: 0.49468326568603516\n",
            " - layer2_input.bias grad norm: 0.0005676904111169279\n",
            " - layer3.weight grad norm: 0.002587194787338376\n",
            " - layer3.bias grad norm: 0.0005924015422351658\n",
            " - layer3_input.weight grad norm: 0.5106263756752014\n",
            " - layer3_input.bias grad norm: 0.0005924015422351658\n",
            " - layer4.weight grad norm: 0.0031017016153782606\n",
            " - layer4.bias grad norm: 0.0005507045425474644\n",
            " - layer4_input.weight grad norm: 0.4788529872894287\n",
            " - layer4_input.bias grad norm: 0.0005507045425474644\n",
            " - layer5.weight grad norm: 0.012818610295653343\n",
            " - layer5.bias grad norm: 0.005322934128344059\n",
            "Gradients at iteration 409:\n",
            " - layer1.weight grad norm: 0.5233613848686218\n",
            " - layer1.bias grad norm: 0.0006072311080060899\n",
            " - layer2.weight grad norm: 0.001915827626362443\n",
            " - layer2.bias grad norm: 0.0006270810845308006\n",
            " - layer2_input.weight grad norm: 0.5361188054084778\n",
            " - layer2_input.bias grad norm: 0.0006270810845308006\n",
            " - layer3.weight grad norm: 0.0025830119848251343\n",
            " - layer3.bias grad norm: 0.0005724024958908558\n",
            " - layer3_input.weight grad norm: 0.4930032193660736\n",
            " - layer3_input.bias grad norm: 0.0005724024958908558\n",
            " - layer4.weight grad norm: 0.0031367558985948563\n",
            " - layer4.bias grad norm: 0.0005043157143518329\n",
            " - layer4_input.weight grad norm: 0.44205668568611145\n",
            " - layer4_input.bias grad norm: 0.0005043157143518329\n",
            " - layer5.weight grad norm: 0.012329069897532463\n",
            " - layer5.bias grad norm: 0.005342656746506691\n",
            "Gradients at iteration 410:\n",
            " - layer1.weight grad norm: 0.5345998406410217\n",
            " - layer1.bias grad norm: 0.0006146716768853366\n",
            " - layer2.weight grad norm: 0.001930947182700038\n",
            " - layer2.bias grad norm: 0.0005709675024263561\n",
            " - layer2_input.weight grad norm: 0.4988679587841034\n",
            " - layer2_input.bias grad norm: 0.0005709675024263561\n",
            " - layer3.weight grad norm: 0.0026789356488734484\n",
            " - layer3.bias grad norm: 0.0005391226150095463\n",
            " - layer3_input.weight grad norm: 0.47287172079086304\n",
            " - layer3_input.bias grad norm: 0.0005391226150095463\n",
            " - layer4.weight grad norm: 0.00318285939283669\n",
            " - layer4.bias grad norm: 0.000563085253816098\n",
            " - layer4_input.weight grad norm: 0.49144527316093445\n",
            " - layer4_input.bias grad norm: 0.000563085253816098\n",
            " - layer5.weight grad norm: 0.012399469502270222\n",
            " - layer5.bias grad norm: 0.005522710736840963\n",
            "Gradients at iteration 411:\n",
            " - layer1.weight grad norm: 0.4961429536342621\n",
            " - layer1.bias grad norm: 0.0005706800729967654\n",
            " - layer2.weight grad norm: 0.0018065929180011153\n",
            " - layer2.bias grad norm: 0.0006148758693598211\n",
            " - layer2_input.weight grad norm: 0.5289035439491272\n",
            " - layer2_input.bias grad norm: 0.0006148758693598211\n",
            " - layer3.weight grad norm: 0.0025018223095685244\n",
            " - layer3.bias grad norm: 0.0006191973225213587\n",
            " - layer3_input.weight grad norm: 0.5303717255592346\n",
            " - layer3_input.bias grad norm: 0.0006191973225213587\n",
            " - layer4.weight grad norm: 0.002907546004280448\n",
            " - layer4.bias grad norm: 0.0004935804754495621\n",
            " - layer4_input.weight grad norm: 0.4388708174228668\n",
            " - layer4_input.bias grad norm: 0.0004935804754495621\n",
            " - layer5.weight grad norm: 0.012414975091814995\n",
            " - layer5.bias grad norm: 0.005179106257855892\n",
            "Gradients at iteration 412:\n",
            " - layer1.weight grad norm: 0.532433032989502\n",
            " - layer1.bias grad norm: 0.0006154901348054409\n",
            " - layer2.weight grad norm: 0.0018608170794323087\n",
            " - layer2.bias grad norm: 0.0005560026620514691\n",
            " - layer2_input.weight grad norm: 0.4813409745693207\n",
            " - layer2_input.bias grad norm: 0.0005560026620514691\n",
            " - layer3.weight grad norm: 0.0025549475103616714\n",
            " - layer3.bias grad norm: 0.0005754849989898503\n",
            " - layer3_input.weight grad norm: 0.49520593881607056\n",
            " - layer3_input.bias grad norm: 0.0005754849989898503\n",
            " - layer4.weight grad norm: 0.0030384836718440056\n",
            " - layer4.bias grad norm: 0.000567423936445266\n",
            " - layer4_input.weight grad norm: 0.48929116129875183\n",
            " - layer4_input.bias grad norm: 0.000567423936445266\n",
            " - layer5.weight grad norm: 0.011920920573174953\n",
            " - layer5.bias grad norm: 0.005253173410892487\n",
            "Gradients at iteration 413:\n",
            " - layer1.weight grad norm: 0.49480733275413513\n",
            " - layer1.bias grad norm: 0.0005681233596988022\n",
            " - layer2.weight grad norm: 0.0018924967152997851\n",
            " - layer2.bias grad norm: 0.0006198958726599813\n",
            " - layer2_input.weight grad norm: 0.5293910503387451\n",
            " - layer2_input.bias grad norm: 0.0006198958726599813\n",
            " - layer3.weight grad norm: 0.0026216055266559124\n",
            " - layer3.bias grad norm: 0.0004990312154404819\n",
            " - layer3_input.weight grad norm: 0.4406789243221283\n",
            " - layer3_input.bias grad norm: 0.0004990312154404819\n",
            " - layer4.weight grad norm: 0.0031279423274099827\n",
            " - layer4.bias grad norm: 0.0006207769620232284\n",
            " - layer4_input.weight grad norm: 0.5296465754508972\n",
            " - layer4_input.bias grad norm: 0.0006207769620232284\n",
            " - layer5.weight grad norm: 0.011642031371593475\n",
            " - layer5.bias grad norm: 0.005400775000452995\n",
            "Gradients at iteration 414:\n",
            " - layer1.weight grad norm: 0.5216453671455383\n",
            " - layer1.bias grad norm: 0.0006072426913306117\n",
            " - layer2.weight grad norm: 0.0018762960098683834\n",
            " - layer2.bias grad norm: 0.0006040380685590208\n",
            " - layer2_input.weight grad norm: 0.5150790214538574\n",
            " - layer2_input.bias grad norm: 0.0006040380685590208\n",
            " - layer3.weight grad norm: 0.0025808641221374273\n",
            " - layer3.bias grad norm: 0.0005620551528409123\n",
            " - layer3_input.weight grad norm: 0.48132026195526123\n",
            " - layer3_input.bias grad norm: 0.0005620551528409123\n",
            " - layer4.weight grad norm: 0.0031169611029326916\n",
            " - layer4.bias grad norm: 0.0005623637116514146\n",
            " - layer4_input.weight grad norm: 0.48034125566482544\n",
            " - layer4_input.bias grad norm: 0.0005623637116514146\n",
            " - layer5.weight grad norm: 0.011492790654301643\n",
            " - layer5.bias grad norm: 0.005327754188328981\n",
            "Gradients at iteration 415:\n",
            " - layer1.weight grad norm: 0.5697855949401855\n",
            " - layer1.bias grad norm: 0.0006715498748235404\n",
            " - layer2.weight grad norm: 0.0018528932705521584\n",
            " - layer2.bias grad norm: 0.0006039587897248566\n",
            " - layer2_input.weight grad norm: 0.5188660025596619\n",
            " - layer2_input.bias grad norm: 0.0006039587897248566\n",
            " - layer3.weight grad norm: 0.0025767511688172817\n",
            " - layer3.bias grad norm: 0.0005365675315260887\n",
            " - layer3_input.weight grad norm: 0.46376657485961914\n",
            " - layer3_input.bias grad norm: 0.0005365675315260887\n",
            " - layer4.weight grad norm: 0.00307559035718441\n",
            " - layer4.bias grad norm: 0.0005025833961553872\n",
            " - layer4_input.weight grad norm: 0.4368723928928375\n",
            " - layer4_input.bias grad norm: 0.0005025833961553872\n",
            " - layer5.weight grad norm: 0.011670497246086597\n",
            " - layer5.bias grad norm: 0.005259352270513773\n",
            "Gradients at iteration 416:\n",
            " - layer1.weight grad norm: 0.5773850083351135\n",
            " - layer1.bias grad norm: 0.0006854463135823607\n",
            " - layer2.weight grad norm: 0.001893247477710247\n",
            " - layer2.bias grad norm: 0.0005961043061688542\n",
            " - layer2_input.weight grad norm: 0.5141516327857971\n",
            " - layer2_input.bias grad norm: 0.0005961043061688542\n",
            " - layer3.weight grad norm: 0.002584253204986453\n",
            " - layer3.bias grad norm: 0.0005218597361817956\n",
            " - layer3_input.weight grad norm: 0.4576326906681061\n",
            " - layer3_input.bias grad norm: 0.0005218597361817956\n",
            " - layer4.weight grad norm: 0.003087586723268032\n",
            " - layer4.bias grad norm: 0.0005003709811717272\n",
            " - layer4_input.weight grad norm: 0.43890804052352905\n",
            " - layer4_input.bias grad norm: 0.0005003709811717272\n",
            " - layer5.weight grad norm: 0.012485500425100327\n",
            " - layer5.bias grad norm: 0.005370217375457287\n",
            "Gradients at iteration 417:\n",
            " - layer1.weight grad norm: 0.5434664487838745\n",
            " - layer1.bias grad norm: 0.0006323856650851667\n",
            " - layer2.weight grad norm: 0.001875569112598896\n",
            " - layer2.bias grad norm: 0.0005662835319526494\n",
            " - layer2_input.weight grad norm: 0.49222129583358765\n",
            " - layer2_input.bias grad norm: 0.0005662835319526494\n",
            " - layer3.weight grad norm: 0.0025744985323399305\n",
            " - layer3.bias grad norm: 0.0005552455550059676\n",
            " - layer3_input.weight grad norm: 0.4828762412071228\n",
            " - layer3_input.bias grad norm: 0.0005552455550059676\n",
            " - layer4.weight grad norm: 0.0030100762378424406\n",
            " - layer4.bias grad norm: 0.0005561488214880228\n",
            " - layer4_input.weight grad norm: 0.4785492718219757\n",
            " - layer4_input.bias grad norm: 0.0005561488214880228\n",
            " - layer5.weight grad norm: 0.01158242765814066\n",
            " - layer5.bias grad norm: 0.005263545084744692\n",
            "Gradients at iteration 418:\n",
            " - layer1.weight grad norm: 0.5041631460189819\n",
            " - layer1.bias grad norm: 0.0005862299585714936\n",
            " - layer2.weight grad norm: 0.001778651145286858\n",
            " - layer2.bias grad norm: 0.0005720226909033954\n",
            " - layer2_input.weight grad norm: 0.4928050637245178\n",
            " - layer2_input.bias grad norm: 0.0005720226909033954\n",
            " - layer3.weight grad norm: 0.002456005895510316\n",
            " - layer3.bias grad norm: 0.0006355307414196432\n",
            " - layer3_input.weight grad norm: 0.537861168384552\n",
            " - layer3_input.bias grad norm: 0.0006355307414196432\n",
            " - layer4.weight grad norm: 0.0029336512088775635\n",
            " - layer4.bias grad norm: 0.0005384031101129949\n",
            " - layer4_input.weight grad norm: 0.4620510935783386\n",
            " - layer4_input.bias grad norm: 0.0005384031101129949\n",
            " - layer5.weight grad norm: 0.011443356983363628\n",
            " - layer5.bias grad norm: 0.005059921648353338\n",
            "Gradients at iteration 419:\n",
            " - layer1.weight grad norm: 0.5212080478668213\n",
            " - layer1.bias grad norm: 0.000603276479523629\n",
            " - layer2.weight grad norm: 0.0019746955949813128\n",
            " - layer2.bias grad norm: 0.0005614276160486042\n",
            " - layer2_input.weight grad norm: 0.48910433053970337\n",
            " - layer2_input.bias grad norm: 0.0005614276160486042\n",
            " - layer3.weight grad norm: 0.002719548298045993\n",
            " - layer3.bias grad norm: 0.0005953673971816897\n",
            " - layer3_input.weight grad norm: 0.513907253742218\n",
            " - layer3_input.bias grad norm: 0.0005953673971816897\n",
            " - layer4.weight grad norm: 0.003217505756765604\n",
            " - layer4.bias grad norm: 0.0005427417927421629\n",
            " - layer4_input.weight grad norm: 0.47412699460983276\n",
            " - layer4_input.bias grad norm: 0.0005427417927421629\n",
            " - layer5.weight grad norm: 0.012913830578327179\n",
            " - layer5.bias grad norm: 0.005588100757449865\n",
            "Gradients at iteration 420:\n",
            " - layer1.weight grad norm: 0.5053824782371521\n",
            " - layer1.bias grad norm: 0.0005939486436545849\n",
            " - layer2.weight grad norm: 0.0018671907018870115\n",
            " - layer2.bias grad norm: 0.0005691408296115696\n",
            " - layer2_input.weight grad norm: 0.4881010949611664\n",
            " - layer2_input.bias grad norm: 0.0005691408296115696\n",
            " - layer3.weight grad norm: 0.0026112268678843975\n",
            " - layer3.bias grad norm: 0.0005976316751912236\n",
            " - layer3_input.weight grad norm: 0.5089647173881531\n",
            " - layer3_input.bias grad norm: 0.0005976316751912236\n",
            " - layer4.weight grad norm: 0.003044538665562868\n",
            " - layer4.bias grad norm: 0.0005915433866903186\n",
            " - layer4_input.weight grad norm: 0.49711519479751587\n",
            " - layer4_input.bias grad norm: 0.0005915433866903186\n",
            " - layer5.weight grad norm: 0.011279466561973095\n",
            " - layer5.bias grad norm: 0.005293050315231085\n",
            "Gradients at iteration 421:\n",
            " - layer1.weight grad norm: 0.528275728225708\n",
            " - layer1.bias grad norm: 0.0006169629632495344\n",
            " - layer2.weight grad norm: 0.0018584077479317784\n",
            " - layer2.bias grad norm: 0.000562300905585289\n",
            " - layer2_input.weight grad norm: 0.4860444962978363\n",
            " - layer2_input.bias grad norm: 0.000562300905585289\n",
            " - layer3.weight grad norm: 0.002492920495569706\n",
            " - layer3.bias grad norm: 0.0005674522835761309\n",
            " - layer3_input.weight grad norm: 0.48675698041915894\n",
            " - layer3_input.bias grad norm: 0.0005674522835761309\n",
            " - layer4.weight grad norm: 0.002982991747558117\n",
            " - layer4.bias grad norm: 0.0005862422985956073\n",
            " - layer4_input.weight grad norm: 0.4975634813308716\n",
            " - layer4_input.bias grad norm: 0.0005862422985956073\n",
            " - layer5.weight grad norm: 0.011653400957584381\n",
            " - layer5.bias grad norm: 0.005204679444432259\n",
            "Gradients at iteration 422:\n",
            " - layer1.weight grad norm: 0.5032532215118408\n",
            " - layer1.bias grad norm: 0.0005867709987796843\n",
            " - layer2.weight grad norm: 0.0017723412020131946\n",
            " - layer2.bias grad norm: 0.0006243214593268931\n",
            " - layer2_input.weight grad norm: 0.5307956337928772\n",
            " - layer2_input.bias grad norm: 0.0006243214593268931\n",
            " - layer3.weight grad norm: 0.0024390355683863163\n",
            " - layer3.bias grad norm: 0.0005818987265229225\n",
            " - layer3_input.weight grad norm: 0.49833646416664124\n",
            " - layer3_input.bias grad norm: 0.0005818987265229225\n",
            " - layer4.weight grad norm: 0.0029182150028645992\n",
            " - layer4.bias grad norm: 0.000544277485460043\n",
            " - layer4_input.weight grad norm: 0.4652596116065979\n",
            " - layer4_input.bias grad norm: 0.000544277485460043\n",
            " - layer5.weight grad norm: 0.011871254071593285\n",
            " - layer5.bias grad norm: 0.005050924140959978\n",
            "Gradients at iteration 423:\n",
            " - layer1.weight grad norm: 0.5219665765762329\n",
            " - layer1.bias grad norm: 0.000605350942350924\n",
            " - layer2.weight grad norm: 0.0018264544196426868\n",
            " - layer2.bias grad norm: 0.0005789740243926644\n",
            " - layer2_input.weight grad norm: 0.5004909634590149\n",
            " - layer2_input.bias grad norm: 0.0005789740243926644\n",
            " - layer3.weight grad norm: 0.0025827991776168346\n",
            " - layer3.bias grad norm: 0.0005780520150437951\n",
            " - layer3_input.weight grad norm: 0.4948749244213104\n",
            " - layer3_input.bias grad norm: 0.0005780520150437951\n",
            " - layer4.weight grad norm: 0.0031042462214827538\n",
            " - layer4.bias grad norm: 0.0005584523314610124\n",
            " - layer4_input.weight grad norm: 0.4816473424434662\n",
            " - layer4_input.bias grad norm: 0.0005584523314610124\n",
            " - layer5.weight grad norm: 0.011149903759360313\n",
            " - layer5.bias grad norm: 0.005288480315357447\n",
            "Gradients at iteration 424:\n",
            " - layer1.weight grad norm: 0.4977255165576935\n",
            " - layer1.bias grad norm: 0.0005779309431090951\n",
            " - layer2.weight grad norm: 0.0018195768352597952\n",
            " - layer2.bias grad norm: 0.0005921683041378856\n",
            " - layer2_input.weight grad norm: 0.5063583850860596\n",
            " - layer2_input.bias grad norm: 0.0005921683041378856\n",
            " - layer3.weight grad norm: 0.0025126219261437654\n",
            " - layer3.bias grad norm: 0.0005624163313768804\n",
            " - layer3_input.weight grad norm: 0.4838998317718506\n",
            " - layer3_input.bias grad norm: 0.0005624163313768804\n",
            " - layer4.weight grad norm: 0.0029936835635453463\n",
            " - layer4.bias grad norm: 0.0006016789120621979\n",
            " - layer4_input.weight grad norm: 0.5114015936851501\n",
            " - layer4_input.bias grad norm: 0.0006016789120621979\n",
            " - layer5.weight grad norm: 0.011493883095681667\n",
            " - layer5.bias grad norm: 0.005170036572962999\n",
            "Gradients at iteration 425:\n",
            " - layer1.weight grad norm: 0.49841517210006714\n",
            " - layer1.bias grad norm: 0.0005800017970614135\n",
            " - layer2.weight grad norm: 0.0017812545411288738\n",
            " - layer2.bias grad norm: 0.0005938394460827112\n",
            " - layer2_input.weight grad norm: 0.5094049572944641\n",
            " - layer2_input.bias grad norm: 0.0005938394460827112\n",
            " - layer3.weight grad norm: 0.0024316119961440563\n",
            " - layer3.bias grad norm: 0.0005931293126195669\n",
            " - layer3_input.weight grad norm: 0.5099344253540039\n",
            " - layer3_input.bias grad norm: 0.0005931293126195669\n",
            " - layer4.weight grad norm: 0.002904379041865468\n",
            " - layer4.bias grad norm: 0.0005585725302807987\n",
            " - layer4_input.weight grad norm: 0.4815339148044586\n",
            " - layer4_input.bias grad norm: 0.0005585725302807987\n",
            " - layer5.weight grad norm: 0.011645115911960602\n",
            " - layer5.bias grad norm: 0.00501159206032753\n",
            "Gradients at iteration 426:\n",
            " - layer1.weight grad norm: 0.5174899697303772\n",
            " - layer1.bias grad norm: 0.0005980912828817964\n",
            " - layer2.weight grad norm: 0.0018400178523734212\n",
            " - layer2.bias grad norm: 0.0005536747630685568\n",
            " - layer2_input.weight grad norm: 0.4828431010246277\n",
            " - layer2_input.bias grad norm: 0.0005536747630685568\n",
            " - layer3.weight grad norm: 0.002594769699499011\n",
            " - layer3.bias grad norm: 0.0005792242591269314\n",
            " - layer3_input.weight grad norm: 0.49885666370391846\n",
            " - layer3_input.bias grad norm: 0.0005792242591269314\n",
            " - layer4.weight grad norm: 0.0029948244336992502\n",
            " - layer4.bias grad norm: 0.0005839947261847556\n",
            " - layer4_input.weight grad norm: 0.5000165700912476\n",
            " - layer4_input.bias grad norm: 0.0005839947261847556\n",
            " - layer5.weight grad norm: 0.011959132738411427\n",
            " - layer5.bias grad norm: 0.005260353907942772\n",
            "Gradients at iteration 427:\n",
            " - layer1.weight grad norm: 0.5459437370300293\n",
            " - layer1.bias grad norm: 0.0006355937221087515\n",
            " - layer2.weight grad norm: 0.0018835760420188308\n",
            " - layer2.bias grad norm: 0.00057144311722368\n",
            " - layer2_input.weight grad norm: 0.4964039921760559\n",
            " - layer2_input.bias grad norm: 0.00057144311722368\n",
            " - layer3.weight grad norm: 0.0025944567751139402\n",
            " - layer3.bias grad norm: 0.0005235602729953825\n",
            " - layer3_input.weight grad norm: 0.45958462357521057\n",
            " - layer3_input.bias grad norm: 0.0005235602729953825\n",
            " - layer4.weight grad norm: 0.0030440897680819035\n",
            " - layer4.bias grad norm: 0.0005680661415681243\n",
            " - layer4_input.weight grad norm: 0.4940870702266693\n",
            " - layer4_input.bias grad norm: 0.0005680661415681243\n",
            " - layer5.weight grad norm: 0.011755730025470257\n",
            " - layer5.bias grad norm: 0.005329333711415529\n",
            "Gradients at iteration 428:\n",
            " - layer1.weight grad norm: 0.5231067538261414\n",
            " - layer1.bias grad norm: 0.0005992375081405044\n",
            " - layer2.weight grad norm: 0.0020061014220118523\n",
            " - layer2.bias grad norm: 0.0006048842915333807\n",
            " - layer2_input.weight grad norm: 0.5257216095924377\n",
            " - layer2_input.bias grad norm: 0.0006048842915333807\n",
            " - layer3.weight grad norm: 0.0028157534543424845\n",
            " - layer3.bias grad norm: 0.0005327688413672149\n",
            " - layer3_input.weight grad norm: 0.4702587127685547\n",
            " - layer3_input.bias grad norm: 0.0005327688413672149\n",
            " - layer4.weight grad norm: 0.003258345415815711\n",
            " - layer4.bias grad norm: 0.0005496544181369245\n",
            " - layer4_input.weight grad norm: 0.4781284034252167\n",
            " - layer4_input.bias grad norm: 0.0005496544181369245\n",
            " - layer5.weight grad norm: 0.012997332029044628\n",
            " - layer5.bias grad norm: 0.005701057612895966\n",
            "Gradients at iteration 429:\n",
            " - layer1.weight grad norm: 0.5143241286277771\n",
            " - layer1.bias grad norm: 0.0005860746605321765\n",
            " - layer2.weight grad norm: 0.0018592700362205505\n",
            " - layer2.bias grad norm: 0.00056794926058501\n",
            " - layer2_input.weight grad norm: 0.4956040382385254\n",
            " - layer2_input.bias grad norm: 0.00056794926058501\n",
            " - layer3.weight grad norm: 0.0025939529296010733\n",
            " - layer3.bias grad norm: 0.0005693467683158815\n",
            " - layer3_input.weight grad norm: 0.49642297625541687\n",
            " - layer3_input.bias grad norm: 0.0005693467683158815\n",
            " - layer4.weight grad norm: 0.0030963048338890076\n",
            " - layer4.bias grad norm: 0.0005718015017919242\n",
            " - layer4_input.weight grad norm: 0.49315470457077026\n",
            " - layer4_input.bias grad norm: 0.0005718015017919242\n",
            " - layer5.weight grad norm: 0.012643605470657349\n",
            " - layer5.bias grad norm: 0.005297177005559206\n",
            "Gradients at iteration 430:\n",
            " - layer1.weight grad norm: 0.5680026412010193\n",
            " - layer1.bias grad norm: 0.0006691903108730912\n",
            " - layer2.weight grad norm: 0.0019210709724575281\n",
            " - layer2.bias grad norm: 0.000545971211977303\n",
            " - layer2_input.weight grad norm: 0.4794009029865265\n",
            " - layer2_input.bias grad norm: 0.000545971211977303\n",
            " - layer3.weight grad norm: 0.0026810355484485626\n",
            " - layer3.bias grad norm: 0.0005429668235592544\n",
            " - layer3_input.weight grad norm: 0.4717903137207031\n",
            " - layer3_input.bias grad norm: 0.0005429668235592544\n",
            " - layer4.weight grad norm: 0.0031584661919623613\n",
            " - layer4.bias grad norm: 0.000552506186068058\n",
            " - layer4_input.weight grad norm: 0.4740583002567291\n",
            " - layer4_input.bias grad norm: 0.000552506186068058\n",
            " - layer5.weight grad norm: 0.013321936130523682\n",
            " - layer5.bias grad norm: 0.005448078736662865\n",
            "Gradients at iteration 431:\n",
            " - layer1.weight grad norm: 0.49617186188697815\n",
            " - layer1.bias grad norm: 0.0005614981055259705\n",
            " - layer2.weight grad norm: 0.002046982292085886\n",
            " - layer2.bias grad norm: 0.0006213481537997723\n",
            " - layer2_input.weight grad norm: 0.5349780917167664\n",
            " - layer2_input.bias grad norm: 0.0006213481537997723\n",
            " - layer3.weight grad norm: 0.0027801035903394222\n",
            " - layer3.bias grad norm: 0.0005772922304458916\n",
            " - layer3_input.weight grad norm: 0.5040589570999146\n",
            " - layer3_input.bias grad norm: 0.0005772922304458916\n",
            " - layer4.weight grad norm: 0.0032775786239653826\n",
            " - layer4.bias grad norm: 0.0005335346795618534\n",
            " - layer4_input.weight grad norm: 0.4618493318557739\n",
            " - layer4_input.bias grad norm: 0.0005335346795618534\n",
            " - layer5.weight grad norm: 0.0132081164047122\n",
            " - layer5.bias grad norm: 0.005678755231201649\n",
            "Gradients at iteration 432:\n",
            " - layer1.weight grad norm: 0.524115800857544\n",
            " - layer1.bias grad norm: 0.0006096096476539969\n",
            " - layer2.weight grad norm: 0.0017979413969442248\n",
            " - layer2.bias grad norm: 0.0005733784637413919\n",
            " - layer2_input.weight grad norm: 0.4992303252220154\n",
            " - layer2_input.bias grad norm: 0.0005733784637413919\n",
            " - layer3.weight grad norm: 0.002514007966965437\n",
            " - layer3.bias grad norm: 0.0005797733901999891\n",
            " - layer3_input.weight grad norm: 0.5000874996185303\n",
            " - layer3_input.bias grad norm: 0.0005797733901999891\n",
            " - layer4.weight grad norm: 0.00290718418546021\n",
            " - layer4.bias grad norm: 0.000550855475012213\n",
            " - layer4_input.weight grad norm: 0.47519323229789734\n",
            " - layer4_input.bias grad norm: 0.000550855475012213\n",
            " - layer5.weight grad norm: 0.011366461403667927\n",
            " - layer5.bias grad norm: 0.0051098656840622425\n",
            "Gradients at iteration 433:\n",
            " - layer1.weight grad norm: 0.516938328742981\n",
            " - layer1.bias grad norm: 0.0005932014901190996\n",
            " - layer2.weight grad norm: 0.0019008389208465815\n",
            " - layer2.bias grad norm: 0.0005609178333543241\n",
            " - layer2_input.weight grad norm: 0.4904470443725586\n",
            " - layer2_input.bias grad norm: 0.0005609178333543241\n",
            " - layer3.weight grad norm: 0.0025745201855897903\n",
            " - layer3.bias grad norm: 0.0005781421787105501\n",
            " - layer3_input.weight grad norm: 0.5005602240562439\n",
            " - layer3_input.bias grad norm: 0.0005781421787105501\n",
            " - layer4.weight grad norm: 0.003097527427598834\n",
            " - layer4.bias grad norm: 0.0005643372423946857\n",
            " - layer4_input.weight grad norm: 0.4914063513278961\n",
            " - layer4_input.bias grad norm: 0.0005643372423946857\n",
            " - layer5.weight grad norm: 0.012035341002047062\n",
            " - layer5.bias grad norm: 0.005362704861909151\n",
            "Gradients at iteration 434:\n",
            " - layer1.weight grad norm: 0.517350971698761\n",
            " - layer1.bias grad norm: 0.0005992043879814446\n",
            " - layer2.weight grad norm: 0.0018876738613471389\n",
            " - layer2.bias grad norm: 0.0005819990183226764\n",
            " - layer2_input.weight grad norm: 0.5028866529464722\n",
            " - layer2_input.bias grad norm: 0.0005819990183226764\n",
            " - layer3.weight grad norm: 0.0026038968935608864\n",
            " - layer3.bias grad norm: 0.0005579767166636884\n",
            " - layer3_input.weight grad norm: 0.4824126958847046\n",
            " - layer3_input.bias grad norm: 0.0005579767166636884\n",
            " - layer4.weight grad norm: 0.003085170406848192\n",
            " - layer4.bias grad norm: 0.0005785964312963188\n",
            " - layer4_input.weight grad norm: 0.49651414155960083\n",
            " - layer4_input.bias grad norm: 0.0005785964312963188\n",
            " - layer5.weight grad norm: 0.012425133027136326\n",
            " - layer5.bias grad norm: 0.005300320219248533\n",
            "Gradients at iteration 435:\n",
            " - layer1.weight grad norm: 0.49104511737823486\n",
            " - layer1.bias grad norm: 0.000568320625461638\n",
            " - layer2.weight grad norm: 0.0018904011230915785\n",
            " - layer2.bias grad norm: 0.0005613538669422269\n",
            " - layer2_input.weight grad norm: 0.4833207428455353\n",
            " - layer2_input.bias grad norm: 0.0005613538669422269\n",
            " - layer3.weight grad norm: 0.0026381323114037514\n",
            " - layer3.bias grad norm: 0.0006027555791661143\n",
            " - layer3_input.weight grad norm: 0.5139338970184326\n",
            " - layer3_input.bias grad norm: 0.0006027555791661143\n",
            " - layer4.weight grad norm: 0.003141860943287611\n",
            " - layer4.bias grad norm: 0.0006022960878908634\n",
            " - layer4_input.weight grad norm: 0.5108199119567871\n",
            " - layer4_input.bias grad norm: 0.0006022960878908634\n",
            " - layer5.weight grad norm: 0.012568129226565361\n",
            " - layer5.bias grad norm: 0.005464901216328144\n",
            "Gradients at iteration 436:\n",
            " - layer1.weight grad norm: 0.5403239727020264\n",
            " - layer1.bias grad norm: 0.0006384706939570606\n",
            " - layer2.weight grad norm: 0.0018368263263255358\n",
            " - layer2.bias grad norm: 0.0005799821810796857\n",
            " - layer2_input.weight grad norm: 0.4984194040298462\n",
            " - layer2_input.bias grad norm: 0.0005799821810796857\n",
            " - layer3.weight grad norm: 0.0025229586753994226\n",
            " - layer3.bias grad norm: 0.0005561047582887113\n",
            " - layer3_input.weight grad norm: 0.4777599573135376\n",
            " - layer3_input.bias grad norm: 0.0005561047582887113\n",
            " - layer4.weight grad norm: 0.00307270884513855\n",
            " - layer4.bias grad norm: 0.0005583789316006005\n",
            " - layer4_input.weight grad norm: 0.48080334067344666\n",
            " - layer4_input.bias grad norm: 0.0005583789316006005\n",
            " - layer5.weight grad norm: 0.012350262142717838\n",
            " - layer5.bias grad norm: 0.0052507854998111725\n",
            "Gradients at iteration 437:\n",
            " - layer1.weight grad norm: 0.5411356091499329\n",
            " - layer1.bias grad norm: 0.0006344282883219421\n",
            " - layer2.weight grad norm: 0.001871877466328442\n",
            " - layer2.bias grad norm: 0.0005443716654554009\n",
            " - layer2_input.weight grad norm: 0.4739712178707123\n",
            " - layer2_input.bias grad norm: 0.0005443716654554009\n",
            " - layer3.weight grad norm: 0.0026090648025274277\n",
            " - layer3.bias grad norm: 0.0006058241124264896\n",
            " - layer3_input.weight grad norm: 0.5179151296615601\n",
            " - layer3_input.bias grad norm: 0.0006058241124264896\n",
            " - layer4.weight grad norm: 0.0030577597208321095\n",
            " - layer4.bias grad norm: 0.0005336758331395686\n",
            " - layer4_input.weight grad norm: 0.46269869804382324\n",
            " - layer4_input.bias grad norm: 0.0005336758331395686\n",
            " - layer5.weight grad norm: 0.012136643752455711\n",
            " - layer5.bias grad norm: 0.005313861183822155\n",
            "Gradients at iteration 438:\n",
            " - layer1.weight grad norm: 0.5032344460487366\n",
            " - layer1.bias grad norm: 0.0005778719205409288\n",
            " - layer2.weight grad norm: 0.001864356454461813\n",
            " - layer2.bias grad norm: 0.0006104200147092342\n",
            " - layer2_input.weight grad norm: 0.524230420589447\n",
            " - layer2_input.bias grad norm: 0.0006104200147092342\n",
            " - layer3.weight grad norm: 0.0025943689979612827\n",
            " - layer3.bias grad norm: 0.0005893727648071945\n",
            " - layer3_input.weight grad norm: 0.5075284242630005\n",
            " - layer3_input.bias grad norm: 0.0005893727648071945\n",
            " - layer4.weight grad norm: 0.0030689004343003035\n",
            " - layer4.bias grad norm: 0.0005316159222275019\n",
            " - layer4_input.weight grad norm: 0.4627566933631897\n",
            " - layer4_input.bias grad norm: 0.0005316159222275019\n",
            " - layer5.weight grad norm: 0.012601693160831928\n",
            " - layer5.bias grad norm: 0.005291864741593599\n",
            "Gradients at iteration 439:\n",
            " - layer1.weight grad norm: 0.5155234932899475\n",
            " - layer1.bias grad norm: 0.0005949123296886683\n",
            " - layer2.weight grad norm: 0.0018932315288111567\n",
            " - layer2.bias grad norm: 0.0006060718442313373\n",
            " - layer2_input.weight grad norm: 0.5201206207275391\n",
            " - layer2_input.bias grad norm: 0.0006060718442313373\n",
            " - layer3.weight grad norm: 0.0026441689115017653\n",
            " - layer3.bias grad norm: 0.0005735629238188267\n",
            " - layer3_input.weight grad norm: 0.4924258887767792\n",
            " - layer3_input.bias grad norm: 0.0005735629238188267\n",
            " - layer4.weight grad norm: 0.00307597778737545\n",
            " - layer4.bias grad norm: 0.0005410969024524093\n",
            " - layer4_input.weight grad norm: 0.4701222777366638\n",
            " - layer4_input.bias grad norm: 0.0005410969024524093\n",
            " - layer5.weight grad norm: 0.012699740007519722\n",
            " - layer5.bias grad norm: 0.005293811671435833\n",
            "Gradients at iteration 440:\n",
            " - layer1.weight grad norm: 0.49383002519607544\n",
            " - layer1.bias grad norm: 0.0005672855186276138\n",
            " - layer2.weight grad norm: 0.0018095619743689895\n",
            " - layer2.bias grad norm: 0.0006115895812399685\n",
            " - layer2_input.weight grad norm: 0.5253653526306152\n",
            " - layer2_input.bias grad norm: 0.0006115895812399685\n",
            " - layer3.weight grad norm: 0.0025176412891596556\n",
            " - layer3.bias grad norm: 0.0005790149443782866\n",
            " - layer3_input.weight grad norm: 0.499685674905777\n",
            " - layer3_input.bias grad norm: 0.0005790149443782866\n",
            " - layer4.weight grad norm: 0.003032142762094736\n",
            " - layer4.bias grad norm: 0.0005554979434236884\n",
            " - layer4_input.weight grad norm: 0.47983238101005554\n",
            " - layer4_input.bias grad norm: 0.0005554979434236884\n",
            " - layer5.weight grad norm: 0.012266037985682487\n",
            " - layer5.bias grad norm: 0.005178959108889103\n",
            "Gradients at iteration 441:\n",
            " - layer1.weight grad norm: 0.5317489504814148\n",
            " - layer1.bias grad norm: 0.0006105626234784722\n",
            " - layer2.weight grad norm: 0.001905664219520986\n",
            " - layer2.bias grad norm: 0.0005689021782018244\n",
            " - layer2_input.weight grad norm: 0.5003543496131897\n",
            " - layer2_input.bias grad norm: 0.0005689021782018244\n",
            " - layer3.weight grad norm: 0.0026397218462079763\n",
            " - layer3.bias grad norm: 0.0005466658622026443\n",
            " - layer3_input.weight grad norm: 0.47819939255714417\n",
            " - layer3_input.bias grad norm: 0.0005466658622026443\n",
            " - layer4.weight grad norm: 0.0031556435860693455\n",
            " - layer4.bias grad norm: 0.0005584062892012298\n",
            " - layer4_input.weight grad norm: 0.48783648014068604\n",
            " - layer4_input.bias grad norm: 0.0005584062892012298\n",
            " - layer5.weight grad norm: 0.013317842036485672\n",
            " - layer5.bias grad norm: 0.0054127248004078865\n",
            "Gradients at iteration 442:\n",
            " - layer1.weight grad norm: 0.5418517589569092\n",
            " - layer1.bias grad norm: 0.0006167216342873871\n",
            " - layer2.weight grad norm: 0.0019102563383057714\n",
            " - layer2.bias grad norm: 0.0005245652864687145\n",
            " - layer2_input.weight grad norm: 0.4678346812725067\n",
            " - layer2_input.bias grad norm: 0.0005245652864687145\n",
            " - layer3.weight grad norm: 0.002630628179758787\n",
            " - layer3.bias grad norm: 0.000558456580620259\n",
            " - layer3_input.weight grad norm: 0.4865834712982178\n",
            " - layer3_input.bias grad norm: 0.000558456580620259\n",
            " - layer4.weight grad norm: 0.003131304634734988\n",
            " - layer4.bias grad norm: 0.0005744113586843014\n",
            " - layer4_input.weight grad norm: 0.5005642175674438\n",
            " - layer4_input.bias grad norm: 0.0005744113586843014\n",
            " - layer5.weight grad norm: 0.012163752689957619\n",
            " - layer5.bias grad norm: 0.0053748986683785915\n",
            "Gradients at iteration 443:\n",
            " - layer1.weight grad norm: 0.5181350708007812\n",
            " - layer1.bias grad norm: 0.0006002384470775723\n",
            " - layer2.weight grad norm: 0.0018886111211031675\n",
            " - layer2.bias grad norm: 0.0006215094472281635\n",
            " - layer2_input.weight grad norm: 0.5360692143440247\n",
            " - layer2_input.bias grad norm: 0.0006215094472281635\n",
            " - layer3.weight grad norm: 0.002632163232192397\n",
            " - layer3.bias grad norm: 0.0005764515371993184\n",
            " - layer3_input.weight grad norm: 0.5003827810287476\n",
            " - layer3_input.bias grad norm: 0.0005764515371993184\n",
            " - layer4.weight grad norm: 0.0031182647217065096\n",
            " - layer4.bias grad norm: 0.0005031018517911434\n",
            " - layer4_input.weight grad norm: 0.4399481415748596\n",
            " - layer4_input.bias grad norm: 0.0005031018517911434\n",
            " - layer5.weight grad norm: 0.013323256745934486\n",
            " - layer5.bias grad norm: 0.0053472500294446945\n",
            "Gradients at iteration 444:\n",
            " - layer1.weight grad norm: 0.5172180533409119\n",
            " - layer1.bias grad norm: 0.0005906486767344177\n",
            " - layer2.weight grad norm: 0.0019394458504393697\n",
            " - layer2.bias grad norm: 0.0006035126862116158\n",
            " - layer2_input.weight grad norm: 0.5232616066932678\n",
            " - layer2_input.bias grad norm: 0.0006035126862116158\n",
            " - layer3.weight grad norm: 0.0026437544729560614\n",
            " - layer3.bias grad norm: 0.0005491321790032089\n",
            " - layer3_input.weight grad norm: 0.4793485105037689\n",
            " - layer3_input.bias grad norm: 0.0005491321790032089\n",
            " - layer4.weight grad norm: 0.00316008017398417\n",
            " - layer4.bias grad norm: 0.0005445466958917677\n",
            " - layer4_input.weight grad norm: 0.4782271087169647\n",
            " - layer4_input.bias grad norm: 0.0005445466958917677\n",
            " - layer5.weight grad norm: 0.012402147054672241\n",
            " - layer5.bias grad norm: 0.005456889513880014\n",
            "Gradients at iteration 445:\n",
            " - layer1.weight grad norm: 0.5468136668205261\n",
            " - layer1.bias grad norm: 0.0006379838450811803\n",
            " - layer2.weight grad norm: 0.0018725099507719278\n",
            " - layer2.bias grad norm: 0.0005948427715338767\n",
            " - layer2_input.weight grad norm: 0.5143014192581177\n",
            " - layer2_input.bias grad norm: 0.0005948427715338767\n",
            " - layer3.weight grad norm: 0.0026071954052895308\n",
            " - layer3.bias grad norm: 0.0005479588871821761\n",
            " - layer3_input.weight grad norm: 0.4769282937049866\n",
            " - layer3_input.bias grad norm: 0.0005479588871821761\n",
            " - layer4.weight grad norm: 0.003146571107208729\n",
            " - layer4.bias grad norm: 0.0005224923952482641\n",
            " - layer4_input.weight grad norm: 0.4569624066352844\n",
            " - layer4_input.bias grad norm: 0.0005224923952482641\n",
            " - layer5.weight grad norm: 0.01274050772190094\n",
            " - layer5.bias grad norm: 0.005373724736273289\n",
            "Gradients at iteration 446:\n",
            " - layer1.weight grad norm: 0.49214833974838257\n",
            " - layer1.bias grad norm: 0.0005646771751344204\n",
            " - layer2.weight grad norm: 0.0018853966612368822\n",
            " - layer2.bias grad norm: 0.000598994258325547\n",
            " - layer2_input.weight grad norm: 0.5161694884300232\n",
            " - layer2_input.bias grad norm: 0.000598994258325547\n",
            " - layer3.weight grad norm: 0.002612005453556776\n",
            " - layer3.bias grad norm: 0.0006056947750039399\n",
            " - layer3_input.weight grad norm: 0.5154847502708435\n",
            " - layer3_input.bias grad norm: 0.0006056947750039399\n",
            " - layer4.weight grad norm: 0.0030869548209011555\n",
            " - layer4.bias grad norm: 0.0005538045079447329\n",
            " - layer4_input.weight grad norm: 0.47480204701423645\n",
            " - layer4_input.bias grad norm: 0.0005538045079447329\n",
            " - layer5.weight grad norm: 0.01209580060094595\n",
            " - layer5.bias grad norm: 0.00537347886711359\n",
            "Gradients at iteration 447:\n",
            " - layer1.weight grad norm: 0.5370926260948181\n",
            " - layer1.bias grad norm: 0.0006340544787235558\n",
            " - layer2.weight grad norm: 0.0017486221622675657\n",
            " - layer2.bias grad norm: 0.0005571871297433972\n",
            " - layer2_input.weight grad norm: 0.4799216389656067\n",
            " - layer2_input.bias grad norm: 0.0005571871297433972\n",
            " - layer3.weight grad norm: 0.0024088218342512846\n",
            " - layer3.bias grad norm: 0.000534845341462642\n",
            " - layer3_input.weight grad norm: 0.46014803647994995\n",
            " - layer3_input.bias grad norm: 0.000534845341462642\n",
            " - layer4.weight grad norm: 0.0028563025407493114\n",
            " - layer4.bias grad norm: 0.0006197376060299575\n",
            " - layer4_input.weight grad norm: 0.5189211368560791\n",
            " - layer4_input.bias grad norm: 0.0006197376060299575\n",
            " - layer5.weight grad norm: 0.012145222164690495\n",
            " - layer5.bias grad norm: 0.00493107084184885\n",
            "Gradients at iteration 448:\n",
            " - layer1.weight grad norm: 0.5411679148674011\n",
            " - layer1.bias grad norm: 0.0006265609408728778\n",
            " - layer2.weight grad norm: 0.0018920345464721322\n",
            " - layer2.bias grad norm: 0.0005942436400800943\n",
            " - layer2_input.weight grad norm: 0.5126863718032837\n",
            " - layer2_input.bias grad norm: 0.0005942436400800943\n",
            " - layer3.weight grad norm: 0.002576186554506421\n",
            " - layer3.bias grad norm: 0.0005745177040807903\n",
            " - layer3_input.weight grad norm: 0.49448761343955994\n",
            " - layer3_input.bias grad norm: 0.0005745177040807903\n",
            " - layer4.weight grad norm: 0.0030978042632341385\n",
            " - layer4.bias grad norm: 0.0005055444780737162\n",
            " - layer4_input.weight grad norm: 0.44673317670822144\n",
            " - layer4_input.bias grad norm: 0.0005055444780737162\n",
            " - layer5.weight grad norm: 0.012263230979442596\n",
            " - layer5.bias grad norm: 0.005367367994040251\n",
            "Gradients at iteration 449:\n",
            " - layer1.weight grad norm: 0.5046492218971252\n",
            " - layer1.bias grad norm: 0.0005810537841171026\n",
            " - layer2.weight grad norm: 0.0018160173203796148\n",
            " - layer2.bias grad norm: 0.0006436069961637259\n",
            " - layer2_input.weight grad norm: 0.5495635867118835\n",
            " - layer2_input.bias grad norm: 0.0006436069961637259\n",
            " - layer3.weight grad norm: 0.0024863826110959053\n",
            " - layer3.bias grad norm: 0.0005910638719797134\n",
            " - layer3_input.weight grad norm: 0.5042580366134644\n",
            " - layer3_input.bias grad norm: 0.0005910638719797134\n",
            " - layer4.weight grad norm: 0.0029659068677574396\n",
            " - layer4.bias grad norm: 0.0004951951559633017\n",
            " - layer4_input.weight grad norm: 0.4345666468143463\n",
            " - layer4_input.bias grad norm: 0.0004951951559633017\n",
            " - layer5.weight grad norm: 0.01174765732139349\n",
            " - layer5.bias grad norm: 0.0051038069650530815\n",
            "Gradients at iteration 450:\n",
            " - layer1.weight grad norm: 0.4825950264930725\n",
            " - layer1.bias grad norm: 0.0005495014484040439\n",
            " - layer2.weight grad norm: 0.001922228024341166\n",
            " - layer2.bias grad norm: 0.0006477890419773757\n",
            " - layer2_input.weight grad norm: 0.5491089820861816\n",
            " - layer2_input.bias grad norm: 0.0006477890419773757\n",
            " - layer3.weight grad norm: 0.002588124480098486\n",
            " - layer3.bias grad norm: 0.0005625175544992089\n",
            " - layer3_input.weight grad norm: 0.4859042167663574\n",
            " - layer3_input.bias grad norm: 0.0005625175544992089\n",
            " - layer4.weight grad norm: 0.003129039192572236\n",
            " - layer4.bias grad norm: 0.0005534590454772115\n",
            " - layer4_input.weight grad norm: 0.47882020473480225\n",
            " - layer4_input.bias grad norm: 0.0005534590454772115\n",
            " - layer5.weight grad norm: 0.012578096240758896\n",
            " - layer5.bias grad norm: 0.005380460061132908\n",
            "Gradients at iteration 451:\n",
            " - layer1.weight grad norm: 0.5055886507034302\n",
            " - layer1.bias grad norm: 0.0005870727472938597\n",
            " - layer2.weight grad norm: 0.00190248666331172\n",
            " - layer2.bias grad norm: 0.0005504581495188177\n",
            " - layer2_input.weight grad norm: 0.48033249378204346\n",
            " - layer2_input.bias grad norm: 0.0005504581495188177\n",
            " - layer3.weight grad norm: 0.0025952637661248446\n",
            " - layer3.bias grad norm: 0.0006512847612611949\n",
            " - layer3_input.weight grad norm: 0.5528564453125\n",
            " - layer3_input.bias grad norm: 0.0006512847612611949\n",
            " - layer4.weight grad norm: 0.0030939443968236446\n",
            " - layer4.bias grad norm: 0.0005270988331176341\n",
            " - layer4_input.weight grad norm: 0.45585939288139343\n",
            " - layer4_input.bias grad norm: 0.0005270988331176341\n",
            " - layer5.weight grad norm: 0.012343564070761204\n",
            " - layer5.bias grad norm: 0.005311231594532728\n",
            "Gradients at iteration 452:\n",
            " - layer1.weight grad norm: 0.5280976891517639\n",
            " - layer1.bias grad norm: 0.0006116956355981529\n",
            " - layer2.weight grad norm: 0.0018489144276827574\n",
            " - layer2.bias grad norm: 0.0005617125425487757\n",
            " - layer2_input.weight grad norm: 0.4900265038013458\n",
            " - layer2_input.bias grad norm: 0.0005617125425487757\n",
            " - layer3.weight grad norm: 0.002538308035582304\n",
            " - layer3.bias grad norm: 0.0005768027040176094\n",
            " - layer3_input.weight grad norm: 0.4971413016319275\n",
            " - layer3_input.bias grad norm: 0.0005768027040176094\n",
            " - layer4.weight grad norm: 0.0030137423891574144\n",
            " - layer4.bias grad norm: 0.0005599296418949962\n",
            " - layer4_input.weight grad norm: 0.4833777844905853\n",
            " - layer4_input.bias grad norm: 0.0005599296418949962\n",
            " - layer5.weight grad norm: 0.011598975397646427\n",
            " - layer5.bias grad norm: 0.005246206186711788\n",
            "Gradients at iteration 453:\n",
            " - layer1.weight grad norm: 0.5167709589004517\n",
            " - layer1.bias grad norm: 0.000596032477915287\n",
            " - layer2.weight grad norm: 0.001880659256130457\n",
            " - layer2.bias grad norm: 0.0005190899828448892\n",
            " - layer2_input.weight grad norm: 0.45727965235710144\n",
            " - layer2_input.bias grad norm: 0.0005190899828448892\n",
            " - layer3.weight grad norm: 0.002601807238534093\n",
            " - layer3.bias grad norm: 0.0006293103797361255\n",
            " - layer3_input.weight grad norm: 0.5372675061225891\n",
            " - layer3_input.bias grad norm: 0.0006293103797361255\n",
            " - layer4.weight grad norm: 0.003081963397562504\n",
            " - layer4.bias grad norm: 0.0005621135351248085\n",
            " - layer4_input.weight grad norm: 0.48476552963256836\n",
            " - layer4_input.bias grad norm: 0.0005621135351248085\n",
            " - layer5.weight grad norm: 0.011739018373191357\n",
            " - layer5.bias grad norm: 0.005402581766247749\n",
            "Gradients at iteration 454:\n",
            " - layer1.weight grad norm: 0.5002774000167847\n",
            " - layer1.bias grad norm: 0.0005790249560959637\n",
            " - layer2.weight grad norm: 0.0018617537571117282\n",
            " - layer2.bias grad norm: 0.0005878127994947135\n",
            " - layer2_input.weight grad norm: 0.5068246126174927\n",
            " - layer2_input.bias grad norm: 0.0005878127994947135\n",
            " - layer3.weight grad norm: 0.0025482228957116604\n",
            " - layer3.bias grad norm: 0.0005469021270982921\n",
            " - layer3_input.weight grad norm: 0.47357815504074097\n",
            " - layer3_input.bias grad norm: 0.0005469021270982921\n",
            " - layer4.weight grad norm: 0.002995832357555628\n",
            " - layer4.bias grad norm: 0.0006082260515540838\n",
            " - layer4_input.weight grad norm: 0.5180613994598389\n",
            " - layer4_input.bias grad norm: 0.0006082260515540838\n",
            " - layer5.weight grad norm: 0.011794948019087315\n",
            " - layer5.bias grad norm: 0.00520498538389802\n",
            "Gradients at iteration 455:\n",
            " - layer1.weight grad norm: 0.5430424809455872\n",
            " - layer1.bias grad norm: 0.0006274406332522631\n",
            " - layer2.weight grad norm: 0.0018530869856476784\n",
            " - layer2.bias grad norm: 0.0006093806587159634\n",
            " - layer2_input.weight grad norm: 0.5257204174995422\n",
            " - layer2_input.bias grad norm: 0.0006093806587159634\n",
            " - layer3.weight grad norm: 0.002551793586462736\n",
            " - layer3.bias grad norm: 0.0005612886743620038\n",
            " - layer3_input.weight grad norm: 0.48645293712615967\n",
            " - layer3_input.bias grad norm: 0.0005612886743620038\n",
            " - layer4.weight grad norm: 0.003049160586670041\n",
            " - layer4.bias grad norm: 0.0005018385709263384\n",
            " - layer4_input.weight grad norm: 0.4380771815776825\n",
            " - layer4_input.bias grad norm: 0.0005018385709263384\n",
            " - layer5.weight grad norm: 0.01122851949185133\n",
            " - layer5.bias grad norm: 0.005227352492511272\n",
            "Gradients at iteration 456:\n",
            " - layer1.weight grad norm: 0.518348217010498\n",
            " - layer1.bias grad norm: 0.0006025846232660115\n",
            " - layer2.weight grad norm: 0.001864921534433961\n",
            " - layer2.bias grad norm: 0.0005704044597223401\n",
            " - layer2_input.weight grad norm: 0.49114474654197693\n",
            " - layer2_input.bias grad norm: 0.0005704044597223401\n",
            " - layer3.weight grad norm: 0.002552216872572899\n",
            " - layer3.bias grad norm: 0.0006143293576315045\n",
            " - layer3_input.weight grad norm: 0.5218986868858337\n",
            " - layer3_input.bias grad norm: 0.0006143293576315045\n",
            " - layer4.weight grad norm: 0.0030681376811116934\n",
            " - layer4.bias grad norm: 0.0005403441027738154\n",
            " - layer4_input.weight grad norm: 0.4663947522640228\n",
            " - layer4_input.bias grad norm: 0.0005403441027738154\n",
            " - layer5.weight grad norm: 0.011838595382869244\n",
            " - layer5.bias grad norm: 0.005256634205579758\n",
            "Gradients at iteration 457:\n",
            " - layer1.weight grad norm: 0.5096338391304016\n",
            " - layer1.bias grad norm: 0.0005861598765477538\n",
            " - layer2.weight grad norm: 0.0019151336746290326\n",
            " - layer2.bias grad norm: 0.0005887733423151076\n",
            " - layer2_input.weight grad norm: 0.5126665234565735\n",
            " - layer2_input.bias grad norm: 0.0005887733423151076\n",
            " - layer3.weight grad norm: 0.0026398489717394114\n",
            " - layer3.bias grad norm: 0.0005606372142210603\n",
            " - layer3_input.weight grad norm: 0.4893367290496826\n",
            " - layer3_input.bias grad norm: 0.0005606372142210603\n",
            " - layer4.weight grad norm: 0.0031452025286853313\n",
            " - layer4.bias grad norm: 0.0005606149206869304\n",
            " - layer4_input.weight grad norm: 0.4876301884651184\n",
            " - layer4_input.bias grad norm: 0.0005606149206869304\n",
            " - layer5.weight grad norm: 0.012665965594351292\n",
            " - layer5.bias grad norm: 0.005444711539894342\n",
            "Gradients at iteration 458:\n",
            " - layer1.weight grad norm: 0.538797914981842\n",
            " - layer1.bias grad norm: 0.0006191835855133832\n",
            " - layer2.weight grad norm: 0.0020473680924624205\n",
            " - layer2.bias grad norm: 0.00059841712936759\n",
            " - layer2_input.weight grad norm: 0.5186988711357117\n",
            " - layer2_input.bias grad norm: 0.00059841712936759\n",
            " - layer3.weight grad norm: 0.0027972450479865074\n",
            " - layer3.bias grad norm: 0.00054918322712183\n",
            " - layer3_input.weight grad norm: 0.48073244094848633\n",
            " - layer3_input.bias grad norm: 0.00054918322712183\n",
            " - layer4.weight grad norm: 0.0033399942331016064\n",
            " - layer4.bias grad norm: 0.0005283405771479011\n",
            " - layer4_input.weight grad norm: 0.45754608511924744\n",
            " - layer4_input.bias grad norm: 0.0005283405771479011\n",
            " - layer5.weight grad norm: 0.011737968772649765\n",
            " - layer5.bias grad norm: 0.005743111949414015\n",
            "Gradients at iteration 459:\n",
            " - layer1.weight grad norm: 0.5259672403335571\n",
            " - layer1.bias grad norm: 0.0006063454202376306\n",
            " - layer2.weight grad norm: 0.0018664388917386532\n",
            " - layer2.bias grad norm: 0.0005101529532112181\n",
            " - layer2_input.weight grad norm: 0.4518171548843384\n",
            " - layer2_input.bias grad norm: 0.0005101529532112181\n",
            " - layer3.weight grad norm: 0.0026285394560545683\n",
            " - layer3.bias grad norm: 0.0005765191745012999\n",
            " - layer3_input.weight grad norm: 0.5009759068489075\n",
            " - layer3_input.bias grad norm: 0.0005765191745012999\n",
            " - layer4.weight grad norm: 0.003050062106922269\n",
            " - layer4.bias grad norm: 0.0006029495852999389\n",
            " - layer4_input.weight grad norm: 0.5177323222160339\n",
            " - layer4_input.bias grad norm: 0.0006029495852999389\n",
            " - layer5.weight grad norm: 0.012071930803358555\n",
            " - layer5.bias grad norm: 0.0053215776570141315\n",
            "Gradients at iteration 460:\n",
            " - layer1.weight grad norm: 0.548168957233429\n",
            " - layer1.bias grad norm: 0.0006424381863325834\n",
            " - layer2.weight grad norm: 0.0018345036078244448\n",
            " - layer2.bias grad norm: 0.0005751395365223289\n",
            " - layer2_input.weight grad norm: 0.49763593077659607\n",
            " - layer2_input.bias grad norm: 0.0005751395365223289\n",
            " - layer3.weight grad norm: 0.002540540648624301\n",
            " - layer3.bias grad norm: 0.000531176570802927\n",
            " - layer3_input.weight grad norm: 0.462429940700531\n",
            " - layer3_input.bias grad norm: 0.000531176570802927\n",
            " - layer4.weight grad norm: 0.003013198496773839\n",
            " - layer4.bias grad norm: 0.0005638272850774229\n",
            " - layer4_input.weight grad norm: 0.48768144845962524\n",
            " - layer4_input.bias grad norm: 0.0005638272850774229\n",
            " - layer5.weight grad norm: 0.012071071192622185\n",
            " - layer5.bias grad norm: 0.005263935308903456\n",
            "Gradients at iteration 461:\n",
            " - layer1.weight grad norm: 0.5490001440048218\n",
            " - layer1.bias grad norm: 0.000637696182820946\n",
            " - layer2.weight grad norm: 0.001833861693739891\n",
            " - layer2.bias grad norm: 0.00058413902297616\n",
            " - layer2_input.weight grad norm: 0.5070167779922485\n",
            " - layer2_input.bias grad norm: 0.00058413902297616\n",
            " - layer3.weight grad norm: 0.0025558429770171642\n",
            " - layer3.bias grad norm: 0.0005298425676301122\n",
            " - layer3_input.weight grad norm: 0.4639025926589966\n",
            " - layer3_input.bias grad norm: 0.0005298425676301122\n",
            " - layer4.weight grad norm: 0.0030298749916255474\n",
            " - layer4.bias grad norm: 0.0005471307085826993\n",
            " - layer4_input.weight grad norm: 0.4755460321903229\n",
            " - layer4_input.bias grad norm: 0.0005471307085826993\n",
            " - layer5.weight grad norm: 0.011591306887567043\n",
            " - layer5.bias grad norm: 0.005236534867435694\n",
            "Gradients at iteration 462:\n",
            " - layer1.weight grad norm: 0.5010179877281189\n",
            " - layer1.bias grad norm: 0.0005741934292018414\n",
            " - layer2.weight grad norm: 0.0018224790692329407\n",
            " - layer2.bias grad norm: 0.0006059724255464971\n",
            " - layer2_input.weight grad norm: 0.5223433971405029\n",
            " - layer2_input.bias grad norm: 0.0006059724255464971\n",
            " - layer3.weight grad norm: 0.0025373732205480337\n",
            " - layer3.bias grad norm: 0.0005750947166234255\n",
            " - layer3_input.weight grad norm: 0.49630314111709595\n",
            " - layer3_input.bias grad norm: 0.0005750947166234255\n",
            " - layer4.weight grad norm: 0.003035949543118477\n",
            " - layer4.bias grad norm: 0.0005539344856515527\n",
            " - layer4_input.weight grad norm: 0.4791809916496277\n",
            " - layer4_input.bias grad norm: 0.0005539344856515527\n",
            " - layer5.weight grad norm: 0.012593035586178303\n",
            " - layer5.bias grad norm: 0.005217315629124641\n",
            "Gradients at iteration 463:\n",
            " - layer1.weight grad norm: 0.5244052410125732\n",
            " - layer1.bias grad norm: 0.0006059615407139063\n",
            " - layer2.weight grad norm: 0.0017776344902813435\n",
            " - layer2.bias grad norm: 0.0005701524205505848\n",
            " - layer2_input.weight grad norm: 0.49633660912513733\n",
            " - layer2_input.bias grad norm: 0.0005701524205505848\n",
            " - layer3.weight grad norm: 0.002519363071769476\n",
            " - layer3.bias grad norm: 0.0005386496777646244\n",
            " - layer3_input.weight grad norm: 0.47010451555252075\n",
            " - layer3_input.bias grad norm: 0.0005386496777646244\n",
            " - layer4.weight grad norm: 0.0029506380669772625\n",
            " - layer4.bias grad norm: 0.000589885690715164\n",
            " - layer4_input.weight grad norm: 0.5074147582054138\n",
            " - layer4_input.bias grad norm: 0.000589885690715164\n",
            " - layer5.weight grad norm: 0.011594222858548164\n",
            " - layer5.bias grad norm: 0.005127043928951025\n",
            "Gradients at iteration 464:\n",
            " - layer1.weight grad norm: 0.5158835649490356\n",
            " - layer1.bias grad norm: 0.0006010369397699833\n",
            " - layer2.weight grad norm: 0.0017571530770510435\n",
            " - layer2.bias grad norm: 0.0005815914482809603\n",
            " - layer2_input.weight grad norm: 0.49638181924819946\n",
            " - layer2_input.bias grad norm: 0.0005815914482809603\n",
            " - layer3.weight grad norm: 0.0023868922144174576\n",
            " - layer3.bias grad norm: 0.0005339983035810292\n",
            " - layer3_input.weight grad norm: 0.45508068799972534\n",
            " - layer3_input.bias grad norm: 0.0005339983035810292\n",
            " - layer4.weight grad norm: 0.0028700458351522684\n",
            " - layer4.bias grad norm: 0.0006277677603065968\n",
            " - layer4_input.weight grad norm: 0.5293456315994263\n",
            " - layer4_input.bias grad norm: 0.0006277677603065968\n",
            " - layer5.weight grad norm: 0.01094018667936325\n",
            " - layer5.bias grad norm: 0.00499222194775939\n",
            "Gradients at iteration 465:\n",
            " - layer1.weight grad norm: 0.5100854635238647\n",
            " - layer1.bias grad norm: 0.0005982895963825285\n",
            " - layer2.weight grad norm: 0.0017852751770988107\n",
            " - layer2.bias grad norm: 0.0005995315150357783\n",
            " - layer2_input.weight grad norm: 0.508899450302124\n",
            " - layer2_input.bias grad norm: 0.0005995315150357783\n",
            " - layer3.weight grad norm: 0.00245700404047966\n",
            " - layer3.bias grad norm: 0.0005969959311187267\n",
            " - layer3_input.weight grad norm: 0.506047248840332\n",
            " - layer3_input.bias grad norm: 0.0005969959311187267\n",
            " - layer4.weight grad norm: 0.002949229208752513\n",
            " - layer4.bias grad norm: 0.0005583206657320261\n",
            " - layer4_input.weight grad norm: 0.4738861322402954\n",
            " - layer4_input.bias grad norm: 0.0005583206657320261\n",
            " - layer5.weight grad norm: 0.011664566583931446\n",
            " - layer5.bias grad norm: 0.005093446932733059\n",
            "Gradients at iteration 466:\n",
            " - layer1.weight grad norm: 0.5203841924667358\n",
            " - layer1.bias grad norm: 0.0006068247021175921\n",
            " - layer2.weight grad norm: 0.0018753963522613049\n",
            " - layer2.bias grad norm: 0.0005669739912264049\n",
            " - layer2_input.weight grad norm: 0.4895612299442291\n",
            " - layer2_input.bias grad norm: 0.0005669739912264049\n",
            " - layer3.weight grad norm: 0.0025809374637901783\n",
            " - layer3.bias grad norm: 0.0005313096917234361\n",
            " - layer3_input.weight grad norm: 0.4600706100463867\n",
            " - layer3_input.bias grad norm: 0.0005313096917234361\n",
            " - layer4.weight grad norm: 0.0030903732404112816\n",
            " - layer4.bias grad norm: 0.0006205198587849736\n",
            " - layer4_input.weight grad norm: 0.5269547700881958\n",
            " - layer4_input.bias grad norm: 0.0006205198587849736\n",
            " - layer5.weight grad norm: 0.011574415490031242\n",
            " - layer5.bias grad norm: 0.00525491451844573\n",
            "Gradients at iteration 467:\n",
            " - layer1.weight grad norm: 0.5318563580513\n",
            " - layer1.bias grad norm: 0.0006196378380991518\n",
            " - layer2.weight grad norm: 0.0018524541519582272\n",
            " - layer2.bias grad norm: 0.0005671429680660367\n",
            " - layer2_input.weight grad norm: 0.48866480588912964\n",
            " - layer2_input.bias grad norm: 0.0005671429680660367\n",
            " - layer3.weight grad norm: 0.0025279540568590164\n",
            " - layer3.bias grad norm: 0.0005969543708488345\n",
            " - layer3_input.weight grad norm: 0.5108808875083923\n",
            " - layer3_input.bias grad norm: 0.0005969543708488345\n",
            " - layer4.weight grad norm: 0.0030452574137598276\n",
            " - layer4.bias grad norm: 0.000538912252523005\n",
            " - layer4_input.weight grad norm: 0.46597716212272644\n",
            " - layer4_input.bias grad norm: 0.000538912252523005\n",
            " - layer5.weight grad norm: 0.01235897745937109\n",
            " - layer5.bias grad norm: 0.005235522054135799\n",
            "Gradients at iteration 468:\n",
            " - layer1.weight grad norm: 0.5318463444709778\n",
            " - layer1.bias grad norm: 0.0006148283137008548\n",
            " - layer2.weight grad norm: 0.0019549306016415358\n",
            " - layer2.bias grad norm: 0.0005789187853224576\n",
            " - layer2_input.weight grad norm: 0.4948585629463196\n",
            " - layer2_input.bias grad norm: 0.0005789187853224576\n",
            " - layer3.weight grad norm: 0.0027422034181654453\n",
            " - layer3.bias grad norm: 0.0005295927403494716\n",
            " - layer3_input.weight grad norm: 0.46040311455726624\n",
            " - layer3_input.bias grad norm: 0.0005295927403494716\n",
            " - layer4.weight grad norm: 0.003143120091408491\n",
            " - layer4.bias grad norm: 0.0005997302359901369\n",
            " - layer4_input.weight grad norm: 0.509951114654541\n",
            " - layer4_input.bias grad norm: 0.0005997302359901369\n",
            " - layer5.weight grad norm: 0.013394996523857117\n",
            " - layer5.bias grad norm: 0.0055186012759804726\n",
            "Gradients at iteration 469:\n",
            " - layer1.weight grad norm: 0.5191790461540222\n",
            " - layer1.bias grad norm: 0.0006064593326300383\n",
            " - layer2.weight grad norm: 0.001829823013395071\n",
            " - layer2.bias grad norm: 0.0005951758939772844\n",
            " - layer2_input.weight grad norm: 0.5096065402030945\n",
            " - layer2_input.bias grad norm: 0.0005951758939772844\n",
            " - layer3.weight grad norm: 0.0025573743041604757\n",
            " - layer3.bias grad norm: 0.0005894157220609486\n",
            " - layer3_input.weight grad norm: 0.5006628632545471\n",
            " - layer3_input.bias grad norm: 0.0005894157220609486\n",
            " - layer4.weight grad norm: 0.0030148483347147703\n",
            " - layer4.bias grad norm: 0.000545191578567028\n",
            " - layer4_input.weight grad norm: 0.4689484238624573\n",
            " - layer4_input.bias grad norm: 0.000545191578567028\n",
            " - layer5.weight grad norm: 0.011364738456904888\n",
            " - layer5.bias grad norm: 0.005262437276542187\n",
            "Gradients at iteration 470:\n",
            " - layer1.weight grad norm: 0.5191711783409119\n",
            " - layer1.bias grad norm: 0.0005966308526694775\n",
            " - layer2.weight grad norm: 0.001949662109836936\n",
            " - layer2.bias grad norm: 0.0006414827657863498\n",
            " - layer2_input.weight grad norm: 0.5474087595939636\n",
            " - layer2_input.bias grad norm: 0.0006414827657863498\n",
            " - layer3.weight grad norm: 0.002698237309232354\n",
            " - layer3.bias grad norm: 0.0005419496446847916\n",
            " - layer3_input.weight grad norm: 0.4705871641635895\n",
            " - layer3_input.bias grad norm: 0.0005419496446847916\n",
            " - layer4.weight grad norm: 0.003182480577379465\n",
            " - layer4.bias grad norm: 0.000525248353369534\n",
            " - layer4_input.weight grad norm: 0.45731887221336365\n",
            " - layer4_input.bias grad norm: 0.000525248353369534\n",
            " - layer5.weight grad norm: 0.012565933167934418\n",
            " - layer5.bias grad norm: 0.005534864962100983\n",
            "Gradients at iteration 471:\n",
            " - layer1.weight grad norm: 0.5029414296150208\n",
            " - layer1.bias grad norm: 0.000578239792957902\n",
            " - layer2.weight grad norm: 0.001885993406176567\n",
            " - layer2.bias grad norm: 0.0006172812427394092\n",
            " - layer2_input.weight grad norm: 0.5264766216278076\n",
            " - layer2_input.bias grad norm: 0.0006172812427394092\n",
            " - layer3.weight grad norm: 0.0025687748566269875\n",
            " - layer3.bias grad norm: 0.000576790829654783\n",
            " - layer3_input.weight grad norm: 0.49541234970092773\n",
            " - layer3_input.bias grad norm: 0.000576790829654783\n",
            " - layer4.weight grad norm: 0.0030572176910936832\n",
            " - layer4.bias grad norm: 0.0005473639466799796\n",
            " - layer4_input.weight grad norm: 0.4735303223133087\n",
            " - layer4_input.bias grad norm: 0.0005473639466799796\n",
            " - layer5.weight grad norm: 0.01256021298468113\n",
            " - layer5.bias grad norm: 0.005307971965521574\n",
            "Gradients at iteration 472:\n",
            " - layer1.weight grad norm: 0.5186976790428162\n",
            " - layer1.bias grad norm: 0.0006000801804475486\n",
            " - layer2.weight grad norm: 0.0018060322618111968\n",
            " - layer2.bias grad norm: 0.0005972942453809083\n",
            " - layer2_input.weight grad norm: 0.5147508382797241\n",
            " - layer2_input.bias grad norm: 0.0005972942453809083\n",
            " - layer3.weight grad norm: 0.002489991718903184\n",
            " - layer3.bias grad norm: 0.0006114039570093155\n",
            " - layer3_input.weight grad norm: 0.5187488794326782\n",
            " - layer3_input.bias grad norm: 0.0006114039570093155\n",
            " - layer4.weight grad norm: 0.0030065884348005056\n",
            " - layer4.bias grad norm: 0.0005092386272735894\n",
            " - layer4_input.weight grad norm: 0.4434884190559387\n",
            " - layer4_input.bias grad norm: 0.0005092386272735894\n",
            " - layer5.weight grad norm: 0.012421425431966782\n",
            " - layer5.bias grad norm: 0.005173043813556433\n",
            "Gradients at iteration 473:\n",
            " - layer1.weight grad norm: 0.5246946811676025\n",
            " - layer1.bias grad norm: 0.0006113756680861115\n",
            " - layer2.weight grad norm: 0.0018639322370290756\n",
            " - layer2.bias grad norm: 0.0005957817193120718\n",
            " - layer2_input.weight grad norm: 0.5137512683868408\n",
            " - layer2_input.bias grad norm: 0.0005957817193120718\n",
            " - layer3.weight grad norm: 0.0025957636535167694\n",
            " - layer3.bias grad norm: 0.0006006248877383769\n",
            " - layer3_input.weight grad norm: 0.5137161612510681\n",
            " - layer3_input.bias grad norm: 0.0006006248877383769\n",
            " - layer4.weight grad norm: 0.003058670787140727\n",
            " - layer4.bias grad norm: 0.0005037992377765477\n",
            " - layer4_input.weight grad norm: 0.443464994430542\n",
            " - layer4_input.bias grad norm: 0.0005037992377765477\n",
            " - layer5.weight grad norm: 0.011812534183263779\n",
            " - layer5.bias grad norm: 0.005324495956301689\n",
            "Gradients at iteration 474:\n",
            " - layer1.weight grad norm: 0.5276272892951965\n",
            " - layer1.bias grad norm: 0.0006150317494757473\n",
            " - layer2.weight grad norm: 0.0018182893982157111\n",
            " - layer2.bias grad norm: 0.0005344355595298111\n",
            " - layer2_input.weight grad norm: 0.4687318205833435\n",
            " - layer2_input.bias grad norm: 0.0005344355595298111\n",
            " - layer3.weight grad norm: 0.0025036544539034367\n",
            " - layer3.bias grad norm: 0.0006178052863106132\n",
            " - layer3_input.weight grad norm: 0.5310598015785217\n",
            " - layer3_input.bias grad norm: 0.0006178052863106132\n",
            " - layer4.weight grad norm: 0.0029571501072496176\n",
            " - layer4.bias grad norm: 0.0005398894427344203\n",
            " - layer4_input.weight grad norm: 0.4687134921550751\n",
            " - layer4_input.bias grad norm: 0.0005398894427344203\n",
            " - layer5.weight grad norm: 0.011655266396701336\n",
            " - layer5.bias grad norm: 0.00515546090900898\n",
            "Gradients at iteration 475:\n",
            " - layer1.weight grad norm: 0.5356652140617371\n",
            " - layer1.bias grad norm: 0.0006239184876903892\n",
            " - layer2.weight grad norm: 0.0018701134249567986\n",
            " - layer2.bias grad norm: 0.000565359543543309\n",
            " - layer2_input.weight grad norm: 0.4889030456542969\n",
            " - layer2_input.bias grad norm: 0.000565359543543309\n",
            " - layer3.weight grad norm: 0.002566114766523242\n",
            " - layer3.bias grad norm: 0.0006177753093652427\n",
            " - layer3_input.weight grad norm: 0.5291837453842163\n",
            " - layer3_input.bias grad norm: 0.0006177753093652427\n",
            " - layer4.weight grad norm: 0.0030169980600476265\n",
            " - layer4.bias grad norm: 0.000503264949657023\n",
            " - layer4_input.weight grad norm: 0.44022494554519653\n",
            " - layer4_input.bias grad norm: 0.000503264949657023\n",
            " - layer5.weight grad norm: 0.01241152174770832\n",
            " - layer5.bias grad norm: 0.0052728853188455105\n",
            "Gradients at iteration 476:\n",
            " - layer1.weight grad norm: 0.5058645009994507\n",
            " - layer1.bias grad norm: 0.0005816626362502575\n",
            " - layer2.weight grad norm: 0.0018140648026019335\n",
            " - layer2.bias grad norm: 0.0006125359795987606\n",
            " - layer2_input.weight grad norm: 0.5278803706169128\n",
            " - layer2_input.bias grad norm: 0.0006125359795987606\n",
            " - layer3.weight grad norm: 0.002510023070499301\n",
            " - layer3.bias grad norm: 0.0005460486863739789\n",
            " - layer3_input.weight grad norm: 0.47822996973991394\n",
            " - layer3_input.bias grad norm: 0.0005460486863739789\n",
            " - layer4.weight grad norm: 0.0029749060049653053\n",
            " - layer4.bias grad norm: 0.000559201231226325\n",
            " - layer4_input.weight grad norm: 0.4863581657409668\n",
            " - layer4_input.bias grad norm: 0.000559201231226325\n",
            " - layer5.weight grad norm: 0.01214391179382801\n",
            " - layer5.bias grad norm: 0.005199035629630089\n",
            "Gradients at iteration 477:\n",
            " - layer1.weight grad norm: 0.5238921642303467\n",
            " - layer1.bias grad norm: 0.000610449118539691\n",
            " - layer2.weight grad norm: 0.0018631811253726482\n",
            " - layer2.bias grad norm: 0.0005427058786153793\n",
            " - layer2_input.weight grad norm: 0.4708423316478729\n",
            " - layer2_input.bias grad norm: 0.0005427058786153793\n",
            " - layer3.weight grad norm: 0.002583496505394578\n",
            " - layer3.bias grad norm: 0.0005961141432635486\n",
            " - layer3_input.weight grad norm: 0.5100157260894775\n",
            " - layer3_input.bias grad norm: 0.0005961141432635486\n",
            " - layer4.weight grad norm: 0.003117929445579648\n",
            " - layer4.bias grad norm: 0.0005786760593764484\n",
            " - layer4_input.weight grad norm: 0.49346521496772766\n",
            " - layer4_input.bias grad norm: 0.0005786760593764484\n",
            " - layer5.weight grad norm: 0.013035505078732967\n",
            " - layer5.bias grad norm: 0.00531630776822567\n",
            "Gradients at iteration 478:\n",
            " - layer1.weight grad norm: 0.4916435182094574\n",
            " - layer1.bias grad norm: 0.0005622346652671695\n",
            " - layer2.weight grad norm: 0.0018435671227052808\n",
            " - layer2.bias grad norm: 0.0005586072220467031\n",
            " - layer2_input.weight grad norm: 0.4873668849468231\n",
            " - layer2_input.bias grad norm: 0.0005586072220467031\n",
            " - layer3.weight grad norm: 0.0025571680162101984\n",
            " - layer3.bias grad norm: 0.0006318686064332724\n",
            " - layer3_input.weight grad norm: 0.5356630086898804\n",
            " - layer3_input.bias grad norm: 0.0006318686064332724\n",
            " - layer4.weight grad norm: 0.0030429293401539326\n",
            " - layer4.bias grad norm: 0.0005632221000269055\n",
            " - layer4_input.weight grad norm: 0.48336198925971985\n",
            " - layer4_input.bias grad norm: 0.0005632221000269055\n",
            " - layer5.weight grad norm: 0.011714267544448376\n",
            " - layer5.bias grad norm: 0.005254372488707304\n",
            "Gradients at iteration 479:\n",
            " - layer1.weight grad norm: 0.5458745956420898\n",
            " - layer1.bias grad norm: 0.0006387481698766351\n",
            " - layer2.weight grad norm: 0.001876289490610361\n",
            " - layer2.bias grad norm: 0.0005849241861142218\n",
            " - layer2_input.weight grad norm: 0.5041470527648926\n",
            " - layer2_input.bias grad norm: 0.0005849241861142218\n",
            " - layer3.weight grad norm: 0.002542701782658696\n",
            " - layer3.bias grad norm: 0.0005481243133544922\n",
            " - layer3_input.weight grad norm: 0.4758678376674652\n",
            " - layer3_input.bias grad norm: 0.0005481243133544922\n",
            " - layer4.weight grad norm: 0.0030306775588542223\n",
            " - layer4.bias grad norm: 0.0005411921883933246\n",
            " - layer4_input.weight grad norm: 0.4703518748283386\n",
            " - layer4_input.bias grad norm: 0.0005411921883933246\n",
            " - layer5.weight grad norm: 0.011231784708797932\n",
            " - layer5.bias grad norm: 0.005280952900648117\n",
            "Gradients at iteration 480:\n",
            " - layer1.weight grad norm: 0.5252964496612549\n",
            " - layer1.bias grad norm: 0.000609963433817029\n",
            " - layer2.weight grad norm: 0.0018097840948030353\n",
            " - layer2.bias grad norm: 0.0005870574968867004\n",
            " - layer2_input.weight grad norm: 0.5064094662666321\n",
            " - layer2_input.bias grad norm: 0.0005870574968867004\n",
            " - layer3.weight grad norm: 0.0025136645417660475\n",
            " - layer3.bias grad norm: 0.0005838724318891764\n",
            " - layer3_input.weight grad norm: 0.5031418204307556\n",
            " - layer3_input.bias grad norm: 0.0005838724318891764\n",
            " - layer4.weight grad norm: 0.0029796387534588575\n",
            " - layer4.bias grad norm: 0.0005346560501493514\n",
            " - layer4_input.weight grad norm: 0.46289387345314026\n",
            " - layer4_input.bias grad norm: 0.0005346560501493514\n",
            " - layer5.weight grad norm: 0.011989460326731205\n",
            " - layer5.bias grad norm: 0.005110242869704962\n",
            "Gradients at iteration 481:\n",
            " - layer1.weight grad norm: 0.5485450625419617\n",
            " - layer1.bias grad norm: 0.0006474006222561002\n",
            " - layer2.weight grad norm: 0.0017234069528058171\n",
            " - layer2.bias grad norm: 0.0006126417429186404\n",
            " - layer2_input.weight grad norm: 0.5162080526351929\n",
            " - layer2_input.bias grad norm: 0.0006126417429186404\n",
            " - layer3.weight grad norm: 0.0023932931944727898\n",
            " - layer3.bias grad norm: 0.0005737246247008443\n",
            " - layer3_input.weight grad norm: 0.49000683426856995\n",
            " - layer3_input.bias grad norm: 0.0005737246247008443\n",
            " - layer4.weight grad norm: 0.00286532542668283\n",
            " - layer4.bias grad norm: 0.0005078570684418082\n",
            " - layer4_input.weight grad norm: 0.4385971426963806\n",
            " - layer4_input.bias grad norm: 0.0005078570684418082\n",
            " - layer5.weight grad norm: 0.01046733371913433\n",
            " - layer5.bias grad norm: 0.004968095570802689\n",
            "Gradients at iteration 482:\n",
            " - layer1.weight grad norm: 0.5107951164245605\n",
            " - layer1.bias grad norm: 0.0005886870785616338\n",
            " - layer2.weight grad norm: 0.0018856910755857825\n",
            " - layer2.bias grad norm: 0.0005915851215831935\n",
            " - layer2_input.weight grad norm: 0.5126951932907104\n",
            " - layer2_input.bias grad norm: 0.0005915851215831935\n",
            " - layer3.weight grad norm: 0.002615862525999546\n",
            " - layer3.bias grad norm: 0.0005797150079160929\n",
            " - layer3_input.weight grad norm: 0.5021786689758301\n",
            " - layer3_input.bias grad norm: 0.0005797150079160929\n",
            " - layer4.weight grad norm: 0.0030050543136894703\n",
            " - layer4.bias grad norm: 0.0005432667094282806\n",
            " - layer4_input.weight grad norm: 0.4731450378894806\n",
            " - layer4_input.bias grad norm: 0.0005432667094282806\n",
            " - layer5.weight grad norm: 0.011503910645842552\n",
            " - layer5.bias grad norm: 0.005328003317117691\n",
            "Gradients at iteration 483:\n",
            " - layer1.weight grad norm: 0.4926571547985077\n",
            " - layer1.bias grad norm: 0.0005593393580056727\n",
            " - layer2.weight grad norm: 0.0018798761302605271\n",
            " - layer2.bias grad norm: 0.0005727232783101499\n",
            " - layer2_input.weight grad norm: 0.5012909770011902\n",
            " - layer2_input.bias grad norm: 0.0005727232783101499\n",
            " - layer3.weight grad norm: 0.0026027788408100605\n",
            " - layer3.bias grad norm: 0.0005721524939872324\n",
            " - layer3_input.weight grad norm: 0.4970463514328003\n",
            " - layer3_input.bias grad norm: 0.0005721524939872324\n",
            " - layer4.weight grad norm: 0.0031181147787719965\n",
            " - layer4.bias grad norm: 0.0005903182318434119\n",
            " - layer4_input.weight grad norm: 0.5086718201637268\n",
            " - layer4_input.bias grad norm: 0.0005903182318434119\n",
            " - layer5.weight grad norm: 0.011943051591515541\n",
            " - layer5.bias grad norm: 0.005412857513874769\n",
            "Gradients at iteration 484:\n",
            " - layer1.weight grad norm: 0.52109295129776\n",
            " - layer1.bias grad norm: 0.0005953513900749385\n",
            " - layer2.weight grad norm: 0.0018454637611284852\n",
            " - layer2.bias grad norm: 0.0005521303392015398\n",
            " - layer2_input.weight grad norm: 0.48428013920783997\n",
            " - layer2_input.bias grad norm: 0.0005521303392015398\n",
            " - layer3.weight grad norm: 0.0025410070084035397\n",
            " - layer3.bias grad norm: 0.0005744547815993428\n",
            " - layer3_input.weight grad norm: 0.49795013666152954\n",
            " - layer3_input.bias grad norm: 0.0005744547815993428\n",
            " - layer4.weight grad norm: 0.003004478756338358\n",
            " - layer4.bias grad norm: 0.0005754389567300677\n",
            " - layer4_input.weight grad norm: 0.49576887488365173\n",
            " - layer4_input.bias grad norm: 0.0005754389567300677\n",
            " - layer5.weight grad norm: 0.012059283442795277\n",
            " - layer5.bias grad norm: 0.005210438277572393\n",
            "Gradients at iteration 485:\n",
            " - layer1.weight grad norm: 0.4790697395801544\n",
            " - layer1.bias grad norm: 0.0005497881793417037\n",
            " - layer2.weight grad norm: 0.0018856077222153544\n",
            " - layer2.bias grad norm: 0.0005858736694790423\n",
            " - layer2_input.weight grad norm: 0.5051150321960449\n",
            " - layer2_input.bias grad norm: 0.0005858736694790423\n",
            " - layer3.weight grad norm: 0.0025517556350678205\n",
            " - layer3.bias grad norm: 0.0005873442278243601\n",
            " - layer3_input.weight grad norm: 0.5036949515342712\n",
            " - layer3_input.bias grad norm: 0.0005873442278243601\n",
            " - layer4.weight grad norm: 0.003081604838371277\n",
            " - layer4.bias grad norm: 0.00060411257436499\n",
            " - layer4_input.weight grad norm: 0.5113323330879211\n",
            " - layer4_input.bias grad norm: 0.00060411257436499\n",
            " - layer5.weight grad norm: 0.011461891233921051\n",
            " - layer5.bias grad norm: 0.005314848385751247\n",
            "Gradients at iteration 486:\n",
            " - layer1.weight grad norm: 0.533508837223053\n",
            " - layer1.bias grad norm: 0.0006197015754878521\n",
            " - layer2.weight grad norm: 0.0018892339430749416\n",
            " - layer2.bias grad norm: 0.0005618255818262696\n",
            " - layer2_input.weight grad norm: 0.4936366677284241\n",
            " - layer2_input.bias grad norm: 0.0005618255818262696\n",
            " - layer3.weight grad norm: 0.002548151183873415\n",
            " - layer3.bias grad norm: 0.0005973336519673467\n",
            " - layer3_input.weight grad norm: 0.5129947662353516\n",
            " - layer3_input.bias grad norm: 0.0005973336519673467\n",
            " - layer4.weight grad norm: 0.0030661970376968384\n",
            " - layer4.bias grad norm: 0.0005214645643718541\n",
            " - layer4_input.weight grad norm: 0.4564583897590637\n",
            " - layer4_input.bias grad norm: 0.0005214645643718541\n",
            " - layer5.weight grad norm: 0.011132796294987202\n",
            " - layer5.bias grad norm: 0.005257073324173689\n",
            "Gradients at iteration 487:\n",
            " - layer1.weight grad norm: 0.5180578827857971\n",
            " - layer1.bias grad norm: 0.0006023086607456207\n",
            " - layer2.weight grad norm: 0.0017985178856179118\n",
            " - layer2.bias grad norm: 0.0005535717937164009\n",
            " - layer2_input.weight grad norm: 0.48269200325012207\n",
            " - layer2_input.bias grad norm: 0.0005535717937164009\n",
            " - layer3.weight grad norm: 0.0024835653603076935\n",
            " - layer3.bias grad norm: 0.0006228401907719672\n",
            " - layer3_input.weight grad norm: 0.532988429069519\n",
            " - layer3_input.bias grad norm: 0.0006228401907719672\n",
            " - layer4.weight grad norm: 0.0029418496415019035\n",
            " - layer4.bias grad norm: 0.0005355763714760542\n",
            " - layer4_input.weight grad norm: 0.4629957377910614\n",
            " - layer4_input.bias grad norm: 0.0005355763714760542\n",
            " - layer5.weight grad norm: 0.011661171913146973\n",
            " - layer5.bias grad norm: 0.005121943075209856\n",
            "Gradients at iteration 488:\n",
            " - layer1.weight grad norm: 0.5116112232208252\n",
            " - layer1.bias grad norm: 0.0005931485793553293\n",
            " - layer2.weight grad norm: 0.0018367087468504906\n",
            " - layer2.bias grad norm: 0.0005709060933440924\n",
            " - layer2_input.weight grad norm: 0.4917237162590027\n",
            " - layer2_input.bias grad norm: 0.0005709060933440924\n",
            " - layer3.weight grad norm: 0.0024900403805077076\n",
            " - layer3.bias grad norm: 0.000606250949203968\n",
            " - layer3_input.weight grad norm: 0.5158773064613342\n",
            " - layer3_input.bias grad norm: 0.000606250949203968\n",
            " - layer4.weight grad norm: 0.00295493146404624\n",
            " - layer4.bias grad norm: 0.0005595204420387745\n",
            " - layer4_input.weight grad norm: 0.4797368049621582\n",
            " - layer4_input.bias grad norm: 0.0005595204420387745\n",
            " - layer5.weight grad norm: 0.01171599980443716\n",
            " - layer5.bias grad norm: 0.005202597938477993\n",
            "Gradients at iteration 489:\n",
            " - layer1.weight grad norm: 0.5240872502326965\n",
            " - layer1.bias grad norm: 0.0006135177682153881\n",
            " - layer2.weight grad norm: 0.0017971895867958665\n",
            " - layer2.bias grad norm: 0.0006078848382458091\n",
            " - layer2_input.weight grad norm: 0.5160261392593384\n",
            " - layer2_input.bias grad norm: 0.0006078848382458091\n",
            " - layer3.weight grad norm: 0.002504665870219469\n",
            " - layer3.bias grad norm: 0.0005766941467300057\n",
            " - layer3_input.weight grad norm: 0.491670697927475\n",
            " - layer3_input.bias grad norm: 0.0005766941467300057\n",
            " - layer4.weight grad norm: 0.0029399110935628414\n",
            " - layer4.bias grad norm: 0.0005455289501696825\n",
            " - layer4_input.weight grad norm: 0.46599704027175903\n",
            " - layer4_input.bias grad norm: 0.0005455289501696825\n",
            " - layer5.weight grad norm: 0.010474713519215584\n",
            " - layer5.bias grad norm: 0.005095124244689941\n",
            "Gradients at iteration 490:\n",
            " - layer1.weight grad norm: 0.5504037141799927\n",
            " - layer1.bias grad norm: 0.0006412474322132766\n",
            " - layer2.weight grad norm: 0.0018610740080475807\n",
            " - layer2.bias grad norm: 0.0005725392256863415\n",
            " - layer2_input.weight grad norm: 0.4961596429347992\n",
            " - layer2_input.bias grad norm: 0.0005725392256863415\n",
            " - layer3.weight grad norm: 0.0025340665597468615\n",
            " - layer3.bias grad norm: 0.00056139484513551\n",
            " - layer3_input.weight grad norm: 0.48742416501045227\n",
            " - layer3_input.bias grad norm: 0.00056139484513551\n",
            " - layer4.weight grad norm: 0.003032959997653961\n",
            " - layer4.bias grad norm: 0.0005312736611813307\n",
            " - layer4_input.weight grad norm: 0.46164941787719727\n",
            " - layer4_input.bias grad norm: 0.0005312736611813307\n",
            " - layer5.weight grad norm: 0.011398190632462502\n",
            " - layer5.bias grad norm: 0.005254365969449282\n",
            "Gradients at iteration 491:\n",
            " - layer1.weight grad norm: 0.5209293961524963\n",
            " - layer1.bias grad norm: 0.0006081401370465755\n",
            " - layer2.weight grad norm: 0.001770700211636722\n",
            " - layer2.bias grad norm: 0.0006155567825771868\n",
            " - layer2_input.weight grad norm: 0.5239109396934509\n",
            " - layer2_input.bias grad norm: 0.0006155567825771868\n",
            " - layer3.weight grad norm: 0.002515223575755954\n",
            " - layer3.bias grad norm: 0.0005567391635850072\n",
            " - layer3_input.weight grad norm: 0.47726696729660034\n",
            " - layer3_input.bias grad norm: 0.0005567391635850072\n",
            " - layer4.weight grad norm: 0.0029715283308178186\n",
            " - layer4.bias grad norm: 0.0005538276745937765\n",
            " - layer4_input.weight grad norm: 0.4755586087703705\n",
            " - layer4_input.bias grad norm: 0.0005538276745937765\n",
            " - layer5.weight grad norm: 0.012793868780136108\n",
            " - layer5.bias grad norm: 0.0050813257694244385\n",
            "Gradients at iteration 492:\n",
            " - layer1.weight grad norm: 0.5186614394187927\n",
            " - layer1.bias grad norm: 0.0006094902055338025\n",
            " - layer2.weight grad norm: 0.0017411451553925872\n",
            " - layer2.bias grad norm: 0.0006063972832635045\n",
            " - layer2_input.weight grad norm: 0.5119249224662781\n",
            " - layer2_input.bias grad norm: 0.0006063972832635045\n",
            " - layer3.weight grad norm: 0.0024038171395659447\n",
            " - layer3.bias grad norm: 0.000598779646679759\n",
            " - layer3_input.weight grad norm: 0.5031585693359375\n",
            " - layer3_input.bias grad norm: 0.000598779646679759\n",
            " - layer4.weight grad norm: 0.0028600096702575684\n",
            " - layer4.bias grad norm: 0.0005487941671162844\n",
            " - layer4_input.weight grad norm: 0.46431371569633484\n",
            " - layer4_input.bias grad norm: 0.0005487941671162844\n",
            " - layer5.weight grad norm: 0.01111962553113699\n",
            " - layer5.bias grad norm: 0.004947347566485405\n",
            "Gradients at iteration 493:\n",
            " - layer1.weight grad norm: 0.5095019936561584\n",
            " - layer1.bias grad norm: 0.0005927912425249815\n",
            " - layer2.weight grad norm: 0.0019871897529810667\n",
            " - layer2.bias grad norm: 0.0005944111035205424\n",
            " - layer2_input.weight grad norm: 0.5098606944084167\n",
            " - layer2_input.bias grad norm: 0.0005944111035205424\n",
            " - layer3.weight grad norm: 0.0027125836350023746\n",
            " - layer3.bias grad norm: 0.0005383729585446417\n",
            " - layer3_input.weight grad norm: 0.4667739272117615\n",
            " - layer3_input.bias grad norm: 0.0005383729585446417\n",
            " - layer4.weight grad norm: 0.003214464755728841\n",
            " - layer4.bias grad norm: 0.0006013784441165626\n",
            " - layer4_input.weight grad norm: 0.5122207403182983\n",
            " - layer4_input.bias grad norm: 0.0006013784441165626\n",
            " - layer5.weight grad norm: 0.012119860388338566\n",
            " - layer5.bias grad norm: 0.0055555966682732105\n",
            "Gradients at iteration 494:\n",
            " - layer1.weight grad norm: 0.5330939292907715\n",
            " - layer1.bias grad norm: 0.0006247048731893301\n",
            " - layer2.weight grad norm: 0.001802570652216673\n",
            " - layer2.bias grad norm: 0.0005704504437744617\n",
            " - layer2_input.weight grad norm: 0.492958128452301\n",
            " - layer2_input.bias grad norm: 0.0005704504437744617\n",
            " - layer3.weight grad norm: 0.00250939279794693\n",
            " - layer3.bias grad norm: 0.0005599094438366592\n",
            " - layer3_input.weight grad norm: 0.48070016503334045\n",
            " - layer3_input.bias grad norm: 0.0005599094438366592\n",
            " - layer4.weight grad norm: 0.0029474194161593914\n",
            " - layer4.bias grad norm: 0.0005725049413740635\n",
            " - layer4_input.weight grad norm: 0.4914915859699249\n",
            " - layer4_input.bias grad norm: 0.0005725049413740635\n",
            " - layer5.weight grad norm: 0.010937601327896118\n",
            " - layer5.bias grad norm: 0.005142455454915762\n",
            "Gradients at iteration 495:\n",
            " - layer1.weight grad norm: 0.48217740654945374\n",
            " - layer1.bias grad norm: 0.0005435629282146692\n",
            " - layer2.weight grad norm: 0.001891636522486806\n",
            " - layer2.bias grad norm: 0.0005658679292537272\n",
            " - layer2_input.weight grad norm: 0.4996993839740753\n",
            " - layer2_input.bias grad norm: 0.0005658679292537272\n",
            " - layer3.weight grad norm: 0.002574065001681447\n",
            " - layer3.bias grad norm: 0.0006056171841919422\n",
            " - layer3_input.weight grad norm: 0.5224736332893372\n",
            " - layer3_input.bias grad norm: 0.0006056171841919422\n",
            " - layer4.weight grad norm: 0.003122941358014941\n",
            " - layer4.bias grad norm: 0.000570557895116508\n",
            " - layer4_input.weight grad norm: 0.4945913553237915\n",
            " - layer4_input.bias grad norm: 0.000570557895116508\n",
            " - layer5.weight grad norm: 0.012448707595467567\n",
            " - layer5.bias grad norm: 0.0053656199015676975\n",
            "Gradients at iteration 496:\n",
            " - layer1.weight grad norm: 0.501438558101654\n",
            " - layer1.bias grad norm: 0.0005805682158097625\n",
            " - layer2.weight grad norm: 0.001933372812345624\n",
            " - layer2.bias grad norm: 0.0006224242970347404\n",
            " - layer2_input.weight grad norm: 0.535064697265625\n",
            " - layer2_input.bias grad norm: 0.0006224242970347404\n",
            " - layer3.weight grad norm: 0.002639713464304805\n",
            " - layer3.bias grad norm: 0.0005600657314062119\n",
            " - layer3_input.weight grad norm: 0.4808875620365143\n",
            " - layer3_input.bias grad norm: 0.0005600657314062119\n",
            " - layer4.weight grad norm: 0.00316589861176908\n",
            " - layer4.bias grad norm: 0.000558461353648454\n",
            " - layer4_input.weight grad norm: 0.480403333902359\n",
            " - layer4_input.bias grad norm: 0.000558461353648454\n",
            " - layer5.weight grad norm: 0.013124515302479267\n",
            " - layer5.bias grad norm: 0.005440004635602236\n",
            "Gradients at iteration 497:\n",
            " - layer1.weight grad norm: 0.5011617541313171\n",
            " - layer1.bias grad norm: 0.0005727886455133557\n",
            " - layer2.weight grad norm: 0.0018877891125157475\n",
            " - layer2.bias grad norm: 0.000593148753978312\n",
            " - layer2_input.weight grad norm: 0.5105940699577332\n",
            " - layer2_input.bias grad norm: 0.000593148753978312\n",
            " - layer3.weight grad norm: 0.002581876004114747\n",
            " - layer3.bias grad norm: 0.0005654218839481473\n",
            " - layer3_input.weight grad norm: 0.4894540011882782\n",
            " - layer3_input.bias grad norm: 0.0005654218839481473\n",
            " - layer4.weight grad norm: 0.0030936086550354958\n",
            " - layer4.bias grad norm: 0.0005785259418189526\n",
            " - layer4_input.weight grad norm: 0.49836450815200806\n",
            " - layer4_input.bias grad norm: 0.0005785259418189526\n",
            " - layer5.weight grad norm: 0.012159066274762154\n",
            " - layer5.bias grad norm: 0.0053115044720470905\n",
            "Gradients at iteration 498:\n",
            " - layer1.weight grad norm: 0.5386654138565063\n",
            " - layer1.bias grad norm: 0.0006205812096595764\n",
            " - layer2.weight grad norm: 0.0019679937977343798\n",
            " - layer2.bias grad norm: 0.0005758200422860682\n",
            " - layer2_input.weight grad norm: 0.4980860948562622\n",
            " - layer2_input.bias grad norm: 0.0005758200422860682\n",
            " - layer3.weight grad norm: 0.0026904938276857138\n",
            " - layer3.bias grad norm: 0.0005583656602539122\n",
            " - layer3_input.weight grad norm: 0.48431116342544556\n",
            " - layer3_input.bias grad norm: 0.0005583656602539122\n",
            " - layer4.weight grad norm: 0.0031797641422599554\n",
            " - layer4.bias grad norm: 0.0005513248033821583\n",
            " - layer4_input.weight grad norm: 0.47642838954925537\n",
            " - layer4_input.bias grad norm: 0.0005513248033821583\n",
            " - layer5.weight grad norm: 0.012437326833605766\n",
            " - layer5.bias grad norm: 0.0055139269679784775\n",
            "Gradients at iteration 499:\n",
            " - layer1.weight grad norm: 0.5272631645202637\n",
            " - layer1.bias grad norm: 0.0006161092314869165\n",
            " - layer2.weight grad norm: 0.0018199989572167397\n",
            " - layer2.bias grad norm: 0.000589161179959774\n",
            " - layer2_input.weight grad norm: 0.5044456720352173\n",
            " - layer2_input.bias grad norm: 0.000589161179959774\n",
            " - layer3.weight grad norm: 0.002528316108509898\n",
            " - layer3.bias grad norm: 0.00056066969409585\n",
            " - layer3_input.weight grad norm: 0.48358798027038574\n",
            " - layer3_input.bias grad norm: 0.00056066969409585\n",
            " - layer4.weight grad norm: 0.0029666416812688112\n",
            " - layer4.bias grad norm: 0.0005629323422908783\n",
            " - layer4_input.weight grad norm: 0.48318278789520264\n",
            " - layer4_input.bias grad norm: 0.0005629323422908783\n",
            " - layer5.weight grad norm: 0.01256115548312664\n",
            " - layer5.bias grad norm: 0.0051639690063893795\n",
            "Gradients at iteration 500:\n",
            " - layer1.weight grad norm: 0.5028054714202881\n",
            " - layer1.bias grad norm: 0.0005806364351883531\n",
            " - layer2.weight grad norm: 0.0019103118684142828\n",
            " - layer2.bias grad norm: 0.0006413756054826081\n",
            " - layer2_input.weight grad norm: 0.544407844543457\n",
            " - layer2_input.bias grad norm: 0.0006413756054826081\n",
            " - layer3.weight grad norm: 0.0025962672661989927\n",
            " - layer3.bias grad norm: 0.0006239732028916478\n",
            " - layer3_input.weight grad norm: 0.5264226794242859\n",
            " - layer3_input.bias grad norm: 0.0006239732028916478\n",
            " - layer4.weight grad norm: 0.0030871378257870674\n",
            " - layer4.bias grad norm: 0.0004729654174298048\n",
            " - layer4_input.weight grad norm: 0.4165096879005432\n",
            " - layer4_input.bias grad norm: 0.0004729654174298048\n",
            " - layer5.weight grad norm: 0.012446563690900803\n",
            " - layer5.bias grad norm: 0.005328369326889515\n",
            "It: 500, Loss: 5.234e+13, Y0: 0.815, Time: 1.59, Learning Rate: 1.000e-03\n",
            "Gradients at iteration 501:\n",
            " - layer1.weight grad norm: 0.5352947115898132\n",
            " - layer1.bias grad norm: 0.0006290606688708067\n",
            " - layer2.weight grad norm: 0.001820095581933856\n",
            " - layer2.bias grad norm: 0.0005375347100198269\n",
            " - layer2_input.weight grad norm: 0.4657411277294159\n",
            " - layer2_input.bias grad norm: 0.0005375347100198269\n",
            " - layer3.weight grad norm: 0.002518182387575507\n",
            " - layer3.bias grad norm: 0.0006145155057311058\n",
            " - layer3_input.weight grad norm: 0.521041989326477\n",
            " - layer3_input.bias grad norm: 0.0006145155057311058\n",
            " - layer4.weight grad norm: 0.002969930414110422\n",
            " - layer4.bias grad norm: 0.0005547177279368043\n",
            " - layer4_input.weight grad norm: 0.4742092490196228\n",
            " - layer4_input.bias grad norm: 0.0005547177279368043\n",
            " - layer5.weight grad norm: 0.011743196286261082\n",
            " - layer5.bias grad norm: 0.005170698743313551\n",
            "Gradients at iteration 502:\n",
            " - layer1.weight grad norm: 0.49792394042015076\n",
            " - layer1.bias grad norm: 0.0005729617550969124\n",
            " - layer2.weight grad norm: 0.001883201184682548\n",
            " - layer2.bias grad norm: 0.0006055487901903689\n",
            " - layer2_input.weight grad norm: 0.5206361413002014\n",
            " - layer2_input.bias grad norm: 0.0006055487901903689\n",
            " - layer3.weight grad norm: 0.002600826555863023\n",
            " - layer3.bias grad norm: 0.0005506681045517325\n",
            " - layer3_input.weight grad norm: 0.47931089997291565\n",
            " - layer3_input.bias grad norm: 0.0005506681045517325\n",
            " - layer4.weight grad norm: 0.003103091148659587\n",
            " - layer4.bias grad norm: 0.0005862242542207241\n",
            " - layer4_input.weight grad norm: 0.5010858178138733\n",
            " - layer4_input.bias grad norm: 0.0005862242542207241\n",
            " - layer5.weight grad norm: 0.011539675295352936\n",
            " - layer5.bias grad norm: 0.005329379346221685\n",
            "Gradients at iteration 503:\n",
            " - layer1.weight grad norm: 0.526824414730072\n",
            " - layer1.bias grad norm: 0.0006123853963799775\n",
            " - layer2.weight grad norm: 0.0018697817577049136\n",
            " - layer2.bias grad norm: 0.0006206565885804594\n",
            " - layer2_input.weight grad norm: 0.5296361446380615\n",
            " - layer2_input.bias grad norm: 0.0006206565885804594\n",
            " - layer3.weight grad norm: 0.0026110559701919556\n",
            " - layer3.bias grad norm: 0.000577446655370295\n",
            " - layer3_input.weight grad norm: 0.4979051947593689\n",
            " - layer3_input.bias grad norm: 0.000577446655370295\n",
            " - layer4.weight grad norm: 0.0030788714066147804\n",
            " - layer4.bias grad norm: 0.0005040178075432777\n",
            " - layer4_input.weight grad norm: 0.4402535557746887\n",
            " - layer4_input.bias grad norm: 0.0005040178075432777\n",
            " - layer5.weight grad norm: 0.012602767907083035\n",
            " - layer5.bias grad norm: 0.005277629941701889\n",
            "Gradients at iteration 504:\n",
            " - layer1.weight grad norm: 0.47828900814056396\n",
            " - layer1.bias grad norm: 0.0005425793933682144\n",
            " - layer2.weight grad norm: 0.0018782485276460648\n",
            " - layer2.bias grad norm: 0.0006515789427794516\n",
            " - layer2_input.weight grad norm: 0.5574126839637756\n",
            " - layer2_input.bias grad norm: 0.0006515789427794516\n",
            " - layer3.weight grad norm: 0.0025929391849786043\n",
            " - layer3.bias grad norm: 0.0005852858303114772\n",
            " - layer3_input.weight grad norm: 0.5041574835777283\n",
            " - layer3_input.bias grad norm: 0.0005852858303114772\n",
            " - layer4.weight grad norm: 0.0030874963849782944\n",
            " - layer4.bias grad norm: 0.0005203946493566036\n",
            " - layer4_input.weight grad norm: 0.4540446996688843\n",
            " - layer4_input.bias grad norm: 0.0005203946493566036\n",
            " - layer5.weight grad norm: 0.012188775464892387\n",
            " - layer5.bias grad norm: 0.005362026393413544\n",
            "Gradients at iteration 505:\n",
            " - layer1.weight grad norm: 0.4826892614364624\n",
            " - layer1.bias grad norm: 0.0005470788455568254\n",
            " - layer2.weight grad norm: 0.0018676855834200978\n",
            " - layer2.bias grad norm: 0.0006144081708043814\n",
            " - layer2_input.weight grad norm: 0.5232299566268921\n",
            " - layer2_input.bias grad norm: 0.0006144081708043814\n",
            " - layer3.weight grad norm: 0.002546854317188263\n",
            " - layer3.bias grad norm: 0.0005968179902993143\n",
            " - layer3_input.weight grad norm: 0.5127108097076416\n",
            " - layer3_input.bias grad norm: 0.0005968179902993143\n",
            " - layer4.weight grad norm: 0.003014124697074294\n",
            " - layer4.bias grad norm: 0.0005583696183748543\n",
            " - layer4_input.weight grad norm: 0.4797533452510834\n",
            " - layer4_input.bias grad norm: 0.0005583696183748543\n",
            " - layer5.weight grad norm: 0.012527997605502605\n",
            " - layer5.bias grad norm: 0.005229203496128321\n",
            "Gradients at iteration 506:\n",
            " - layer1.weight grad norm: 0.542212963104248\n",
            " - layer1.bias grad norm: 0.0006308931042440236\n",
            " - layer2.weight grad norm: 0.0018592319684103131\n",
            " - layer2.bias grad norm: 0.0005812346353195608\n",
            " - layer2_input.weight grad norm: 0.5036400556564331\n",
            " - layer2_input.bias grad norm: 0.0005812346353195608\n",
            " - layer3.weight grad norm: 0.002579560037702322\n",
            " - layer3.bias grad norm: 0.0005631244275718927\n",
            " - layer3_input.weight grad norm: 0.48732635378837585\n",
            " - layer3_input.bias grad norm: 0.0005631244275718927\n",
            " - layer4.weight grad norm: 0.0030810877215117216\n",
            " - layer4.bias grad norm: 0.0005297975149005651\n",
            " - layer4_input.weight grad norm: 0.46330881118774414\n",
            " - layer4_input.bias grad norm: 0.0005297975149005651\n",
            " - layer5.weight grad norm: 0.012655255384743214\n",
            " - layer5.bias grad norm: 0.0052710347808897495\n",
            "Gradients at iteration 507:\n",
            " - layer1.weight grad norm: 0.5173793435096741\n",
            " - layer1.bias grad norm: 0.0005992250517010689\n",
            " - layer2.weight grad norm: 0.0018024295568466187\n",
            " - layer2.bias grad norm: 0.0005476687802001834\n",
            " - layer2_input.weight grad norm: 0.4774479269981384\n",
            " - layer2_input.bias grad norm: 0.0005476687802001834\n",
            " - layer3.weight grad norm: 0.0024858342949301004\n",
            " - layer3.bias grad norm: 0.0005918596289120615\n",
            " - layer3_input.weight grad norm: 0.5096853971481323\n",
            " - layer3_input.bias grad norm: 0.0005918596289120615\n",
            " - layer4.weight grad norm: 0.0029572201892733574\n",
            " - layer4.bias grad norm: 0.0005689173121936619\n",
            " - layer4_input.weight grad norm: 0.4943564832210541\n",
            " - layer4_input.bias grad norm: 0.0005689173121936619\n",
            " - layer5.weight grad norm: 0.012163919396698475\n",
            " - layer5.bias grad norm: 0.0051030865870416164\n",
            "Gradients at iteration 508:\n",
            " - layer1.weight grad norm: 0.5553733706474304\n",
            " - layer1.bias grad norm: 0.0006559797911904752\n",
            " - layer2.weight grad norm: 0.0018695786129683256\n",
            " - layer2.bias grad norm: 0.0005127076874487102\n",
            " - layer2_input.weight grad norm: 0.4521564841270447\n",
            " - layer2_input.bias grad norm: 0.0005127076874487102\n",
            " - layer3.weight grad norm: 0.0026114366482943296\n",
            " - layer3.bias grad norm: 0.0005862260586582124\n",
            " - layer3_input.weight grad norm: 0.5049193501472473\n",
            " - layer3_input.bias grad norm: 0.0005862260586582124\n",
            " - layer4.weight grad norm: 0.003038625232875347\n",
            " - layer4.bias grad norm: 0.0005588915664702654\n",
            " - layer4_input.weight grad norm: 0.48163673281669617\n",
            " - layer4_input.bias grad norm: 0.0005588915664702654\n",
            " - layer5.weight grad norm: 0.012146268971264362\n",
            " - layer5.bias grad norm: 0.005289582069963217\n",
            "Gradients at iteration 509:\n",
            " - layer1.weight grad norm: 0.5071384310722351\n",
            " - layer1.bias grad norm: 0.0005993170198053122\n",
            " - layer2.weight grad norm: 0.00184431963134557\n",
            " - layer2.bias grad norm: 0.000619316881056875\n",
            " - layer2_input.weight grad norm: 0.520932137966156\n",
            " - layer2_input.bias grad norm: 0.000619316881056875\n",
            " - layer3.weight grad norm: 0.0024971405509859324\n",
            " - layer3.bias grad norm: 0.0005784108652733266\n",
            " - layer3_input.weight grad norm: 0.49210381507873535\n",
            " - layer3_input.bias grad norm: 0.0005784108652733266\n",
            " - layer4.weight grad norm: 0.003000606084242463\n",
            " - layer4.bias grad norm: 0.0005669720703735948\n",
            " - layer4_input.weight grad norm: 0.47865167260169983\n",
            " - layer4_input.bias grad norm: 0.0005669720703735948\n",
            " - layer5.weight grad norm: 0.010902788490056992\n",
            " - layer5.bias grad norm: 0.0051655396819114685\n",
            "Gradients at iteration 510:\n",
            " - layer1.weight grad norm: 0.53819340467453\n",
            " - layer1.bias grad norm: 0.0006189516861923039\n",
            " - layer2.weight grad norm: 0.0018693833844736218\n",
            " - layer2.bias grad norm: 0.0005609381478279829\n",
            " - layer2_input.weight grad norm: 0.48957085609436035\n",
            " - layer2_input.bias grad norm: 0.0005609381478279829\n",
            " - layer3.weight grad norm: 0.0025313282385468483\n",
            " - layer3.bias grad norm: 0.0005302884383127093\n",
            " - layer3_input.weight grad norm: 0.4624544084072113\n",
            " - layer3_input.bias grad norm: 0.0005302884383127093\n",
            " - layer4.weight grad norm: 0.0030590035021305084\n",
            " - layer4.bias grad norm: 0.000587981368880719\n",
            " - layer4_input.weight grad norm: 0.5065568089485168\n",
            " - layer4_input.bias grad norm: 0.000587981368880719\n",
            " - layer5.weight grad norm: 0.012428229674696922\n",
            " - layer5.bias grad norm: 0.005315483082085848\n",
            "Gradients at iteration 511:\n",
            " - layer1.weight grad norm: 0.5062420964241028\n",
            " - layer1.bias grad norm: 0.0005920140538364649\n",
            " - layer2.weight grad norm: 0.001836028415709734\n",
            " - layer2.bias grad norm: 0.0006289301090873778\n",
            " - layer2_input.weight grad norm: 0.5298536419868469\n",
            " - layer2_input.bias grad norm: 0.0006289301090873778\n",
            " - layer3.weight grad norm: 0.0025355543475598097\n",
            " - layer3.bias grad norm: 0.0005085279117338359\n",
            " - layer3_input.weight grad norm: 0.44190776348114014\n",
            " - layer3_input.bias grad norm: 0.0005085279117338359\n",
            " - layer4.weight grad norm: 0.003004611236974597\n",
            " - layer4.bias grad norm: 0.0006122380727902055\n",
            " - layer4_input.weight grad norm: 0.5172087550163269\n",
            " - layer4_input.bias grad norm: 0.0006122380727902055\n",
            " - layer5.weight grad norm: 0.011778770945966244\n",
            " - layer5.bias grad norm: 0.005192359443753958\n",
            "Gradients at iteration 512:\n",
            " - layer1.weight grad norm: 0.5334619283676147\n",
            " - layer1.bias grad norm: 0.0006142313941381872\n",
            " - layer2.weight grad norm: 0.0018088624347001314\n",
            " - layer2.bias grad norm: 0.0006072308751754463\n",
            " - layer2_input.weight grad norm: 0.5226227045059204\n",
            " - layer2_input.bias grad norm: 0.0006072308751754463\n",
            " - layer3.weight grad norm: 0.0024721387308090925\n",
            " - layer3.bias grad norm: 0.000572853023186326\n",
            " - layer3_input.weight grad norm: 0.4986700415611267\n",
            " - layer3_input.bias grad norm: 0.000572853023186326\n",
            " - layer4.weight grad norm: 0.0029544327408075333\n",
            " - layer4.bias grad norm: 0.0004969356814399362\n",
            " - layer4_input.weight grad norm: 0.4398130774497986\n",
            " - layer4_input.bias grad norm: 0.0004969356814399362\n",
            " - layer5.weight grad norm: 0.011387578211724758\n",
            " - layer5.bias grad norm: 0.005144570022821426\n",
            "Gradients at iteration 513:\n",
            " - layer1.weight grad norm: 0.503919780254364\n",
            " - layer1.bias grad norm: 0.0005754544981755316\n",
            " - layer2.weight grad norm: 0.0018730253214016557\n",
            " - layer2.bias grad norm: 0.0006015321123413742\n",
            " - layer2_input.weight grad norm: 0.5185787081718445\n",
            " - layer2_input.bias grad norm: 0.0006015321123413742\n",
            " - layer3.weight grad norm: 0.002566817682236433\n",
            " - layer3.bias grad norm: 0.0005691742990165949\n",
            " - layer3_input.weight grad norm: 0.49200791120529175\n",
            " - layer3_input.bias grad norm: 0.0005691742990165949\n",
            " - layer4.weight grad norm: 0.003122547874227166\n",
            " - layer4.bias grad norm: 0.0005619353032670915\n",
            " - layer4_input.weight grad norm: 0.4846305549144745\n",
            " - layer4_input.bias grad norm: 0.0005619353032670915\n",
            " - layer5.weight grad norm: 0.012318055145442486\n",
            " - layer5.bias grad norm: 0.005341032519936562\n",
            "Gradients at iteration 514:\n",
            " - layer1.weight grad norm: 0.5017368197441101\n",
            " - layer1.bias grad norm: 0.0005762541550211608\n",
            " - layer2.weight grad norm: 0.001879277522675693\n",
            " - layer2.bias grad norm: 0.0005829731817357242\n",
            " - layer2_input.weight grad norm: 0.5046671628952026\n",
            " - layer2_input.bias grad norm: 0.0005829731817357242\n",
            " - layer3.weight grad norm: 0.0025478641036897898\n",
            " - layer3.bias grad norm: 0.000591145595535636\n",
            " - layer3_input.weight grad norm: 0.5057326555252075\n",
            " - layer3_input.bias grad norm: 0.000591145595535636\n",
            " - layer4.weight grad norm: 0.003022147808223963\n",
            " - layer4.bias grad norm: 0.0005673171253874898\n",
            " - layer4_input.weight grad norm: 0.4874648153781891\n",
            " - layer4_input.bias grad norm: 0.0005673171253874898\n",
            " - layer5.weight grad norm: 0.011574842035770416\n",
            " - layer5.bias grad norm: 0.005306329112499952\n",
            "Gradients at iteration 515:\n",
            " - layer1.weight grad norm: 0.5169447660446167\n",
            " - layer1.bias grad norm: 0.0005961785209365189\n",
            " - layer2.weight grad norm: 0.001929274876601994\n",
            " - layer2.bias grad norm: 0.0005845531704835594\n",
            " - layer2_input.weight grad norm: 0.5076541304588318\n",
            " - layer2_input.bias grad norm: 0.0005845531704835594\n",
            " - layer3.weight grad norm: 0.002663099905475974\n",
            " - layer3.bias grad norm: 0.0006302436231635511\n",
            " - layer3_input.weight grad norm: 0.5377547144889832\n",
            " - layer3_input.bias grad norm: 0.0006302436231635511\n",
            " - layer4.weight grad norm: 0.003158020554110408\n",
            " - layer4.bias grad norm: 0.0004908374976366758\n",
            " - layer4_input.weight grad norm: 0.4309185743331909\n",
            " - layer4_input.bias grad norm: 0.0004908374976366758\n",
            " - layer5.weight grad norm: 0.011480060406029224\n",
            " - layer5.bias grad norm: 0.00545516237616539\n",
            "Gradients at iteration 516:\n",
            " - layer1.weight grad norm: 0.5412783622741699\n",
            " - layer1.bias grad norm: 0.0006385939195752144\n",
            " - layer2.weight grad norm: 0.0018822893034666777\n",
            " - layer2.bias grad norm: 0.0005805616383440793\n",
            " - layer2_input.weight grad norm: 0.4991838335990906\n",
            " - layer2_input.bias grad norm: 0.0005805616383440793\n",
            " - layer3.weight grad norm: 0.0025878273881971836\n",
            " - layer3.bias grad norm: 0.000577127153519541\n",
            " - layer3_input.weight grad norm: 0.4929656386375427\n",
            " - layer3_input.bias grad norm: 0.000577127153519541\n",
            " - layer4.weight grad norm: 0.0030388452578336\n",
            " - layer4.bias grad norm: 0.0005392724415287375\n",
            " - layer4_input.weight grad norm: 0.4632813334465027\n",
            " - layer4_input.bias grad norm: 0.0005392724415287375\n",
            " - layer5.weight grad norm: 0.011774453334510326\n",
            " - layer5.bias grad norm: 0.0053016990423202515\n",
            "Gradients at iteration 517:\n",
            " - layer1.weight grad norm: 0.4805053174495697\n",
            " - layer1.bias grad norm: 0.0005468951421789825\n",
            " - layer2.weight grad norm: 0.0018775530625134706\n",
            " - layer2.bias grad norm: 0.0006079812301322818\n",
            " - layer2_input.weight grad norm: 0.5242671966552734\n",
            " - layer2_input.bias grad norm: 0.0006079812301322818\n",
            " - layer3.weight grad norm: 0.0025822154711931944\n",
            " - layer3.bias grad norm: 0.0005755448946729302\n",
            " - layer3_input.weight grad norm: 0.4955860674381256\n",
            " - layer3_input.bias grad norm: 0.0005755448946729302\n",
            " - layer4.weight grad norm: 0.0030560584273189306\n",
            " - layer4.bias grad norm: 0.0005850496818311512\n",
            " - layer4_input.weight grad norm: 0.4984481632709503\n",
            " - layer4_input.bias grad norm: 0.0005850496818311512\n",
            " - layer5.weight grad norm: 0.012364896014332771\n",
            " - layer5.bias grad norm: 0.005267021711915731\n",
            "Gradients at iteration 518:\n",
            " - layer1.weight grad norm: 0.5286818146705627\n",
            " - layer1.bias grad norm: 0.0006109033711254597\n",
            " - layer2.weight grad norm: 0.0018803472630679607\n",
            " - layer2.bias grad norm: 0.0006220407667569816\n",
            " - layer2_input.weight grad norm: 0.5329223275184631\n",
            " - layer2_input.bias grad norm: 0.0006220407667569816\n",
            " - layer3.weight grad norm: 0.002583883935585618\n",
            " - layer3.bias grad norm: 0.0005192781100049615\n",
            " - layer3_input.weight grad norm: 0.4581068158149719\n",
            " - layer3_input.bias grad norm: 0.0005192781100049615\n",
            " - layer4.weight grad norm: 0.003096919972449541\n",
            " - layer4.bias grad norm: 0.0005480386316776276\n",
            " - layer4_input.weight grad norm: 0.4758421778678894\n",
            " - layer4_input.bias grad norm: 0.0005480386316776276\n",
            " - layer5.weight grad norm: 0.012292317114770412\n",
            " - layer5.bias grad norm: 0.005345179233700037\n",
            "Gradients at iteration 519:\n",
            " - layer1.weight grad norm: 0.5466501712799072\n",
            " - layer1.bias grad norm: 0.0006514884880743921\n",
            " - layer2.weight grad norm: 0.001754883909597993\n",
            " - layer2.bias grad norm: 0.0005883476114831865\n",
            " - layer2_input.weight grad norm: 0.5005353689193726\n",
            " - layer2_input.bias grad norm: 0.0005883476114831865\n",
            " - layer3.weight grad norm: 0.0024567756336182356\n",
            " - layer3.bias grad norm: 0.0005731337587349117\n",
            " - layer3_input.weight grad norm: 0.4885953962802887\n",
            " - layer3_input.bias grad norm: 0.0005731337587349117\n",
            " - layer4.weight grad norm: 0.002921848092228174\n",
            " - layer4.bias grad norm: 0.0005383637035265565\n",
            " - layer4_input.weight grad norm: 0.46016520261764526\n",
            " - layer4_input.bias grad norm: 0.0005383637035265565\n",
            " - layer5.weight grad norm: 0.010714939795434475\n",
            " - layer5.bias grad norm: 0.005065269768238068\n",
            "Gradients at iteration 520:\n",
            " - layer1.weight grad norm: 0.5225832462310791\n",
            " - layer1.bias grad norm: 0.0006088356603868306\n",
            " - layer2.weight grad norm: 0.001862367382273078\n",
            " - layer2.bias grad norm: 0.0005944352014921606\n",
            " - layer2_input.weight grad norm: 0.5121909379959106\n",
            " - layer2_input.bias grad norm: 0.0005944352014921606\n",
            " - layer3.weight grad norm: 0.0026042924728244543\n",
            " - layer3.bias grad norm: 0.0005680113099515438\n",
            " - layer3_input.weight grad norm: 0.490657240152359\n",
            " - layer3_input.bias grad norm: 0.0005680113099515438\n",
            " - layer4.weight grad norm: 0.003031159285455942\n",
            " - layer4.bias grad norm: 0.0005507807363756001\n",
            " - layer4_input.weight grad norm: 0.4728807508945465\n",
            " - layer4_input.bias grad norm: 0.0005507807363756001\n",
            " - layer5.weight grad norm: 0.012513169087469578\n",
            " - layer5.bias grad norm: 0.00530638312920928\n",
            "Gradients at iteration 521:\n",
            " - layer1.weight grad norm: 0.5018800497055054\n",
            " - layer1.bias grad norm: 0.0005780910141766071\n",
            " - layer2.weight grad norm: 0.0018299641087651253\n",
            " - layer2.bias grad norm: 0.0005101177375763655\n",
            " - layer2_input.weight grad norm: 0.4523669183254242\n",
            " - layer2_input.bias grad norm: 0.0005101177375763655\n",
            " - layer3.weight grad norm: 0.0024902604054659605\n",
            " - layer3.bias grad norm: 0.000613236625213176\n",
            " - layer3_input.weight grad norm: 0.521304726600647\n",
            " - layer3_input.bias grad norm: 0.000613236625213176\n",
            " - layer4.weight grad norm: 0.002990357344970107\n",
            " - layer4.bias grad norm: 0.0006146196974441409\n",
            " - layer4_input.weight grad norm: 0.5210866928100586\n",
            " - layer4_input.bias grad norm: 0.0006146196974441409\n",
            " - layer5.weight grad norm: 0.011961406096816063\n",
            " - layer5.bias grad norm: 0.005181339103728533\n",
            "Gradients at iteration 522:\n",
            " - layer1.weight grad norm: 0.5308412313461304\n",
            " - layer1.bias grad norm: 0.0006287448923103511\n",
            " - layer2.weight grad norm: 0.0017912755720317364\n",
            " - layer2.bias grad norm: 0.0005418267101049423\n",
            " - layer2_input.weight grad norm: 0.4724504053592682\n",
            " - layer2_input.bias grad norm: 0.0005418267101049423\n",
            " - layer3.weight grad norm: 0.0024903849698603153\n",
            " - layer3.bias grad norm: 0.0005900164251215756\n",
            " - layer3_input.weight grad norm: 0.49998921155929565\n",
            " - layer3_input.bias grad norm: 0.0005900164251215756\n",
            " - layer4.weight grad norm: 0.002963693579658866\n",
            " - layer4.bias grad norm: 0.000580748834181577\n",
            " - layer4_input.weight grad norm: 0.4947926104068756\n",
            " - layer4_input.bias grad norm: 0.000580748834181577\n",
            " - layer5.weight grad norm: 0.011931302025914192\n",
            " - layer5.bias grad norm: 0.0051332334987819195\n",
            "Gradients at iteration 523:\n",
            " - layer1.weight grad norm: 0.531589925289154\n",
            " - layer1.bias grad norm: 0.0006166254170238972\n",
            " - layer2.weight grad norm: 0.001833199872635305\n",
            " - layer2.bias grad norm: 0.0006076663848944008\n",
            " - layer2_input.weight grad norm: 0.5212076902389526\n",
            " - layer2_input.bias grad norm: 0.0006076663848944008\n",
            " - layer3.weight grad norm: 0.0025318481493741274\n",
            " - layer3.bias grad norm: 0.0005647266516461968\n",
            " - layer3_input.weight grad norm: 0.4896232485771179\n",
            " - layer3_input.bias grad norm: 0.0005647266516461968\n",
            " - layer4.weight grad norm: 0.0030116550624370575\n",
            " - layer4.bias grad norm: 0.0005183928878977895\n",
            " - layer4_input.weight grad norm: 0.4537087082862854\n",
            " - layer4_input.bias grad norm: 0.0005183928878977895\n",
            " - layer5.weight grad norm: 0.011124365031719208\n",
            " - layer5.bias grad norm: 0.005247850902378559\n",
            "Gradients at iteration 524:\n",
            " - layer1.weight grad norm: 0.5064074397087097\n",
            " - layer1.bias grad norm: 0.0005766957765445113\n",
            " - layer2.weight grad norm: 0.001897113979794085\n",
            " - layer2.bias grad norm: 0.0006564042414538562\n",
            " - layer2_input.weight grad norm: 0.561684250831604\n",
            " - layer2_input.bias grad norm: 0.0006564042414538562\n",
            " - layer3.weight grad norm: 0.002630889415740967\n",
            " - layer3.bias grad norm: 0.0005441724788397551\n",
            " - layer3_input.weight grad norm: 0.4758816659450531\n",
            " - layer3_input.bias grad norm: 0.0005441724788397551\n",
            " - layer4.weight grad norm: 0.0030978545546531677\n",
            " - layer4.bias grad norm: 0.0005097670946270227\n",
            " - layer4_input.weight grad norm: 0.44879597425460815\n",
            " - layer4_input.bias grad norm: 0.0005097670946270227\n",
            " - layer5.weight grad norm: 0.011411750689148903\n",
            " - layer5.bias grad norm: 0.005346829537302256\n",
            "Gradients at iteration 525:\n",
            " - layer1.weight grad norm: 0.5135713219642639\n",
            " - layer1.bias grad norm: 0.0005934109212830663\n",
            " - layer2.weight grad norm: 0.0018875406822189689\n",
            " - layer2.bias grad norm: 0.0005457628285512328\n",
            " - layer2_input.weight grad norm: 0.480193167924881\n",
            " - layer2_input.bias grad norm: 0.0005457628285512328\n",
            " - layer3.weight grad norm: 0.0026046852581202984\n",
            " - layer3.bias grad norm: 0.0006241408409550786\n",
            " - layer3_input.weight grad norm: 0.5350822806358337\n",
            " - layer3_input.bias grad norm: 0.0006241408409550786\n",
            " - layer4.weight grad norm: 0.0030804872512817383\n",
            " - layer4.bias grad norm: 0.000534370425157249\n",
            " - layer4_input.weight grad norm: 0.46811553835868835\n",
            " - layer4_input.bias grad norm: 0.000534370425157249\n",
            " - layer5.weight grad norm: 0.012777406722307205\n",
            " - layer5.bias grad norm: 0.00534591032192111\n",
            "Gradients at iteration 526:\n",
            " - layer1.weight grad norm: 0.5290701389312744\n",
            " - layer1.bias grad norm: 0.0006204716046340764\n",
            " - layer2.weight grad norm: 0.001791406306438148\n",
            " - layer2.bias grad norm: 0.0006093813572078943\n",
            " - layer2_input.weight grad norm: 0.5211303234100342\n",
            " - layer2_input.bias grad norm: 0.0006093813572078943\n",
            " - layer3.weight grad norm: 0.0025045659858733416\n",
            " - layer3.bias grad norm: 0.0005552747170440853\n",
            " - layer3_input.weight grad norm: 0.47746512293815613\n",
            " - layer3_input.bias grad norm: 0.0005552747170440853\n",
            " - layer4.weight grad norm: 0.0029631059151142836\n",
            " - layer4.bias grad norm: 0.0005472287302836776\n",
            " - layer4_input.weight grad norm: 0.4694174528121948\n",
            " - layer4_input.bias grad norm: 0.0005472287302836776\n",
            " - layer5.weight grad norm: 0.011632291600108147\n",
            " - layer5.bias grad norm: 0.005146874580532312\n",
            "Gradients at iteration 527:\n",
            " - layer1.weight grad norm: 0.520037829875946\n",
            " - layer1.bias grad norm: 0.0005965300952084363\n",
            " - layer2.weight grad norm: 0.0018831524066627026\n",
            " - layer2.bias grad norm: 0.0005805189721286297\n",
            " - layer2_input.weight grad norm: 0.5047605633735657\n",
            " - layer2_input.bias grad norm: 0.0005805189721286297\n",
            " - layer3.weight grad norm: 0.0026004130486398935\n",
            " - layer3.bias grad norm: 0.0005772869917564094\n",
            " - layer3_input.weight grad norm: 0.4978487491607666\n",
            " - layer3_input.bias grad norm: 0.0005772869917564094\n",
            " - layer4.weight grad norm: 0.003099063877016306\n",
            " - layer4.bias grad norm: 0.0005504561704583466\n",
            " - layer4_input.weight grad norm: 0.47615131735801697\n",
            " - layer4_input.bias grad norm: 0.0005504561704583466\n",
            " - layer5.weight grad norm: 0.012380178086459637\n",
            " - layer5.bias grad norm: 0.005350662395358086\n",
            "Gradients at iteration 528:\n",
            " - layer1.weight grad norm: 0.5431622862815857\n",
            " - layer1.bias grad norm: 0.0006497596041299403\n",
            " - layer2.weight grad norm: 0.0017840908840298653\n",
            " - layer2.bias grad norm: 0.0006284432020038366\n",
            " - layer2_input.weight grad norm: 0.5271718502044678\n",
            " - layer2_input.bias grad norm: 0.0006284432020038366\n",
            " - layer3.weight grad norm: 0.0024998479057103395\n",
            " - layer3.bias grad norm: 0.000563579611480236\n",
            " - layer3_input.weight grad norm: 0.480955570936203\n",
            " - layer3_input.bias grad norm: 0.000563579611480236\n",
            " - layer4.weight grad norm: 0.002894616685807705\n",
            " - layer4.bias grad norm: 0.0005154332029633224\n",
            " - layer4_input.weight grad norm: 0.4422079622745514\n",
            " - layer4_input.bias grad norm: 0.0005154332029633224\n",
            " - layer5.weight grad norm: 0.012353060767054558\n",
            " - layer5.bias grad norm: 0.005078874994069338\n",
            "Gradients at iteration 529:\n",
            " - layer1.weight grad norm: 0.49889394640922546\n",
            " - layer1.bias grad norm: 0.00057490065228194\n",
            " - layer2.weight grad norm: 0.0018858619732782245\n",
            " - layer2.bias grad norm: 0.0006045132176950574\n",
            " - layer2_input.weight grad norm: 0.5206488966941833\n",
            " - layer2_input.bias grad norm: 0.0006045132176950574\n",
            " - layer3.weight grad norm: 0.0025892972480505705\n",
            " - layer3.bias grad norm: 0.0005818168865516782\n",
            " - layer3_input.weight grad norm: 0.4999207854270935\n",
            " - layer3_input.bias grad norm: 0.0005818168865516782\n",
            " - layer4.weight grad norm: 0.0031243148259818554\n",
            " - layer4.bias grad norm: 0.0005522249266505241\n",
            " - layer4_input.weight grad norm: 0.4795042872428894\n",
            " - layer4_input.bias grad norm: 0.0005522249266505241\n",
            " - layer5.weight grad norm: 0.011534491553902626\n",
            " - layer5.bias grad norm: 0.005380591377615929\n",
            "Gradients at iteration 530:\n",
            " - layer1.weight grad norm: 0.5041705965995789\n",
            " - layer1.bias grad norm: 0.0005846701678819954\n",
            " - layer2.weight grad norm: 0.0018643091898411512\n",
            " - layer2.bias grad norm: 0.0005829969886690378\n",
            " - layer2_input.weight grad norm: 0.4997386932373047\n",
            " - layer2_input.bias grad norm: 0.0005829969886690378\n",
            " - layer3.weight grad norm: 0.0025388142094016075\n",
            " - layer3.bias grad norm: 0.0005791634321212769\n",
            " - layer3_input.weight grad norm: 0.49372598528862\n",
            " - layer3_input.bias grad norm: 0.0005791634321212769\n",
            " - layer4.weight grad norm: 0.0030361113604158163\n",
            " - layer4.bias grad norm: 0.0005910472245886922\n",
            " - layer4_input.weight grad norm: 0.5021199584007263\n",
            " - layer4_input.bias grad norm: 0.0005910472245886922\n",
            " - layer5.weight grad norm: 0.01158966589719057\n",
            " - layer5.bias grad norm: 0.005249796435236931\n",
            "Gradients at iteration 531:\n",
            " - layer1.weight grad norm: 0.5029239654541016\n",
            " - layer1.bias grad norm: 0.0005796229816041887\n",
            " - layer2.weight grad norm: 0.0018656825413927436\n",
            " - layer2.bias grad norm: 0.0005838042125105858\n",
            " - layer2_input.weight grad norm: 0.5039288997650146\n",
            " - layer2_input.bias grad norm: 0.0005838042125105858\n",
            " - layer3.weight grad norm: 0.002555974991992116\n",
            " - layer3.bias grad norm: 0.0006066117202863097\n",
            " - layer3_input.weight grad norm: 0.520241379737854\n",
            " - layer3_input.bias grad norm: 0.0006066117202863097\n",
            " - layer4.weight grad norm: 0.0030721609946340322\n",
            " - layer4.bias grad norm: 0.0005395797197706997\n",
            " - layer4_input.weight grad norm: 0.47144758701324463\n",
            " - layer4_input.bias grad norm: 0.0005395797197706997\n",
            " - layer5.weight grad norm: 0.012639223597943783\n",
            " - layer5.bias grad norm: 0.005266360938549042\n",
            "Gradients at iteration 532:\n",
            " - layer1.weight grad norm: 0.5156633257865906\n",
            " - layer1.bias grad norm: 0.0006010269862599671\n",
            " - layer2.weight grad norm: 0.0017777683679014444\n",
            " - layer2.bias grad norm: 0.0005962385330349207\n",
            " - layer2_input.weight grad norm: 0.5083631873130798\n",
            " - layer2_input.bias grad norm: 0.0005962385330349207\n",
            " - layer3.weight grad norm: 0.0024920229334384203\n",
            " - layer3.bias grad norm: 0.0005947392783127725\n",
            " - layer3_input.weight grad norm: 0.5080264210700989\n",
            " - layer3_input.bias grad norm: 0.0005947392783127725\n",
            " - layer4.weight grad norm: 0.002940401202067733\n",
            " - layer4.bias grad norm: 0.0005422469694167376\n",
            " - layer4_input.weight grad norm: 0.46627604961395264\n",
            " - layer4_input.bias grad norm: 0.0005422469694167376\n",
            " - layer5.weight grad norm: 0.010368309915065765\n",
            " - layer5.bias grad norm: 0.005112869665026665\n",
            "Gradients at iteration 533:\n",
            " - layer1.weight grad norm: 0.53612220287323\n",
            " - layer1.bias grad norm: 0.0006276391795836389\n",
            " - layer2.weight grad norm: 0.001803696621209383\n",
            " - layer2.bias grad norm: 0.0006108577945269644\n",
            " - layer2_input.weight grad norm: 0.5205079913139343\n",
            " - layer2_input.bias grad norm: 0.0006108577945269644\n",
            " - layer3.weight grad norm: 0.002522936789318919\n",
            " - layer3.bias grad norm: 0.0005654001724906266\n",
            " - layer3_input.weight grad norm: 0.4860104024410248\n",
            " - layer3_input.bias grad norm: 0.0005654001724906266\n",
            " - layer4.weight grad norm: 0.002973052905872464\n",
            " - layer4.bias grad norm: 0.0005275895819067955\n",
            " - layer4_input.weight grad norm: 0.4530380368232727\n",
            " - layer4_input.bias grad norm: 0.0005275895819067955\n",
            " - layer5.weight grad norm: 0.012139585800468922\n",
            " - layer5.bias grad norm: 0.005176878999918699\n",
            "Gradients at iteration 534:\n",
            " - layer1.weight grad norm: 0.4634782373905182\n",
            " - layer1.bias grad norm: 0.0005147960036993027\n",
            " - layer2.weight grad norm: 0.001924233860336244\n",
            " - layer2.bias grad norm: 0.0006476052803918719\n",
            " - layer2_input.weight grad norm: 0.5539066791534424\n",
            " - layer2_input.bias grad norm: 0.0006476052803918719\n",
            " - layer3.weight grad norm: 0.0026830611750483513\n",
            " - layer3.bias grad norm: 0.0005910855834372342\n",
            " - layer3_input.weight grad norm: 0.5107346773147583\n",
            " - layer3_input.bias grad norm: 0.0005910855834372342\n",
            " - layer4.weight grad norm: 0.0031812996603548527\n",
            " - layer4.bias grad norm: 0.0005304719088599086\n",
            " - layer4_input.weight grad norm: 0.4661567509174347\n",
            " - layer4_input.bias grad norm: 0.0005304719088599086\n",
            " - layer5.weight grad norm: 0.013022216036915779\n",
            " - layer5.bias grad norm: 0.005510641727596521\n",
            "Gradients at iteration 535:\n",
            " - layer1.weight grad norm: 0.5359874367713928\n",
            " - layer1.bias grad norm: 0.0006245981785468757\n",
            " - layer2.weight grad norm: 0.0018553079571574926\n",
            " - layer2.bias grad norm: 0.0006233470630832016\n",
            " - layer2_input.weight grad norm: 0.5297704339027405\n",
            " - layer2_input.bias grad norm: 0.0006233470630832016\n",
            " - layer3.weight grad norm: 0.002534140832722187\n",
            " - layer3.bias grad norm: 0.0005495456280186772\n",
            " - layer3_input.weight grad norm: 0.47213467955589294\n",
            " - layer3_input.bias grad norm: 0.0005495456280186772\n",
            " - layer4.weight grad norm: 0.0030097179114818573\n",
            " - layer4.bias grad norm: 0.0005309644620865583\n",
            " - layer4_input.weight grad norm: 0.45713165402412415\n",
            " - layer4_input.bias grad norm: 0.0005309644620865583\n",
            " - layer5.weight grad norm: 0.011486289091408253\n",
            " - layer5.bias grad norm: 0.005206891801208258\n",
            "Gradients at iteration 536:\n",
            " - layer1.weight grad norm: 0.5031359791755676\n",
            " - layer1.bias grad norm: 0.0005875281640328467\n",
            " - layer2.weight grad norm: 0.0018068094504997134\n",
            " - layer2.bias grad norm: 0.0006126413936726749\n",
            " - layer2_input.weight grad norm: 0.5218597054481506\n",
            " - layer2_input.bias grad norm: 0.0006126413936726749\n",
            " - layer3.weight grad norm: 0.002546536037698388\n",
            " - layer3.bias grad norm: 0.0006278311484493315\n",
            " - layer3_input.weight grad norm: 0.5303590297698975\n",
            " - layer3_input.bias grad norm: 0.0006278311484493315\n",
            " - layer4.weight grad norm: 0.0029733574483543634\n",
            " - layer4.bias grad norm: 0.0005037972005084157\n",
            " - layer4_input.weight grad norm: 0.43938660621643066\n",
            " - layer4_input.bias grad norm: 0.0005037972005084157\n",
            " - layer5.weight grad norm: 0.011275469325482845\n",
            " - layer5.bias grad norm: 0.00522737018764019\n",
            "Gradients at iteration 537:\n",
            " - layer1.weight grad norm: 0.5267931222915649\n",
            " - layer1.bias grad norm: 0.0005946318269707263\n",
            " - layer2.weight grad norm: 0.0018988557858392596\n",
            " - layer2.bias grad norm: 0.0005877500516362488\n",
            " - layer2_input.weight grad norm: 0.5143283009529114\n",
            " - layer2_input.bias grad norm: 0.0005877500516362488\n",
            " - layer3.weight grad norm: 0.002598566235974431\n",
            " - layer3.bias grad norm: 0.0005630254745483398\n",
            " - layer3_input.weight grad norm: 0.4909559488296509\n",
            " - layer3_input.bias grad norm: 0.0005630254745483398\n",
            " - layer4.weight grad norm: 0.0031658210791647434\n",
            " - layer4.bias grad norm: 0.0005364894168451428\n",
            " - layer4_input.weight grad norm: 0.46556177735328674\n",
            " - layer4_input.bias grad norm: 0.0005364894168451428\n",
            " - layer5.weight grad norm: 0.010901815257966518\n",
            " - layer5.bias grad norm: 0.005322482902556658\n",
            "Gradients at iteration 538:\n",
            " - layer1.weight grad norm: 0.5349048972129822\n",
            " - layer1.bias grad norm: 0.000617831654381007\n",
            " - layer2.weight grad norm: 0.001851276378147304\n",
            " - layer2.bias grad norm: 0.0005470588803291321\n",
            " - layer2_input.weight grad norm: 0.47969427704811096\n",
            " - layer2_input.bias grad norm: 0.0005470588803291321\n",
            " - layer3.weight grad norm: 0.0025310739874839783\n",
            " - layer3.bias grad norm: 0.0005522054270841181\n",
            " - layer3_input.weight grad norm: 0.47680792212486267\n",
            " - layer3_input.bias grad norm: 0.0005522054270841181\n",
            " - layer4.weight grad norm: 0.0029780608601868153\n",
            " - layer4.bias grad norm: 0.0005941083072684705\n",
            " - layer4_input.weight grad norm: 0.5062047243118286\n",
            " - layer4_input.bias grad norm: 0.0005941083072684705\n",
            " - layer5.weight grad norm: 0.011533955112099648\n",
            " - layer5.bias grad norm: 0.005198396742343903\n",
            "Gradients at iteration 539:\n",
            " - layer1.weight grad norm: 0.5094952583312988\n",
            " - layer1.bias grad norm: 0.0005910824984312057\n",
            " - layer2.weight grad norm: 0.001837942167185247\n",
            " - layer2.bias grad norm: 0.0005548438639380038\n",
            " - layer2_input.weight grad norm: 0.4814532697200775\n",
            " - layer2_input.bias grad norm: 0.0005548438639380038\n",
            " - layer3.weight grad norm: 0.0024720558430999517\n",
            " - layer3.bias grad norm: 0.000589671719353646\n",
            " - layer3_input.weight grad norm: 0.5040273070335388\n",
            " - layer3_input.bias grad norm: 0.000589671719353646\n",
            " - layer4.weight grad norm: 0.0029711138922721148\n",
            " - layer4.bias grad norm: 0.0005912348860874772\n",
            " - layer4_input.weight grad norm: 0.5043725967407227\n",
            " - layer4_input.bias grad norm: 0.0005912348860874772\n",
            " - layer5.weight grad norm: 0.011589054018259048\n",
            " - layer5.bias grad norm: 0.005190238356590271\n",
            "Gradients at iteration 540:\n",
            " - layer1.weight grad norm: 0.5155165791511536\n",
            " - layer1.bias grad norm: 0.0005958734545856714\n",
            " - layer2.weight grad norm: 0.0017839757492765784\n",
            " - layer2.bias grad norm: 0.0006376868113875389\n",
            " - layer2_input.weight grad norm: 0.5442377924919128\n",
            " - layer2_input.bias grad norm: 0.0006376868113875389\n",
            " - layer3.weight grad norm: 0.002553268102928996\n",
            " - layer3.bias grad norm: 0.0005531337228603661\n",
            " - layer3_input.weight grad norm: 0.48018157482147217\n",
            " - layer3_input.bias grad norm: 0.0005531337228603661\n",
            " - layer4.weight grad norm: 0.002964116632938385\n",
            " - layer4.bias grad norm: 0.0005225451895967126\n",
            " - layer4_input.weight grad norm: 0.4552895426750183\n",
            " - layer4_input.bias grad norm: 0.0005225451895967126\n",
            " - layer5.weight grad norm: 0.011735345236957073\n",
            " - layer5.bias grad norm: 0.0051278178580105305\n",
            "Gradients at iteration 541:\n",
            " - layer1.weight grad norm: 0.5516007542610168\n",
            " - layer1.bias grad norm: 0.0006481782766059041\n",
            " - layer2.weight grad norm: 0.0018428766634315252\n",
            " - layer2.bias grad norm: 0.0005537255201488733\n",
            " - layer2_input.weight grad norm: 0.477782279253006\n",
            " - layer2_input.bias grad norm: 0.0005537255201488733\n",
            " - layer3.weight grad norm: 0.002558695152401924\n",
            " - layer3.bias grad norm: 0.0006070093950256705\n",
            " - layer3_input.weight grad norm: 0.5205538868904114\n",
            " - layer3_input.bias grad norm: 0.0006070093950256705\n",
            " - layer4.weight grad norm: 0.003010906046256423\n",
            " - layer4.bias grad norm: 0.0005122532602399588\n",
            " - layer4_input.weight grad norm: 0.4430601894855499\n",
            " - layer4_input.bias grad norm: 0.0005122532602399588\n",
            " - layer5.weight grad norm: 0.011543734930455685\n",
            " - layer5.bias grad norm: 0.0052225226536393166\n",
            "Gradients at iteration 542:\n",
            " - layer1.weight grad norm: 0.5203195214271545\n",
            " - layer1.bias grad norm: 0.0005982554284855723\n",
            " - layer2.weight grad norm: 0.001878396375104785\n",
            " - layer2.bias grad norm: 0.000599948107264936\n",
            " - layer2_input.weight grad norm: 0.5208197236061096\n",
            " - layer2_input.bias grad norm: 0.000599948107264936\n",
            " - layer3.weight grad norm: 0.002594642573967576\n",
            " - layer3.bias grad norm: 0.0005780739593319595\n",
            " - layer3_input.weight grad norm: 0.49873653054237366\n",
            " - layer3_input.bias grad norm: 0.0005780739593319595\n",
            " - layer4.weight grad norm: 0.0030543662142008543\n",
            " - layer4.bias grad norm: 0.000521909911185503\n",
            " - layer4_input.weight grad norm: 0.45726868510246277\n",
            " - layer4_input.bias grad norm: 0.000521909911185503\n",
            " - layer5.weight grad norm: 0.011480157263576984\n",
            " - layer5.bias grad norm: 0.005279690492898226\n",
            "Gradients at iteration 543:\n",
            " - layer1.weight grad norm: 0.5343471169471741\n",
            " - layer1.bias grad norm: 0.0006181293865665793\n",
            " - layer2.weight grad norm: 0.0019817384891211987\n",
            " - layer2.bias grad norm: 0.0006099673919379711\n",
            " - layer2_input.weight grad norm: 0.5233830213546753\n",
            " - layer2_input.bias grad norm: 0.0006099673919379711\n",
            " - layer3.weight grad norm: 0.00275897397659719\n",
            " - layer3.bias grad norm: 0.0005180101725272834\n",
            " - layer3_input.weight grad norm: 0.4561534821987152\n",
            " - layer3_input.bias grad norm: 0.0005180101725272834\n",
            " - layer4.weight grad norm: 0.0032612618524581194\n",
            " - layer4.bias grad norm: 0.0005552904913201928\n",
            " - layer4_input.weight grad norm: 0.4819064736366272\n",
            " - layer4_input.bias grad norm: 0.0005552904913201928\n",
            " - layer5.weight grad norm: 0.013324671424925327\n",
            " - layer5.bias grad norm: 0.00560893677175045\n",
            "Gradients at iteration 544:\n",
            " - layer1.weight grad norm: 0.508085310459137\n",
            " - layer1.bias grad norm: 0.000580830208491534\n",
            " - layer2.weight grad norm: 0.001865879981778562\n",
            " - layer2.bias grad norm: 0.0006139965262264013\n",
            " - layer2_input.weight grad norm: 0.5288141369819641\n",
            " - layer2_input.bias grad norm: 0.0006139965262264013\n",
            " - layer3.weight grad norm: 0.0025334334932267666\n",
            " - layer3.bias grad norm: 0.0005502896383404732\n",
            " - layer3_input.weight grad norm: 0.4803478717803955\n",
            " - layer3_input.bias grad norm: 0.0005502896383404732\n",
            " - layer4.weight grad norm: 0.003048570128157735\n",
            " - layer4.bias grad norm: 0.0005549039342440665\n",
            " - layer4_input.weight grad norm: 0.4809006452560425\n",
            " - layer4_input.bias grad norm: 0.0005549039342440665\n",
            " - layer5.weight grad norm: 0.01249768678098917\n",
            " - layer5.bias grad norm: 0.005280150566250086\n",
            "Gradients at iteration 545:\n",
            " - layer1.weight grad norm: 0.4661765992641449\n",
            " - layer1.bias grad norm: 0.0005352087318897247\n",
            " - layer2.weight grad norm: 0.0017624330939725041\n",
            " - layer2.bias grad norm: 0.0006322686094790697\n",
            " - layer2_input.weight grad norm: 0.5355794429779053\n",
            " - layer2_input.bias grad norm: 0.0006322686094790697\n",
            " - layer3.weight grad norm: 0.0024108286015689373\n",
            " - layer3.bias grad norm: 0.0006000659777782857\n",
            " - layer3_input.weight grad norm: 0.5088321566581726\n",
            " - layer3_input.bias grad norm: 0.0006000659777782857\n",
            " - layer4.weight grad norm: 0.0028885325882583857\n",
            " - layer4.bias grad norm: 0.0005708445096388459\n",
            " - layer4_input.weight grad norm: 0.48656633496284485\n",
            " - layer4_input.bias grad norm: 0.0005708445096388459\n",
            " - layer5.weight grad norm: 0.011513585224747658\n",
            " - layer5.bias grad norm: 0.004984716884791851\n",
            "Gradients at iteration 546:\n",
            " - layer1.weight grad norm: 0.4938546121120453\n",
            " - layer1.bias grad norm: 0.0005737125757150352\n",
            " - layer2.weight grad norm: 0.0017238672589883208\n",
            " - layer2.bias grad norm: 0.0006034068064764142\n",
            " - layer2_input.weight grad norm: 0.5138395428657532\n",
            " - layer2_input.bias grad norm: 0.0006034068064764142\n",
            " - layer3.weight grad norm: 0.002432111417874694\n",
            " - layer3.bias grad norm: 0.000644554034806788\n",
            " - layer3_input.weight grad norm: 0.542927086353302\n",
            " - layer3_input.bias grad norm: 0.000644554034806788\n",
            " - layer4.weight grad norm: 0.002860197564586997\n",
            " - layer4.bias grad norm: 0.0005176711711101234\n",
            " - layer4_input.weight grad norm: 0.4440249800682068\n",
            " - layer4_input.bias grad norm: 0.0005176711711101234\n",
            " - layer5.weight grad norm: 0.01025345828384161\n",
            " - layer5.bias grad norm: 0.004885845817625523\n",
            "Gradients at iteration 547:\n",
            " - layer1.weight grad norm: 0.5041429400444031\n",
            " - layer1.bias grad norm: 0.0005866665742360055\n",
            " - layer2.weight grad norm: 0.0018091254169121385\n",
            " - layer2.bias grad norm: 0.0006085410132072866\n",
            " - layer2_input.weight grad norm: 0.5179158449172974\n",
            " - layer2_input.bias grad norm: 0.0006085410132072866\n",
            " - layer3.weight grad norm: 0.0025016546715050936\n",
            " - layer3.bias grad norm: 0.0006032648961991072\n",
            " - layer3_input.weight grad norm: 0.5134658217430115\n",
            " - layer3_input.bias grad norm: 0.0006032648961991072\n",
            " - layer4.weight grad norm: 0.002974458271637559\n",
            " - layer4.bias grad norm: 0.0005381247028708458\n",
            " - layer4_input.weight grad norm: 0.4623642563819885\n",
            " - layer4_input.bias grad norm: 0.0005381247028708458\n",
            " - layer5.weight grad norm: 0.01131437812000513\n",
            " - layer5.bias grad norm: 0.005136643070727587\n",
            "Gradients at iteration 548:\n",
            " - layer1.weight grad norm: 0.5065124034881592\n",
            " - layer1.bias grad norm: 0.0005898080416955054\n",
            " - layer2.weight grad norm: 0.0018401917768642306\n",
            " - layer2.bias grad norm: 0.0006172713474370539\n",
            " - layer2_input.weight grad norm: 0.5274868011474609\n",
            " - layer2_input.bias grad norm: 0.0006172713474370539\n",
            " - layer3.weight grad norm: 0.002518292283639312\n",
            " - layer3.bias grad norm: 0.0005320238415151834\n",
            " - layer3_input.weight grad norm: 0.46202972531318665\n",
            " - layer3_input.bias grad norm: 0.0005320238415151834\n",
            " - layer4.weight grad norm: 0.0029892567545175552\n",
            " - layer4.bias grad norm: 0.0005857835058122873\n",
            " - layer4_input.weight grad norm: 0.5015385746955872\n",
            " - layer4_input.bias grad norm: 0.0005857835058122873\n",
            " - layer5.weight grad norm: 0.011927292682230473\n",
            " - layer5.bias grad norm: 0.005220103543251753\n",
            "Gradients at iteration 549:\n",
            " - layer1.weight grad norm: 0.5301339626312256\n",
            " - layer1.bias grad norm: 0.0006218493217602372\n",
            " - layer2.weight grad norm: 0.0018655558815225959\n",
            " - layer2.bias grad norm: 0.0006045500049367547\n",
            " - layer2_input.weight grad norm: 0.5170267224311829\n",
            " - layer2_input.bias grad norm: 0.0006045500049367547\n",
            " - layer3.weight grad norm: 0.002540545305237174\n",
            " - layer3.bias grad norm: 0.0005566108739003539\n",
            " - layer3_input.weight grad norm: 0.48134487867355347\n",
            " - layer3_input.bias grad norm: 0.0005566108739003539\n",
            " - layer4.weight grad norm: 0.0030476402025669813\n",
            " - layer4.bias grad norm: 0.0005432244506664574\n",
            " - layer4_input.weight grad norm: 0.468792200088501\n",
            " - layer4_input.bias grad norm: 0.0005432244506664574\n",
            " - layer5.weight grad norm: 0.011521962471306324\n",
            " - layer5.bias grad norm: 0.005283953156322241\n",
            "Gradients at iteration 550:\n",
            " - layer1.weight grad norm: 0.497493714094162\n",
            " - layer1.bias grad norm: 0.0005753061850555241\n",
            " - layer2.weight grad norm: 0.0019170785089954734\n",
            " - layer2.bias grad norm: 0.0005913804052397609\n",
            " - layer2_input.weight grad norm: 0.5106772184371948\n",
            " - layer2_input.bias grad norm: 0.0005913804052397609\n",
            " - layer3.weight grad norm: 0.002630628878250718\n",
            " - layer3.bias grad norm: 0.0006158964242786169\n",
            " - layer3_input.weight grad norm: 0.5262956023216248\n",
            " - layer3_input.bias grad norm: 0.0006158964242786169\n",
            " - layer4.weight grad norm: 0.0031517385505139828\n",
            " - layer4.bias grad norm: 0.0005342839285731316\n",
            " - layer4_input.weight grad norm: 0.46316343545913696\n",
            " - layer4_input.bias grad norm: 0.0005342839285731316\n",
            " - layer5.weight grad norm: 0.012200424447655678\n",
            " - layer5.bias grad norm: 0.005427270662039518\n",
            "Gradients at iteration 551:\n",
            " - layer1.weight grad norm: 0.5129711031913757\n",
            " - layer1.bias grad norm: 0.000594642071519047\n",
            " - layer2.weight grad norm: 0.0017769928090274334\n",
            " - layer2.bias grad norm: 0.000614755495917052\n",
            " - layer2_input.weight grad norm: 0.5286688208580017\n",
            " - layer2_input.bias grad norm: 0.000614755495917052\n",
            " - layer3.weight grad norm: 0.002506836084648967\n",
            " - layer3.bias grad norm: 0.0005527987377718091\n",
            " - layer3_input.weight grad norm: 0.47656363248825073\n",
            " - layer3_input.bias grad norm: 0.0005527987377718091\n",
            " - layer4.weight grad norm: 0.0029148608446121216\n",
            " - layer4.bias grad norm: 0.0005559084238484502\n",
            " - layer4_input.weight grad norm: 0.4796646237373352\n",
            " - layer4_input.bias grad norm: 0.0005559084238484502\n",
            " - layer5.weight grad norm: 0.011521424166858196\n",
            " - layer5.bias grad norm: 0.005080989561975002\n",
            "Gradients at iteration 552:\n",
            " - layer1.weight grad norm: 0.5277785658836365\n",
            " - layer1.bias grad norm: 0.0006145333172753453\n",
            " - layer2.weight grad norm: 0.001850851927883923\n",
            " - layer2.bias grad norm: 0.000583989080041647\n",
            " - layer2_input.weight grad norm: 0.5068489909172058\n",
            " - layer2_input.bias grad norm: 0.000583989080041647\n",
            " - layer3.weight grad norm: 0.002576514147222042\n",
            " - layer3.bias grad norm: 0.0005094990483485162\n",
            " - layer3_input.weight grad norm: 0.44825372099876404\n",
            " - layer3_input.bias grad norm: 0.0005094990483485162\n",
            " - layer4.weight grad norm: 0.00298824324272573\n",
            " - layer4.bias grad norm: 0.0005963323055766523\n",
            " - layer4_input.weight grad norm: 0.5132483839988708\n",
            " - layer4_input.bias grad norm: 0.0005963323055766523\n",
            " - layer5.weight grad norm: 0.012254795990884304\n",
            " - layer5.bias grad norm: 0.0052007692866027355\n",
            "Gradients at iteration 553:\n",
            " - layer1.weight grad norm: 0.5137746930122375\n",
            " - layer1.bias grad norm: 0.0005926701705902815\n",
            " - layer2.weight grad norm: 0.0019169175066053867\n",
            " - layer2.bias grad norm: 0.0006197303300723433\n",
            " - layer2_input.weight grad norm: 0.5310126543045044\n",
            " - layer2_input.bias grad norm: 0.0006197303300723433\n",
            " - layer3.weight grad norm: 0.0026135060470551252\n",
            " - layer3.bias grad norm: 0.000579790270421654\n",
            " - layer3_input.weight grad norm: 0.502023458480835\n",
            " - layer3_input.bias grad norm: 0.000579790270421654\n",
            " - layer4.weight grad norm: 0.0030770476441830397\n",
            " - layer4.bias grad norm: 0.0005120009882375598\n",
            " - layer4_input.weight grad norm: 0.4492584764957428\n",
            " - layer4_input.bias grad norm: 0.0005120009882375598\n",
            " - layer5.weight grad norm: 0.01220504567027092\n",
            " - layer5.bias grad norm: 0.005391089711338282\n",
            "Gradients at iteration 554:\n",
            " - layer1.weight grad norm: 0.5112581849098206\n",
            " - layer1.bias grad norm: 0.0005875069182366133\n",
            " - layer2.weight grad norm: 0.001889808801934123\n",
            " - layer2.bias grad norm: 0.0006104399799369276\n",
            " - layer2_input.weight grad norm: 0.524250864982605\n",
            " - layer2_input.bias grad norm: 0.0006104399799369276\n",
            " - layer3.weight grad norm: 0.0025475057773292065\n",
            " - layer3.bias grad norm: 0.0005440065287984908\n",
            " - layer3_input.weight grad norm: 0.4722181558609009\n",
            " - layer3_input.bias grad norm: 0.0005440065287984908\n",
            " - layer4.weight grad norm: 0.003031548811122775\n",
            " - layer4.bias grad norm: 0.0005713732680305839\n",
            " - layer4_input.weight grad norm: 0.4905175566673279\n",
            " - layer4_input.bias grad norm: 0.0005713732680305839\n",
            " - layer5.weight grad norm: 0.011369685642421246\n",
            " - layer5.bias grad norm: 0.0052705444395542145\n",
            "Gradients at iteration 555:\n",
            " - layer1.weight grad norm: 0.5353929996490479\n",
            " - layer1.bias grad norm: 0.0006249487632885575\n",
            " - layer2.weight grad norm: 0.001815898809581995\n",
            " - layer2.bias grad norm: 0.0005719446344301105\n",
            " - layer2_input.weight grad norm: 0.4943479895591736\n",
            " - layer2_input.bias grad norm: 0.0005719446344301105\n",
            " - layer3.weight grad norm: 0.002504875184968114\n",
            " - layer3.bias grad norm: 0.0005686345975846052\n",
            " - layer3_input.weight grad norm: 0.49131953716278076\n",
            " - layer3_input.bias grad norm: 0.0005686345975846052\n",
            " - layer4.weight grad norm: 0.003018162678927183\n",
            " - layer4.bias grad norm: 0.0005518364487215877\n",
            " - layer4_input.weight grad norm: 0.476849228143692\n",
            " - layer4_input.bias grad norm: 0.0005518364487215877\n",
            " - layer5.weight grad norm: 0.012088279239833355\n",
            " - layer5.bias grad norm: 0.005199480801820755\n",
            "Gradients at iteration 556:\n",
            " - layer1.weight grad norm: 0.5091202259063721\n",
            " - layer1.bias grad norm: 0.0005928659229539335\n",
            " - layer2.weight grad norm: 0.0017938755918294191\n",
            " - layer2.bias grad norm: 0.0006139291217550635\n",
            " - layer2_input.weight grad norm: 0.5206105709075928\n",
            " - layer2_input.bias grad norm: 0.0006139291217550635\n",
            " - layer3.weight grad norm: 0.002428721636533737\n",
            " - layer3.bias grad norm: 0.0005531695787794888\n",
            " - layer3_input.weight grad norm: 0.4778654873371124\n",
            " - layer3_input.bias grad norm: 0.0005531695787794888\n",
            " - layer4.weight grad norm: 0.0029263768810778856\n",
            " - layer4.bias grad norm: 0.0005753198638558388\n",
            " - layer4_input.weight grad norm: 0.4911244213581085\n",
            " - layer4_input.bias grad norm: 0.0005753198638558388\n",
            " - layer5.weight grad norm: 0.012540389783680439\n",
            " - layer5.bias grad norm: 0.005035521928220987\n",
            "Gradients at iteration 557:\n",
            " - layer1.weight grad norm: 0.5100477337837219\n",
            " - layer1.bias grad norm: 0.0005830541485920548\n",
            " - layer2.weight grad norm: 0.0018971140962094069\n",
            " - layer2.bias grad norm: 0.0005872358451597393\n",
            " - layer2_input.weight grad norm: 0.5072308778762817\n",
            " - layer2_input.bias grad norm: 0.0005872358451597393\n",
            " - layer3.weight grad norm: 0.002621718682348728\n",
            " - layer3.bias grad norm: 0.0005647794459946454\n",
            " - layer3_input.weight grad norm: 0.4918545186519623\n",
            " - layer3_input.bias grad norm: 0.0005647794459946454\n",
            " - layer4.weight grad norm: 0.0031154591124504805\n",
            " - layer4.bias grad norm: 0.000564842950552702\n",
            " - layer4_input.weight grad norm: 0.4903396666049957\n",
            " - layer4_input.bias grad norm: 0.000564842950552702\n",
            " - layer5.weight grad norm: 0.012762447819113731\n",
            " - layer5.bias grad norm: 0.005373707972466946\n",
            "Gradients at iteration 558:\n",
            " - layer1.weight grad norm: 0.5494148135185242\n",
            " - layer1.bias grad norm: 0.0006446319748647511\n",
            " - layer2.weight grad norm: 0.001885410980321467\n",
            " - layer2.bias grad norm: 0.0005725317751057446\n",
            " - layer2_input.weight grad norm: 0.4968169927597046\n",
            " - layer2_input.bias grad norm: 0.0005725317751057446\n",
            " - layer3.weight grad norm: 0.0026039951480925083\n",
            " - layer3.bias grad norm: 0.0005821926170028746\n",
            " - layer3_input.weight grad norm: 0.4973068833351135\n",
            " - layer3_input.bias grad norm: 0.0005821926170028746\n",
            " - layer4.weight grad norm: 0.0030858959071338177\n",
            " - layer4.bias grad norm: 0.0005207612994126976\n",
            " - layer4_input.weight grad norm: 0.45140859484672546\n",
            " - layer4_input.bias grad norm: 0.0005207612994126976\n",
            " - layer5.weight grad norm: 0.013484357856214046\n",
            " - layer5.bias grad norm: 0.005346768070012331\n",
            "Gradients at iteration 559:\n",
            " - layer1.weight grad norm: 0.5065287947654724\n",
            " - layer1.bias grad norm: 0.0005795002798549831\n",
            " - layer2.weight grad norm: 0.0018303539836779237\n",
            " - layer2.bias grad norm: 0.0006074384436942637\n",
            " - layer2_input.weight grad norm: 0.524036169052124\n",
            " - layer2_input.bias grad norm: 0.0006074384436942637\n",
            " - layer3.weight grad norm: 0.0025323370937258005\n",
            " - layer3.bias grad norm: 0.0005630427622236311\n",
            " - layer3_input.weight grad norm: 0.4898204207420349\n",
            " - layer3_input.bias grad norm: 0.0005630427622236311\n",
            " - layer4.weight grad norm: 0.003015838796272874\n",
            " - layer4.bias grad norm: 0.0005555994575843215\n",
            " - layer4_input.weight grad norm: 0.47822487354278564\n",
            " - layer4_input.bias grad norm: 0.0005555994575843215\n",
            " - layer5.weight grad norm: 0.0119946813210845\n",
            " - layer5.bias grad norm: 0.005148240830749273\n",
            "Gradients at iteration 560:\n",
            " - layer1.weight grad norm: 0.5425971150398254\n",
            " - layer1.bias grad norm: 0.0006325867143459618\n",
            " - layer2.weight grad norm: 0.0018282225355505943\n",
            " - layer2.bias grad norm: 0.0005824282998219132\n",
            " - layer2_input.weight grad norm: 0.5053887367248535\n",
            " - layer2_input.bias grad norm: 0.0005824282998219132\n",
            " - layer3.weight grad norm: 0.0025372884701937437\n",
            " - layer3.bias grad norm: 0.0005530614871531725\n",
            " - layer3_input.weight grad norm: 0.4837215840816498\n",
            " - layer3_input.bias grad norm: 0.0005530614871531725\n",
            " - layer4.weight grad norm: 0.003036574926227331\n",
            " - layer4.bias grad norm: 0.0005316349561326206\n",
            " - layer4_input.weight grad norm: 0.4647520184516907\n",
            " - layer4_input.bias grad norm: 0.0005316349561326206\n",
            " - layer5.weight grad norm: 0.011873327195644379\n",
            " - layer5.bias grad norm: 0.005224430933594704\n",
            "Gradients at iteration 561:\n",
            " - layer1.weight grad norm: 0.5302631258964539\n",
            " - layer1.bias grad norm: 0.0006139542674645782\n",
            " - layer2.weight grad norm: 0.0018400681437924504\n",
            " - layer2.bias grad norm: 0.0005484740831889212\n",
            " - layer2_input.weight grad norm: 0.47938793897628784\n",
            " - layer2_input.bias grad norm: 0.0005484740831889212\n",
            " - layer3.weight grad norm: 0.0025363792665302753\n",
            " - layer3.bias grad norm: 0.0005770436837337911\n",
            " - layer3_input.weight grad norm: 0.4975336194038391\n",
            " - layer3_input.bias grad norm: 0.0005770436837337911\n",
            " - layer4.weight grad norm: 0.002990970853716135\n",
            " - layer4.bias grad norm: 0.0005746245733462274\n",
            " - layer4_input.weight grad norm: 0.49120476841926575\n",
            " - layer4_input.bias grad norm: 0.0005746245733462274\n",
            " - layer5.weight grad norm: 0.011776714585721493\n",
            " - layer5.bias grad norm: 0.005150853656232357\n",
            "Gradients at iteration 562:\n",
            " - layer1.weight grad norm: 0.5377764105796814\n",
            " - layer1.bias grad norm: 0.0006205127574503422\n",
            " - layer2.weight grad norm: 0.0019133557798340917\n",
            " - layer2.bias grad norm: 0.0005551049835048616\n",
            " - layer2_input.weight grad norm: 0.4867972135543823\n",
            " - layer2_input.bias grad norm: 0.0005551049835048616\n",
            " - layer3.weight grad norm: 0.002610869240015745\n",
            " - layer3.bias grad norm: 0.0005644373595714569\n",
            " - layer3_input.weight grad norm: 0.4951366186141968\n",
            " - layer3_input.bias grad norm: 0.0005644373595714569\n",
            " - layer4.weight grad norm: 0.0031078546307981014\n",
            " - layer4.bias grad norm: 0.000548263662494719\n",
            " - layer4_input.weight grad norm: 0.4779578745365143\n",
            " - layer4_input.bias grad norm: 0.000548263662494719\n",
            " - layer5.weight grad norm: 0.013038679957389832\n",
            " - layer5.bias grad norm: 0.005337476264685392\n",
            "Gradients at iteration 563:\n",
            " - layer1.weight grad norm: 0.5228577256202698\n",
            " - layer1.bias grad norm: 0.0005974746309220791\n",
            " - layer2.weight grad norm: 0.0018428356852382421\n",
            " - layer2.bias grad norm: 0.0006043260800652206\n",
            " - layer2_input.weight grad norm: 0.5249601006507874\n",
            " - layer2_input.bias grad norm: 0.0006043260800652206\n",
            " - layer3.weight grad norm: 0.0025377189740538597\n",
            " - layer3.bias grad norm: 0.000539841188583523\n",
            " - layer3_input.weight grad norm: 0.4717426002025604\n",
            " - layer3_input.bias grad norm: 0.000539841188583523\n",
            " - layer4.weight grad norm: 0.003011814085766673\n",
            " - layer4.bias grad norm: 0.0005475770449265838\n",
            " - layer4_input.weight grad norm: 0.47782206535339355\n",
            " - layer4_input.bias grad norm: 0.0005475770449265838\n",
            " - layer5.weight grad norm: 0.011529183015227318\n",
            " - layer5.bias grad norm: 0.005249444395303726\n",
            "Gradients at iteration 564:\n",
            " - layer1.weight grad norm: 0.5376567244529724\n",
            " - layer1.bias grad norm: 0.0006146907689981163\n",
            " - layer2.weight grad norm: 0.0017823654925450683\n",
            " - layer2.bias grad norm: 0.0005821377271786332\n",
            " - layer2_input.weight grad norm: 0.5060316324234009\n",
            " - layer2_input.bias grad norm: 0.0005821377271786332\n",
            " - layer3.weight grad norm: 0.002455553039908409\n",
            " - layer3.bias grad norm: 0.0005250278627499938\n",
            " - layer3_input.weight grad norm: 0.46041715145111084\n",
            " - layer3_input.bias grad norm: 0.0005250278627499938\n",
            " - layer4.weight grad norm: 0.00290957884863019\n",
            " - layer4.bias grad norm: 0.0005684141651727259\n",
            " - layer4_input.weight grad norm: 0.4926419258117676\n",
            " - layer4_input.bias grad norm: 0.0005684141651727259\n",
            " - layer5.weight grad norm: 0.011488275602459908\n",
            " - layer5.bias grad norm: 0.00503488490357995\n",
            "Gradients at iteration 565:\n",
            " - layer1.weight grad norm: 0.500639796257019\n",
            " - layer1.bias grad norm: 0.0005722258938476443\n",
            " - layer2.weight grad norm: 0.0018288260325789452\n",
            " - layer2.bias grad norm: 0.0005661312025040388\n",
            " - layer2_input.weight grad norm: 0.4942869544029236\n",
            " - layer2_input.bias grad norm: 0.0005661312025040388\n",
            " - layer3.weight grad norm: 0.002536021638661623\n",
            " - layer3.bias grad norm: 0.000622939202003181\n",
            " - layer3_input.weight grad norm: 0.5332550406455994\n",
            " - layer3_input.bias grad norm: 0.000622939202003181\n",
            " - layer4.weight grad norm: 0.0030190495308488607\n",
            " - layer4.bias grad norm: 0.0005435326020233333\n",
            " - layer4_input.weight grad norm: 0.4695761203765869\n",
            " - layer4_input.bias grad norm: 0.0005435326020233333\n",
            " - layer5.weight grad norm: 0.011366229504346848\n",
            " - layer5.bias grad norm: 0.00521282060071826\n",
            "Gradients at iteration 566:\n",
            " - layer1.weight grad norm: 0.5214576721191406\n",
            " - layer1.bias grad norm: 0.0006107068620622158\n",
            " - layer2.weight grad norm: 0.0018007258186116815\n",
            " - layer2.bias grad norm: 0.0005902547854930162\n",
            " - layer2_input.weight grad norm: 0.5041905045509338\n",
            " - layer2_input.bias grad norm: 0.0005902547854930162\n",
            " - layer3.weight grad norm: 0.002468645107001066\n",
            " - layer3.bias grad norm: 0.0005808710120618343\n",
            " - layer3_input.weight grad norm: 0.49367156624794006\n",
            " - layer3_input.bias grad norm: 0.0005808710120618343\n",
            " - layer4.weight grad norm: 0.0029578127432614565\n",
            " - layer4.bias grad norm: 0.0005619870498776436\n",
            " - layer4_input.weight grad norm: 0.4795743227005005\n",
            " - layer4_input.bias grad norm: 0.0005619870498776436\n",
            " - layer5.weight grad norm: 0.011153582483530045\n",
            " - layer5.bias grad norm: 0.005065536592155695\n",
            "Gradients at iteration 567:\n",
            " - layer1.weight grad norm: 0.4653673470020294\n",
            " - layer1.bias grad norm: 0.0005273137940093875\n",
            " - layer2.weight grad norm: 0.001975392457097769\n",
            " - layer2.bias grad norm: 0.0006512078107334673\n",
            " - layer2_input.weight grad norm: 0.5543851256370544\n",
            " - layer2_input.bias grad norm: 0.0006512078107334673\n",
            " - layer3.weight grad norm: 0.0026950149331241846\n",
            " - layer3.bias grad norm: 0.0005673386622220278\n",
            " - layer3_input.weight grad norm: 0.4881645739078522\n",
            " - layer3_input.bias grad norm: 0.0005673386622220278\n",
            " - layer4.weight grad norm: 0.0031935947481542826\n",
            " - layer4.bias grad norm: 0.0005690433899872005\n",
            " - layer4_input.weight grad norm: 0.48742109537124634\n",
            " - layer4_input.bias grad norm: 0.0005690433899872005\n",
            " - layer5.weight grad norm: 0.012326639145612717\n",
            " - layer5.bias grad norm: 0.005551879294216633\n",
            "Gradients at iteration 568:\n",
            " - layer1.weight grad norm: 0.5232895016670227\n",
            " - layer1.bias grad norm: 0.0006014856626279652\n",
            " - layer2.weight grad norm: 0.0018279546638950706\n",
            " - layer2.bias grad norm: 0.0005771394935436547\n",
            " - layer2_input.weight grad norm: 0.5047492980957031\n",
            " - layer2_input.bias grad norm: 0.0005771394935436547\n",
            " - layer3.weight grad norm: 0.002481178380548954\n",
            " - layer3.bias grad norm: 0.0005859446828253567\n",
            " - layer3_input.weight grad norm: 0.5051844716072083\n",
            " - layer3_input.bias grad norm: 0.0005859446828253567\n",
            " - layer4.weight grad norm: 0.002909015864133835\n",
            " - layer4.bias grad norm: 0.0005352419102564454\n",
            " - layer4_input.weight grad norm: 0.46474549174308777\n",
            " - layer4_input.bias grad norm: 0.0005352419102564454\n",
            " - layer5.weight grad norm: 0.012256121262907982\n",
            " - layer5.bias grad norm: 0.005111469887197018\n",
            "Gradients at iteration 569:\n",
            " - layer1.weight grad norm: 0.5284044742584229\n",
            " - layer1.bias grad norm: 0.0006162026547826827\n",
            " - layer2.weight grad norm: 0.001885640318505466\n",
            " - layer2.bias grad norm: 0.0005741812638007104\n",
            " - layer2_input.weight grad norm: 0.4936673939228058\n",
            " - layer2_input.bias grad norm: 0.0005741812638007104\n",
            " - layer3.weight grad norm: 0.002549918834120035\n",
            " - layer3.bias grad norm: 0.000588143477216363\n",
            " - layer3_input.weight grad norm: 0.5019527673721313\n",
            " - layer3_input.bias grad norm: 0.000588143477216363\n",
            " - layer4.weight grad norm: 0.00304145528934896\n",
            " - layer4.bias grad norm: 0.0005541379796341062\n",
            " - layer4_input.weight grad norm: 0.47427043318748474\n",
            " - layer4_input.bias grad norm: 0.0005541379796341062\n",
            " - layer5.weight grad norm: 0.01195349358022213\n",
            " - layer5.bias grad norm: 0.005264077335596085\n",
            "Gradients at iteration 570:\n",
            " - layer1.weight grad norm: 0.4978220462799072\n",
            " - layer1.bias grad norm: 0.0005702986381947994\n",
            " - layer2.weight grad norm: 0.0018993886187672615\n",
            " - layer2.bias grad norm: 0.0005917029338888824\n",
            " - layer2_input.weight grad norm: 0.5120769143104553\n",
            " - layer2_input.bias grad norm: 0.0005917029338888824\n",
            " - layer3.weight grad norm: 0.002634617732837796\n",
            " - layer3.bias grad norm: 0.000592511729337275\n",
            " - layer3_input.weight grad norm: 0.5087670087814331\n",
            " - layer3_input.bias grad norm: 0.000592511729337275\n",
            " - layer4.weight grad norm: 0.0030903334263712168\n",
            " - layer4.bias grad norm: 0.0005618467694148421\n",
            " - layer4_input.weight grad norm: 0.48053014278411865\n",
            " - layer4_input.bias grad norm: 0.0005618467694148421\n",
            " - layer5.weight grad norm: 0.012088737450540066\n",
            " - layer5.bias grad norm: 0.005360867828130722\n",
            "Gradients at iteration 571:\n",
            " - layer1.weight grad norm: 0.5484547019004822\n",
            " - layer1.bias grad norm: 0.0006445842445828021\n",
            " - layer2.weight grad norm: 0.0018471957882866263\n",
            " - layer2.bias grad norm: 0.000545248796697706\n",
            " - layer2_input.weight grad norm: 0.4698226749897003\n",
            " - layer2_input.bias grad norm: 0.000545248796697706\n",
            " - layer3.weight grad norm: 0.0025233791675418615\n",
            " - layer3.bias grad norm: 0.0005949357873760164\n",
            " - layer3_input.weight grad norm: 0.5047183632850647\n",
            " - layer3_input.bias grad norm: 0.0005949357873760164\n",
            " - layer4.weight grad norm: 0.003013936337083578\n",
            " - layer4.bias grad norm: 0.000549622462131083\n",
            " - layer4_input.weight grad norm: 0.47278469800949097\n",
            " - layer4_input.bias grad norm: 0.000549622462131083\n",
            " - layer5.weight grad norm: 0.01224606204777956\n",
            " - layer5.bias grad norm: 0.0051947240717709064\n",
            "Gradients at iteration 572:\n",
            " - layer1.weight grad norm: 0.5017754435539246\n",
            " - layer1.bias grad norm: 0.0005799387581646442\n",
            " - layer2.weight grad norm: 0.0018380647525191307\n",
            " - layer2.bias grad norm: 0.0005778548074886203\n",
            " - layer2_input.weight grad norm: 0.5021967887878418\n",
            " - layer2_input.bias grad norm: 0.0005778548074886203\n",
            " - layer3.weight grad norm: 0.0025637056678533554\n",
            " - layer3.bias grad norm: 0.0006047470960766077\n",
            " - layer3_input.weight grad norm: 0.5141730308532715\n",
            " - layer3_input.bias grad norm: 0.0006047470960766077\n",
            " - layer4.weight grad norm: 0.0030168029479682446\n",
            " - layer4.bias grad norm: 0.0005619216244667768\n",
            " - layer4_input.weight grad norm: 0.48108404874801636\n",
            " - layer4_input.bias grad norm: 0.0005619216244667768\n",
            " - layer5.weight grad norm: 0.012446168810129166\n",
            " - layer5.bias grad norm: 0.005275795701891184\n",
            "Gradients at iteration 573:\n",
            " - layer1.weight grad norm: 0.5422695875167847\n",
            " - layer1.bias grad norm: 0.0006338767125271261\n",
            " - layer2.weight grad norm: 0.0018809422617778182\n",
            " - layer2.bias grad norm: 0.0005697317537851632\n",
            " - layer2_input.weight grad norm: 0.49429893493652344\n",
            " - layer2_input.bias grad norm: 0.0005697317537851632\n",
            " - layer3.weight grad norm: 0.0025765979662537575\n",
            " - layer3.bias grad norm: 0.0005760384956374764\n",
            " - layer3_input.weight grad norm: 0.49925497174263\n",
            " - layer3_input.bias grad norm: 0.0005760384956374764\n",
            " - layer4.weight grad norm: 0.002996073802933097\n",
            " - layer4.bias grad norm: 0.0005307793035171926\n",
            " - layer4_input.weight grad norm: 0.4606168568134308\n",
            " - layer4_input.bias grad norm: 0.0005307793035171926\n",
            " - layer5.weight grad norm: 0.011812576092779636\n",
            " - layer5.bias grad norm: 0.0052659823559224606\n",
            "Gradients at iteration 574:\n",
            " - layer1.weight grad norm: 0.5246344804763794\n",
            " - layer1.bias grad norm: 0.0005988958291709423\n",
            " - layer2.weight grad norm: 0.0018903656164184213\n",
            " - layer2.bias grad norm: 0.0006232442101463675\n",
            " - layer2_input.weight grad norm: 0.5335676074028015\n",
            " - layer2_input.bias grad norm: 0.0006232442101463675\n",
            " - layer3.weight grad norm: 0.0026089586317539215\n",
            " - layer3.bias grad norm: 0.0005718863103538752\n",
            " - layer3_input.weight grad norm: 0.5002160668373108\n",
            " - layer3_input.bias grad norm: 0.0005718863103538752\n",
            " - layer4.weight grad norm: 0.003048401093110442\n",
            " - layer4.bias grad norm: 0.0004910008865408599\n",
            " - layer4_input.weight grad norm: 0.4354913830757141\n",
            " - layer4_input.bias grad norm: 0.0004910008865408599\n",
            " - layer5.weight grad norm: 0.012021918781101704\n",
            " - layer5.bias grad norm: 0.005401609465479851\n",
            "Gradients at iteration 575:\n",
            " - layer1.weight grad norm: 0.5469611883163452\n",
            " - layer1.bias grad norm: 0.000638615048956126\n",
            " - layer2.weight grad norm: 0.0018079226138070226\n",
            " - layer2.bias grad norm: 0.0005649129743687809\n",
            " - layer2_input.weight grad norm: 0.49075761437416077\n",
            " - layer2_input.bias grad norm: 0.0005649129743687809\n",
            " - layer3.weight grad norm: 0.0024484649766236544\n",
            " - layer3.bias grad norm: 0.0005314761074259877\n",
            " - layer3_input.weight grad norm: 0.4620829224586487\n",
            " - layer3_input.bias grad norm: 0.0005314761074259877\n",
            " - layer4.weight grad norm: 0.002902114065364003\n",
            " - layer4.bias grad norm: 0.0005799196078442037\n",
            " - layer4_input.weight grad norm: 0.49625200033187866\n",
            " - layer4_input.bias grad norm: 0.0005799196078442037\n",
            " - layer5.weight grad norm: 0.01259988360106945\n",
            " - layer5.bias grad norm: 0.005018696188926697\n",
            "Gradients at iteration 576:\n",
            " - layer1.weight grad norm: 0.5258554816246033\n",
            " - layer1.bias grad norm: 0.0006162666250020266\n",
            " - layer2.weight grad norm: 0.001861983328126371\n",
            " - layer2.bias grad norm: 0.0005697526503354311\n",
            " - layer2_input.weight grad norm: 0.49238574504852295\n",
            " - layer2_input.bias grad norm: 0.0005697526503354311\n",
            " - layer3.weight grad norm: 0.002517014741897583\n",
            " - layer3.bias grad norm: 0.0005341946380212903\n",
            " - layer3_input.weight grad norm: 0.46391183137893677\n",
            " - layer3_input.bias grad norm: 0.0005341946380212903\n",
            " - layer4.weight grad norm: 0.002990335924550891\n",
            " - layer4.bias grad norm: 0.000605332781560719\n",
            " - layer4_input.weight grad norm: 0.5154021978378296\n",
            " - layer4_input.bias grad norm: 0.000605332781560719\n",
            " - layer5.weight grad norm: 0.01141008734703064\n",
            " - layer5.bias grad norm: 0.005229314789175987\n",
            "Gradients at iteration 577:\n",
            " - layer1.weight grad norm: 0.5021318197250366\n",
            " - layer1.bias grad norm: 0.0005826763808727264\n",
            " - layer2.weight grad norm: 0.0017365149687975645\n",
            " - layer2.bias grad norm: 0.0005591966910287738\n",
            " - layer2_input.weight grad norm: 0.4830639958381653\n",
            " - layer2_input.bias grad norm: 0.0005591966910287738\n",
            " - layer3.weight grad norm: 0.002362225204706192\n",
            " - layer3.bias grad norm: 0.0006030933582223952\n",
            " - layer3_input.weight grad norm: 0.5151270031929016\n",
            " - layer3_input.bias grad norm: 0.0006030933582223952\n",
            " - layer4.weight grad norm: 0.0028303158469498158\n",
            " - layer4.bias grad norm: 0.0005880255484953523\n",
            " - layer4_input.weight grad norm: 0.4989872872829437\n",
            " - layer4_input.bias grad norm: 0.0005880255484953523\n",
            " - layer5.weight grad norm: 0.011219514533877373\n",
            " - layer5.bias grad norm: 0.004866165574640036\n",
            "Gradients at iteration 578:\n",
            " - layer1.weight grad norm: 0.549618124961853\n",
            " - layer1.bias grad norm: 0.0006586371455341578\n",
            " - layer2.weight grad norm: 0.001761141698807478\n",
            " - layer2.bias grad norm: 0.0005921690026298165\n",
            " - layer2_input.weight grad norm: 0.49974820017814636\n",
            " - layer2_input.bias grad norm: 0.0005921690026298165\n",
            " - layer3.weight grad norm: 0.0024069021455943584\n",
            " - layer3.bias grad norm: 0.000569970638025552\n",
            " - layer3_input.weight grad norm: 0.4826986789703369\n",
            " - layer3_input.bias grad norm: 0.000569970638025552\n",
            " - layer4.weight grad norm: 0.0028295167721807957\n",
            " - layer4.bias grad norm: 0.0005459355306811631\n",
            " - layer4_input.weight grad norm: 0.4636766314506531\n",
            " - layer4_input.bias grad norm: 0.0005459355306811631\n",
            " - layer5.weight grad norm: 0.011561164632439613\n",
            " - layer5.bias grad norm: 0.0049577695317566395\n",
            "Gradients at iteration 579:\n",
            " - layer1.weight grad norm: 0.5449884533882141\n",
            " - layer1.bias grad norm: 0.0006336956284940243\n",
            " - layer2.weight grad norm: 0.0018286403501406312\n",
            " - layer2.bias grad norm: 0.0005757326143793762\n",
            " - layer2_input.weight grad norm: 0.5006914734840393\n",
            " - layer2_input.bias grad norm: 0.0005757326143793762\n",
            " - layer3.weight grad norm: 0.0024938820861279964\n",
            " - layer3.bias grad norm: 0.0005726968520320952\n",
            " - layer3_input.weight grad norm: 0.49432069063186646\n",
            " - layer3_input.bias grad norm: 0.0005726968520320952\n",
            " - layer4.weight grad norm: 0.0029856290202587843\n",
            " - layer4.bias grad norm: 0.0005222461186349392\n",
            " - layer4_input.weight grad norm: 0.455802321434021\n",
            " - layer4_input.bias grad norm: 0.0005222461186349392\n",
            " - layer5.weight grad norm: 0.011829896830022335\n",
            " - layer5.bias grad norm: 0.005134334322065115\n",
            "Gradients at iteration 580:\n",
            " - layer1.weight grad norm: 0.5176368951797485\n",
            " - layer1.bias grad norm: 0.0005970355123281479\n",
            " - layer2.weight grad norm: 0.001736238831654191\n",
            " - layer2.bias grad norm: 0.000595427118241787\n",
            " - layer2_input.weight grad norm: 0.5118719935417175\n",
            " - layer2_input.bias grad norm: 0.000595427118241787\n",
            " - layer3.weight grad norm: 0.0024087908677756786\n",
            " - layer3.bias grad norm: 0.0005924690631218255\n",
            " - layer3_input.weight grad norm: 0.5103825926780701\n",
            " - layer3_input.bias grad norm: 0.0005924690631218255\n",
            " - layer4.weight grad norm: 0.0028556236065924168\n",
            " - layer4.bias grad norm: 0.0005291003617458045\n",
            " - layer4_input.weight grad norm: 0.457584410905838\n",
            " - layer4_input.bias grad norm: 0.0005291003617458045\n",
            " - layer5.weight grad norm: 0.011019795201718807\n",
            " - layer5.bias grad norm: 0.004938178230077028\n",
            "Gradients at iteration 581:\n",
            " - layer1.weight grad norm: 0.49237290024757385\n",
            " - layer1.bias grad norm: 0.0005652633844874799\n",
            " - layer2.weight grad norm: 0.001906920806504786\n",
            " - layer2.bias grad norm: 0.0005909041501581669\n",
            " - layer2_input.weight grad norm: 0.5094524025917053\n",
            " - layer2_input.bias grad norm: 0.0005909041501581669\n",
            " - layer3.weight grad norm: 0.002562756882980466\n",
            " - layer3.bias grad norm: 0.0005617052665911615\n",
            " - layer3_input.weight grad norm: 0.4864875078201294\n",
            " - layer3_input.bias grad norm: 0.0005617052665911615\n",
            " - layer4.weight grad norm: 0.003008025698363781\n",
            " - layer4.bias grad norm: 0.00059587846044451\n",
            " - layer4_input.weight grad norm: 0.5110374093055725\n",
            " - layer4_input.bias grad norm: 0.00059587846044451\n",
            " - layer5.weight grad norm: 0.01219917368143797\n",
            " - layer5.bias grad norm: 0.0052278065122663975\n",
            "Gradients at iteration 582:\n",
            " - layer1.weight grad norm: 0.5113641023635864\n",
            " - layer1.bias grad norm: 0.0005917968810535967\n",
            " - layer2.weight grad norm: 0.0017973025096580386\n",
            " - layer2.bias grad norm: 0.0005987074109725654\n",
            " - layer2_input.weight grad norm: 0.5159614086151123\n",
            " - layer2_input.bias grad norm: 0.0005987074109725654\n",
            " - layer3.weight grad norm: 0.002511229831725359\n",
            " - layer3.bias grad norm: 0.0005432519828900695\n",
            " - layer3_input.weight grad norm: 0.4721980094909668\n",
            " - layer3_input.bias grad norm: 0.0005432519828900695\n",
            " - layer4.weight grad norm: 0.0029796690214425325\n",
            " - layer4.bias grad norm: 0.000579615356400609\n",
            " - layer4_input.weight grad norm: 0.499135285615921\n",
            " - layer4_input.bias grad norm: 0.000579615356400609\n",
            " - layer5.weight grad norm: 0.011668825522065163\n",
            " - layer5.bias grad norm: 0.005157159641385078\n",
            "Gradients at iteration 583:\n",
            " - layer1.weight grad norm: 0.49733102321624756\n",
            " - layer1.bias grad norm: 0.0005681528709828854\n",
            " - layer2.weight grad norm: 0.0018899209098890424\n",
            " - layer2.bias grad norm: 0.0006035399273969233\n",
            " - layer2_input.weight grad norm: 0.5190301537513733\n",
            " - layer2_input.bias grad norm: 0.0006035399273969233\n",
            " - layer3.weight grad norm: 0.00256097549572587\n",
            " - layer3.bias grad norm: 0.0005516955279745162\n",
            " - layer3_input.weight grad norm: 0.47794046998023987\n",
            " - layer3_input.bias grad norm: 0.0005516955279745162\n",
            " - layer4.weight grad norm: 0.003007554681971669\n",
            " - layer4.bias grad norm: 0.0005900809774175286\n",
            " - layer4_input.weight grad norm: 0.5046021342277527\n",
            " - layer4_input.bias grad norm: 0.0005900809774175286\n",
            " - layer5.weight grad norm: 0.013016204349696636\n",
            " - layer5.bias grad norm: 0.0053040459752082825\n",
            "Gradients at iteration 584:\n",
            " - layer1.weight grad norm: 0.4835279881954193\n",
            " - layer1.bias grad norm: 0.0005526603199541569\n",
            " - layer2.weight grad norm: 0.0018405072623863816\n",
            " - layer2.bias grad norm: 0.0006177269387990236\n",
            " - layer2_input.weight grad norm: 0.5314699411392212\n",
            " - layer2_input.bias grad norm: 0.0006177269387990236\n",
            " - layer3.weight grad norm: 0.0025347000919282436\n",
            " - layer3.bias grad norm: 0.0005898209055885673\n",
            " - layer3_input.weight grad norm: 0.5053466558456421\n",
            " - layer3_input.bias grad norm: 0.0005898209055885673\n",
            " - layer4.weight grad norm: 0.002964342013001442\n",
            " - layer4.bias grad norm: 0.0005504231667146087\n",
            " - layer4_input.weight grad norm: 0.4776545464992523\n",
            " - layer4_input.bias grad norm: 0.0005504231667146087\n",
            " - layer5.weight grad norm: 0.012776242569088936\n",
            " - layer5.bias grad norm: 0.005211227107793093\n",
            "Gradients at iteration 585:\n",
            " - layer1.weight grad norm: 0.5423295497894287\n",
            " - layer1.bias grad norm: 0.0006309643504209816\n",
            " - layer2.weight grad norm: 0.001886365469545126\n",
            " - layer2.bias grad norm: 0.000588638533372432\n",
            " - layer2_input.weight grad norm: 0.5034505724906921\n",
            " - layer2_input.bias grad norm: 0.000588638533372432\n",
            " - layer3.weight grad norm: 0.0026323283091187477\n",
            " - layer3.bias grad norm: 0.0005065290606580675\n",
            " - layer3_input.weight grad norm: 0.4435230493545532\n",
            " - layer3_input.bias grad norm: 0.0005065290606580675\n",
            " - layer4.weight grad norm: 0.0031170593574643135\n",
            " - layer4.bias grad norm: 0.0005922074196860194\n",
            " - layer4_input.weight grad norm: 0.5054724812507629\n",
            " - layer4_input.bias grad norm: 0.0005922074196860194\n",
            " - layer5.weight grad norm: 0.012246541678905487\n",
            " - layer5.bias grad norm: 0.005360057577490807\n",
            "Gradients at iteration 586:\n",
            " - layer1.weight grad norm: 0.5525744557380676\n",
            " - layer1.bias grad norm: 0.0006558793247677386\n",
            " - layer2.weight grad norm: 0.0018475331598892808\n",
            " - layer2.bias grad norm: 0.0005525504238903522\n",
            " - layer2_input.weight grad norm: 0.47768890857696533\n",
            " - layer2_input.bias grad norm: 0.0005525504238903522\n",
            " - layer3.weight grad norm: 0.0025438705924898386\n",
            " - layer3.bias grad norm: 0.0005091314087621868\n",
            " - layer3_input.weight grad norm: 0.44346901774406433\n",
            " - layer3_input.bias grad norm: 0.0005091314087621868\n",
            " - layer4.weight grad norm: 0.0029835111927241087\n",
            " - layer4.bias grad norm: 0.0006142595666460693\n",
            " - layer4_input.weight grad norm: 0.5192668437957764\n",
            " - layer4_input.bias grad norm: 0.0006142595666460693\n",
            " - layer5.weight grad norm: 0.011103208176791668\n",
            " - layer5.bias grad norm: 0.005254026968032122\n",
            "Gradients at iteration 587:\n",
            " - layer1.weight grad norm: 0.5112187266349792\n",
            " - layer1.bias grad norm: 0.0005976746906526387\n",
            " - layer2.weight grad norm: 0.0017300050240010023\n",
            " - layer2.bias grad norm: 0.000634130323305726\n",
            " - layer2_input.weight grad norm: 0.5365700721740723\n",
            " - layer2_input.bias grad norm: 0.000634130323305726\n",
            " - layer3.weight grad norm: 0.002423361409455538\n",
            " - layer3.bias grad norm: 0.000552881567273289\n",
            " - layer3_input.weight grad norm: 0.47225528955459595\n",
            " - layer3_input.bias grad norm: 0.000552881567273289\n",
            " - layer4.weight grad norm: 0.00286332075484097\n",
            " - layer4.bias grad norm: 0.0005591634544543922\n",
            " - layer4_input.weight grad norm: 0.4770375192165375\n",
            " - layer4_input.bias grad norm: 0.0005591634544543922\n",
            " - layer5.weight grad norm: 0.010679271072149277\n",
            " - layer5.bias grad norm: 0.004952683579176664\n",
            "Gradients at iteration 588:\n",
            " - layer1.weight grad norm: 0.5460602641105652\n",
            " - layer1.bias grad norm: 0.0006442618323490024\n",
            " - layer2.weight grad norm: 0.0019032658310607076\n",
            " - layer2.bias grad norm: 0.000569600728340447\n",
            " - layer2_input.weight grad norm: 0.49090054631233215\n",
            " - layer2_input.bias grad norm: 0.000569600728340447\n",
            " - layer3.weight grad norm: 0.002625921741127968\n",
            " - layer3.bias grad norm: 0.0005717084277421236\n",
            " - layer3_input.weight grad norm: 0.48967158794403076\n",
            " - layer3_input.bias grad norm: 0.0005717084277421236\n",
            " - layer4.weight grad norm: 0.0030064862221479416\n",
            " - layer4.bias grad norm: 0.0005453708581626415\n",
            " - layer4_input.weight grad norm: 0.4699636995792389\n",
            " - layer4_input.bias grad norm: 0.0005453708581626415\n",
            " - layer5.weight grad norm: 0.011833611875772476\n",
            " - layer5.bias grad norm: 0.00535109918564558\n",
            "Gradients at iteration 589:\n",
            " - layer1.weight grad norm: 0.5367974042892456\n",
            " - layer1.bias grad norm: 0.0006224365788511932\n",
            " - layer2.weight grad norm: 0.0018750142771750689\n",
            " - layer2.bias grad norm: 0.0005727519746869802\n",
            " - layer2_input.weight grad norm: 0.49408063292503357\n",
            " - layer2_input.bias grad norm: 0.0005727519746869802\n",
            " - layer3.weight grad norm: 0.0025840571615844965\n",
            " - layer3.bias grad norm: 0.0005951926577836275\n",
            " - layer3_input.weight grad norm: 0.5101099610328674\n",
            " - layer3_input.bias grad norm: 0.0005951926577836275\n",
            " - layer4.weight grad norm: 0.0030357874929904938\n",
            " - layer4.bias grad norm: 0.0005219276063144207\n",
            " - layer4_input.weight grad norm: 0.45532554388046265\n",
            " - layer4_input.bias grad norm: 0.0005219276063144207\n",
            " - layer5.weight grad norm: 0.012220863252878189\n",
            " - layer5.bias grad norm: 0.005319761112332344\n",
            "Gradients at iteration 590:\n",
            " - layer1.weight grad norm: 0.5454756021499634\n",
            " - layer1.bias grad norm: 0.0006351244519464672\n",
            " - layer2.weight grad norm: 0.0018838386749848723\n",
            " - layer2.bias grad norm: 0.000615537166595459\n",
            " - layer2_input.weight grad norm: 0.5233401656150818\n",
            " - layer2_input.bias grad norm: 0.000615537166595459\n",
            " - layer3.weight grad norm: 0.002605717396363616\n",
            " - layer3.bias grad norm: 0.0005568013293668628\n",
            " - layer3_input.weight grad norm: 0.47833606600761414\n",
            " - layer3_input.bias grad norm: 0.0005568013293668628\n",
            " - layer4.weight grad norm: 0.003105713753029704\n",
            " - layer4.bias grad norm: 0.0005131308571435511\n",
            " - layer4_input.weight grad norm: 0.4467388987541199\n",
            " - layer4_input.bias grad norm: 0.0005131308571435511\n",
            " - layer5.weight grad norm: 0.011800697073340416\n",
            " - layer5.bias grad norm: 0.005370528437197208\n",
            "Gradients at iteration 591:\n",
            " - layer1.weight grad norm: 0.5416696071624756\n",
            " - layer1.bias grad norm: 0.0006271488382481039\n",
            " - layer2.weight grad norm: 0.0017185298493131995\n",
            " - layer2.bias grad norm: 0.0006349239847622812\n",
            " - layer2_input.weight grad norm: 0.5420931577682495\n",
            " - layer2_input.bias grad norm: 0.0006349239847622812\n",
            " - layer3.weight grad norm: 0.00239608739502728\n",
            " - layer3.bias grad norm: 0.0005386347183957696\n",
            " - layer3_input.weight grad norm: 0.4625663161277771\n",
            " - layer3_input.bias grad norm: 0.0005386347183957696\n",
            " - layer4.weight grad norm: 0.002868755953386426\n",
            " - layer4.bias grad norm: 0.0005169648211449385\n",
            " - layer4_input.weight grad norm: 0.4456389844417572\n",
            " - layer4_input.bias grad norm: 0.0005169648211449385\n",
            " - layer5.weight grad norm: 0.011126423254609108\n",
            " - layer5.bias grad norm: 0.004932645242661238\n",
            "Gradients at iteration 592:\n",
            " - layer1.weight grad norm: 0.4678207337856293\n",
            " - layer1.bias grad norm: 0.0005266470252536237\n",
            " - layer2.weight grad norm: 0.0019252150086686015\n",
            " - layer2.bias grad norm: 0.0006074457778595388\n",
            " - layer2_input.weight grad norm: 0.5227743983268738\n",
            " - layer2_input.bias grad norm: 0.0006074457778595388\n",
            " - layer3.weight grad norm: 0.0026449428405612707\n",
            " - layer3.bias grad norm: 0.0005840791272930801\n",
            " - layer3_input.weight grad norm: 0.5033891201019287\n",
            " - layer3_input.bias grad norm: 0.0005840791272930801\n",
            " - layer4.weight grad norm: 0.0031855774577707052\n",
            " - layer4.bias grad norm: 0.0005861938116140664\n",
            " - layer4_input.weight grad norm: 0.5042152404785156\n",
            " - layer4_input.bias grad norm: 0.0005861938116140664\n",
            " - layer5.weight grad norm: 0.012807073071599007\n",
            " - layer5.bias grad norm: 0.005477954167872667\n",
            "Gradients at iteration 593:\n",
            " - layer1.weight grad norm: 0.5880911350250244\n",
            " - layer1.bias grad norm: 0.0006859882269054651\n",
            " - layer2.weight grad norm: 0.0018706502160057425\n",
            " - layer2.bias grad norm: 0.0005104573210701346\n",
            " - layer2_input.weight grad norm: 0.4563062787055969\n",
            " - layer2_input.bias grad norm: 0.0005104573210701346\n",
            " - layer3.weight grad norm: 0.002604859648272395\n",
            " - layer3.bias grad norm: 0.0005661948816850781\n",
            " - layer3_input.weight grad norm: 0.49232882261276245\n",
            " - layer3_input.bias grad norm: 0.0005661948816850781\n",
            " - layer4.weight grad norm: 0.0030646182131022215\n",
            " - layer4.bias grad norm: 0.0005081267445348203\n",
            " - layer4_input.weight grad norm: 0.4509177803993225\n",
            " - layer4_input.bias grad norm: 0.0005081267445348203\n",
            " - layer5.weight grad norm: 0.013002242892980576\n",
            " - layer5.bias grad norm: 0.005276649724692106\n",
            "Gradients at iteration 594:\n",
            " - layer1.weight grad norm: 0.5068181753158569\n",
            " - layer1.bias grad norm: 0.0005814617616124451\n",
            " - layer2.weight grad norm: 0.0018174303695559502\n",
            " - layer2.bias grad norm: 0.0005832004244439304\n",
            " - layer2_input.weight grad norm: 0.5046745538711548\n",
            " - layer2_input.bias grad norm: 0.0005832004244439304\n",
            " - layer3.weight grad norm: 0.0025160969235002995\n",
            " - layer3.bias grad norm: 0.0005851777968928218\n",
            " - layer3_input.weight grad norm: 0.5030813813209534\n",
            " - layer3_input.bias grad norm: 0.0005851777968928218\n",
            " - layer4.weight grad norm: 0.002975080395117402\n",
            " - layer4.bias grad norm: 0.0005582602461799979\n",
            " - layer4_input.weight grad norm: 0.4849430024623871\n",
            " - layer4_input.bias grad norm: 0.0005582602461799979\n",
            " - layer5.weight grad norm: 0.011456781066954136\n",
            " - layer5.bias grad norm: 0.005127911455929279\n",
            "Gradients at iteration 595:\n",
            " - layer1.weight grad norm: 0.5079732537269592\n",
            " - layer1.bias grad norm: 0.0005770957213826478\n",
            " - layer2.weight grad norm: 0.0018122723558917642\n",
            " - layer2.bias grad norm: 0.0006393282674252987\n",
            " - layer2_input.weight grad norm: 0.546299934387207\n",
            " - layer2_input.bias grad norm: 0.0006393282674252987\n",
            " - layer3.weight grad norm: 0.002442648634314537\n",
            " - layer3.bias grad norm: 0.0005595030961558223\n",
            " - layer3_input.weight grad norm: 0.48335909843444824\n",
            " - layer3_input.bias grad norm: 0.0005595030961558223\n",
            " - layer4.weight grad norm: 0.002943781204521656\n",
            " - layer4.bias grad norm: 0.0005224479245953262\n",
            " - layer4_input.weight grad norm: 0.4579162001609802\n",
            " - layer4_input.bias grad norm: 0.0005224479245953262\n",
            " - layer5.weight grad norm: 0.012256662361323833\n",
            " - layer5.bias grad norm: 0.005080133676528931\n",
            "Gradients at iteration 596:\n",
            " - layer1.weight grad norm: 0.5110474228858948\n",
            " - layer1.bias grad norm: 0.0005806167027913034\n",
            " - layer2.weight grad norm: 0.0019005972426384687\n",
            " - layer2.bias grad norm: 0.0005584058817476034\n",
            " - layer2_input.weight grad norm: 0.4934520125389099\n",
            " - layer2_input.bias grad norm: 0.0005584058817476034\n",
            " - layer3.weight grad norm: 0.0026340109761804342\n",
            " - layer3.bias grad norm: 0.0006249465513974428\n",
            " - layer3_input.weight grad norm: 0.537872314453125\n",
            " - layer3_input.bias grad norm: 0.0006249465513974428\n",
            " - layer4.weight grad norm: 0.0031207846477627754\n",
            " - layer4.bias grad norm: 0.0005170092335902154\n",
            " - layer4_input.weight grad norm: 0.4536799490451813\n",
            " - layer4_input.bias grad norm: 0.0005170092335902154\n",
            " - layer5.weight grad norm: 0.012319782748818398\n",
            " - layer5.bias grad norm: 0.00541040999814868\n",
            "Gradients at iteration 597:\n",
            " - layer1.weight grad norm: 0.5052485466003418\n",
            " - layer1.bias grad norm: 0.0005814020987600088\n",
            " - layer2.weight grad norm: 0.001901897368952632\n",
            " - layer2.bias grad norm: 0.0006347941234707832\n",
            " - layer2_input.weight grad norm: 0.5444106459617615\n",
            " - layer2_input.bias grad norm: 0.0006347941234707832\n",
            " - layer3.weight grad norm: 0.002615382196381688\n",
            " - layer3.bias grad norm: 0.0005461138789542019\n",
            " - layer3_input.weight grad norm: 0.47911933064460754\n",
            " - layer3_input.bias grad norm: 0.0005461138789542019\n",
            " - layer4.weight grad norm: 0.003017219016328454\n",
            " - layer4.bias grad norm: 0.0005435312050394714\n",
            " - layer4_input.weight grad norm: 0.46753761172294617\n",
            " - layer4_input.bias grad norm: 0.0005435312050394714\n",
            " - layer5.weight grad norm: 0.01202605664730072\n",
            " - layer5.bias grad norm: 0.005271449685096741\n",
            "Gradients at iteration 598:\n",
            " - layer1.weight grad norm: 0.5391178727149963\n",
            " - layer1.bias grad norm: 0.0006221082876436412\n",
            " - layer2.weight grad norm: 0.001915385597385466\n",
            " - layer2.bias grad norm: 0.0005904216086491942\n",
            " - layer2_input.weight grad norm: 0.5155424475669861\n",
            " - layer2_input.bias grad norm: 0.0005904216086491942\n",
            " - layer3.weight grad norm: 0.002612267155200243\n",
            " - layer3.bias grad norm: 0.0005384409450925887\n",
            " - layer3_input.weight grad norm: 0.47358229756355286\n",
            " - layer3_input.bias grad norm: 0.0005384409450925887\n",
            " - layer4.weight grad norm: 0.003114516381174326\n",
            " - layer4.bias grad norm: 0.0005362886004149914\n",
            " - layer4_input.weight grad norm: 0.4680921733379364\n",
            " - layer4_input.bias grad norm: 0.0005362886004149914\n",
            " - layer5.weight grad norm: 0.011203133501112461\n",
            " - layer5.bias grad norm: 0.005435904022306204\n",
            "Gradients at iteration 599:\n",
            " - layer1.weight grad norm: 0.5000524520874023\n",
            " - layer1.bias grad norm: 0.0005840472294948995\n",
            " - layer2.weight grad norm: 0.0018458124250173569\n",
            " - layer2.bias grad norm: 0.0005639035953208804\n",
            " - layer2_input.weight grad norm: 0.4842645525932312\n",
            " - layer2_input.bias grad norm: 0.0005639035953208804\n",
            " - layer3.weight grad norm: 0.0025413655675947666\n",
            " - layer3.bias grad norm: 0.0005996268009766936\n",
            " - layer3_input.weight grad norm: 0.5082488656044006\n",
            " - layer3_input.bias grad norm: 0.0005996268009766936\n",
            " - layer4.weight grad norm: 0.0030053267255425453\n",
            " - layer4.bias grad norm: 0.0005959640257060528\n",
            " - layer4_input.weight grad norm: 0.5068967342376709\n",
            " - layer4_input.bias grad norm: 0.0005959640257060528\n",
            " - layer5.weight grad norm: 0.011216768063604832\n",
            " - layer5.bias grad norm: 0.005199951585382223\n",
            "Gradients at iteration 600:\n",
            " - layer1.weight grad norm: 0.48698219656944275\n",
            " - layer1.bias grad norm: 0.0005592659581452608\n",
            " - layer2.weight grad norm: 0.0017004694091156125\n",
            " - layer2.bias grad norm: 0.0005775505560450256\n",
            " - layer2_input.weight grad norm: 0.4971121549606323\n",
            " - layer2_input.bias grad norm: 0.0005775505560450256\n",
            " - layer3.weight grad norm: 0.0023463177494704723\n",
            " - layer3.bias grad norm: 0.0005872236215509474\n",
            " - layer3_input.weight grad norm: 0.5015478134155273\n",
            " - layer3_input.bias grad norm: 0.0005872236215509474\n",
            " - layer4.weight grad norm: 0.0027959542348980904\n",
            " - layer4.bias grad norm: 0.0006033210083842278\n",
            " - layer4_input.weight grad norm: 0.5138077139854431\n",
            " - layer4_input.bias grad norm: 0.0006033210083842278\n",
            " - layer5.weight grad norm: 0.011712823063135147\n",
            " - layer5.bias grad norm: 0.004830744583159685\n",
            "It: 600, Loss: 5.546e+13, Y0: 0.398, Time: 1.64, Learning Rate: 1.000e-03\n",
            "Gradients at iteration 601:\n",
            " - layer1.weight grad norm: 0.5257585644721985\n",
            " - layer1.bias grad norm: 0.0006085550994612277\n",
            " - layer2.weight grad norm: 0.0018553913105279207\n",
            " - layer2.bias grad norm: 0.0006232439773157239\n",
            " - layer2_input.weight grad norm: 0.5389580726623535\n",
            " - layer2_input.bias grad norm: 0.0006232439773157239\n",
            " - layer3.weight grad norm: 0.002582963090389967\n",
            " - layer3.bias grad norm: 0.0005775853060185909\n",
            " - layer3_input.weight grad norm: 0.4986191689968109\n",
            " - layer3_input.bias grad norm: 0.0005775853060185909\n",
            " - layer4.weight grad norm: 0.0030814942438155413\n",
            " - layer4.bias grad norm: 0.0004829141544178128\n",
            " - layer4_input.weight grad norm: 0.4292837977409363\n",
            " - layer4_input.bias grad norm: 0.0004829141544178128\n",
            " - layer5.weight grad norm: 0.012105956673622131\n",
            " - layer5.bias grad norm: 0.005289027933031321\n",
            "Gradients at iteration 602:\n",
            " - layer1.weight grad norm: 0.5010478496551514\n",
            " - layer1.bias grad norm: 0.0005810249713249505\n",
            " - layer2.weight grad norm: 0.0018208245746791363\n",
            " - layer2.bias grad norm: 0.0006166202947497368\n",
            " - layer2_input.weight grad norm: 0.5237613916397095\n",
            " - layer2_input.bias grad norm: 0.0006166202947497368\n",
            " - layer3.weight grad norm: 0.00249934708699584\n",
            " - layer3.bias grad norm: 0.0006266916170716286\n",
            " - layer3_input.weight grad norm: 0.5290076732635498\n",
            " - layer3_input.bias grad norm: 0.0006266916170716286\n",
            " - layer4.weight grad norm: 0.002963924314826727\n",
            " - layer4.bias grad norm: 0.0005109839257784188\n",
            " - layer4_input.weight grad norm: 0.44111961126327515\n",
            " - layer4_input.bias grad norm: 0.0005109839257784188\n",
            " - layer5.weight grad norm: 0.01193155162036419\n",
            " - layer5.bias grad norm: 0.005138189531862736\n",
            "Gradients at iteration 603:\n",
            " - layer1.weight grad norm: 0.5408572554588318\n",
            " - layer1.bias grad norm: 0.0006382273859344423\n",
            " - layer2.weight grad norm: 0.001956205815076828\n",
            " - layer2.bias grad norm: 0.0005654155393131077\n",
            " - layer2_input.weight grad norm: 0.48494383692741394\n",
            " - layer2_input.bias grad norm: 0.0005654155393131077\n",
            " - layer3.weight grad norm: 0.0027430648915469646\n",
            " - layer3.bias grad norm: 0.0005969738704152405\n",
            " - layer3_input.weight grad norm: 0.5094592571258545\n",
            " - layer3_input.bias grad norm: 0.0005969738704152405\n",
            " - layer4.weight grad norm: 0.0032206582836806774\n",
            " - layer4.bias grad norm: 0.0005360757932066917\n",
            " - layer4_input.weight grad norm: 0.4610360860824585\n",
            " - layer4_input.bias grad norm: 0.0005360757932066917\n",
            " - layer5.weight grad norm: 0.01204310916364193\n",
            " - layer5.bias grad norm: 0.005545463413000107\n",
            "Gradients at iteration 604:\n",
            " - layer1.weight grad norm: 0.5249007940292358\n",
            " - layer1.bias grad norm: 0.0006056915735825896\n",
            " - layer2.weight grad norm: 0.001815436058677733\n",
            " - layer2.bias grad norm: 0.0005998069536872208\n",
            " - layer2_input.weight grad norm: 0.5176664590835571\n",
            " - layer2_input.bias grad norm: 0.0005998069536872208\n",
            " - layer3.weight grad norm: 0.0024813967756927013\n",
            " - layer3.bias grad norm: 0.0005910985637456179\n",
            " - layer3_input.weight grad norm: 0.5057504773139954\n",
            " - layer3_input.bias grad norm: 0.0005910985637456179\n",
            " - layer4.weight grad norm: 0.0029477740172296762\n",
            " - layer4.bias grad norm: 0.0005132223013788462\n",
            " - layer4_input.weight grad norm: 0.4478065073490143\n",
            " - layer4_input.bias grad norm: 0.0005132223013788462\n",
            " - layer5.weight grad norm: 0.011819162406027317\n",
            " - layer5.bias grad norm: 0.005117669235914946\n",
            "Gradients at iteration 605:\n",
            " - layer1.weight grad norm: 0.5505757927894592\n",
            " - layer1.bias grad norm: 0.0006439690478146076\n",
            " - layer2.weight grad norm: 0.0018229203997179866\n",
            " - layer2.bias grad norm: 0.0005372380837798119\n",
            " - layer2_input.weight grad norm: 0.4699835479259491\n",
            " - layer2_input.bias grad norm: 0.0005372380837798119\n",
            " - layer3.weight grad norm: 0.002455899491906166\n",
            " - layer3.bias grad norm: 0.000602173269726336\n",
            " - layer3_input.weight grad norm: 0.5131242275238037\n",
            " - layer3_input.bias grad norm: 0.000602173269726336\n",
            " - layer4.weight grad norm: 0.002920548664405942\n",
            " - layer4.bias grad norm: 0.0005353394662961364\n",
            " - layer4_input.weight grad norm: 0.4609776735305786\n",
            " - layer4_input.bias grad norm: 0.0005353394662961364\n",
            " - layer5.weight grad norm: 0.011744752526283264\n",
            " - layer5.bias grad norm: 0.005164029076695442\n",
            "Gradients at iteration 606:\n",
            " - layer1.weight grad norm: 0.5377746224403381\n",
            " - layer1.bias grad norm: 0.0006272837053984404\n",
            " - layer2.weight grad norm: 0.0018828093307092786\n",
            " - layer2.bias grad norm: 0.0005894351634196937\n",
            " - layer2_input.weight grad norm: 0.5058495998382568\n",
            " - layer2_input.bias grad norm: 0.0005894351634196937\n",
            " - layer3.weight grad norm: 0.002632414223626256\n",
            " - layer3.bias grad norm: 0.000582120381295681\n",
            " - layer3_input.weight grad norm: 0.49741706252098083\n",
            " - layer3_input.bias grad norm: 0.000582120381295681\n",
            " - layer4.weight grad norm: 0.003133603371679783\n",
            " - layer4.bias grad norm: 0.0005164905451238155\n",
            " - layer4_input.weight grad norm: 0.45527371764183044\n",
            " - layer4_input.bias grad norm: 0.0005164905451238155\n",
            " - layer5.weight grad norm: 0.012848061509430408\n",
            " - layer5.bias grad norm: 0.005383817944675684\n",
            "Gradients at iteration 607:\n",
            " - layer1.weight grad norm: 0.5023069977760315\n",
            " - layer1.bias grad norm: 0.0005734855658374727\n",
            " - layer2.weight grad norm: 0.0019214584026485682\n",
            " - layer2.bias grad norm: 0.0005724771763198078\n",
            " - layer2_input.weight grad norm: 0.4939527213573456\n",
            " - layer2_input.bias grad norm: 0.0005724771763198078\n",
            " - layer3.weight grad norm: 0.002619050443172455\n",
            " - layer3.bias grad norm: 0.0005728008109144866\n",
            " - layer3_input.weight grad norm: 0.49516725540161133\n",
            " - layer3_input.bias grad norm: 0.0005728008109144866\n",
            " - layer4.weight grad norm: 0.003091612132266164\n",
            " - layer4.bias grad norm: 0.0005932852509431541\n",
            " - layer4_input.weight grad norm: 0.5082554221153259\n",
            " - layer4_input.bias grad norm: 0.0005932852509431541\n",
            " - layer5.weight grad norm: 0.011539255268871784\n",
            " - layer5.bias grad norm: 0.005361372139304876\n",
            "Gradients at iteration 608:\n",
            " - layer1.weight grad norm: 0.5106257796287537\n",
            " - layer1.bias grad norm: 0.0005838268552906811\n",
            " - layer2.weight grad norm: 0.0018838816322386265\n",
            " - layer2.bias grad norm: 0.0006128947134129703\n",
            " - layer2_input.weight grad norm: 0.5330571532249451\n",
            " - layer2_input.bias grad norm: 0.0006128947134129703\n",
            " - layer3.weight grad norm: 0.0026158879045397043\n",
            " - layer3.bias grad norm: 0.0005523649742826819\n",
            " - layer3_input.weight grad norm: 0.48384562134742737\n",
            " - layer3_input.bias grad norm: 0.0005523649742826819\n",
            " - layer4.weight grad norm: 0.0031250715255737305\n",
            " - layer4.bias grad norm: 0.0005429266020655632\n",
            " - layer4_input.weight grad norm: 0.4698934853076935\n",
            " - layer4_input.bias grad norm: 0.0005429266020655632\n",
            " - layer5.weight grad norm: 0.012406757101416588\n",
            " - layer5.bias grad norm: 0.005339732393622398\n",
            "Gradients at iteration 609:\n",
            " - layer1.weight grad norm: 0.5718072652816772\n",
            " - layer1.bias grad norm: 0.0006709604640491307\n",
            " - layer2.weight grad norm: 0.0019316822290420532\n",
            " - layer2.bias grad norm: 0.0005283445934765041\n",
            " - layer2_input.weight grad norm: 0.4671577513217926\n",
            " - layer2_input.bias grad norm: 0.0005283445934765041\n",
            " - layer3.weight grad norm: 0.0026522837579250336\n",
            " - layer3.bias grad norm: 0.0005352930165827274\n",
            " - layer3_input.weight grad norm: 0.467311292886734\n",
            " - layer3_input.bias grad norm: 0.0005352930165827274\n",
            " - layer4.weight grad norm: 0.0031599418725818396\n",
            " - layer4.bias grad norm: 0.0005629987572319806\n",
            " - layer4_input.weight grad norm: 0.48603355884552\n",
            " - layer4_input.bias grad norm: 0.0005629987572319806\n",
            " - layer5.weight grad norm: 0.011755436658859253\n",
            " - layer5.bias grad norm: 0.005522575229406357\n",
            "Gradients at iteration 610:\n",
            " - layer1.weight grad norm: 0.48913705348968506\n",
            " - layer1.bias grad norm: 0.0005642951000481844\n",
            " - layer2.weight grad norm: 0.0018635038286447525\n",
            " - layer2.bias grad norm: 0.0005524969310499728\n",
            " - layer2_input.weight grad norm: 0.4800044000148773\n",
            " - layer2_input.bias grad norm: 0.0005524969310499728\n",
            " - layer3.weight grad norm: 0.0025175102055072784\n",
            " - layer3.bias grad norm: 0.0005951460916548967\n",
            " - layer3_input.weight grad norm: 0.508854329586029\n",
            " - layer3_input.bias grad norm: 0.0005951460916548967\n",
            " - layer4.weight grad norm: 0.003028622129932046\n",
            " - layer4.bias grad norm: 0.00060888851294294\n",
            " - layer4_input.weight grad norm: 0.520779550075531\n",
            " - layer4_input.bias grad norm: 0.00060888851294294\n",
            " - layer5.weight grad norm: 0.012172520160675049\n",
            " - layer5.bias grad norm: 0.0051985615864396095\n",
            "Gradients at iteration 611:\n",
            " - layer1.weight grad norm: 0.5643659234046936\n",
            " - layer1.bias grad norm: 0.000660136342048645\n",
            " - layer2.weight grad norm: 0.001728684757836163\n",
            " - layer2.bias grad norm: 0.000593624368775636\n",
            " - layer2_input.weight grad norm: 0.5146133899688721\n",
            " - layer2_input.bias grad norm: 0.000593624368775636\n",
            " - layer3.weight grad norm: 0.002402621554210782\n",
            " - layer3.bias grad norm: 0.0005184070323593915\n",
            " - layer3_input.weight grad norm: 0.4562153220176697\n",
            " - layer3_input.bias grad norm: 0.0005184070323593915\n",
            " - layer4.weight grad norm: 0.0028677235823124647\n",
            " - layer4.bias grad norm: 0.0005237545119598508\n",
            " - layer4_input.weight grad norm: 0.4564746618270874\n",
            " - layer4_input.bias grad norm: 0.0005237545119598508\n",
            " - layer5.weight grad norm: 0.010911901481449604\n",
            " - layer5.bias grad norm: 0.004944989457726479\n",
            "Gradients at iteration 612:\n",
            " - layer1.weight grad norm: 0.556325376033783\n",
            " - layer1.bias grad norm: 0.0006537023582495749\n",
            " - layer2.weight grad norm: 0.0017469681333750486\n",
            " - layer2.bias grad norm: 0.000596132013015449\n",
            " - layer2_input.weight grad norm: 0.5077957510948181\n",
            " - layer2_input.bias grad norm: 0.000596132013015449\n",
            " - layer3.weight grad norm: 0.0023933006450533867\n",
            " - layer3.bias grad norm: 0.000547718838788569\n",
            " - layer3_input.weight grad norm: 0.4736979007720947\n",
            " - layer3_input.bias grad norm: 0.000547718838788569\n",
            " - layer4.weight grad norm: 0.0028439892921596766\n",
            " - layer4.bias grad norm: 0.0005288337706588209\n",
            " - layer4_input.weight grad norm: 0.45616358518600464\n",
            " - layer4_input.bias grad norm: 0.0005288337706588209\n",
            " - layer5.weight grad norm: 0.011266592890024185\n",
            " - layer5.bias grad norm: 0.004959178622812033\n",
            "Gradients at iteration 613:\n",
            " - layer1.weight grad norm: 0.5312374830245972\n",
            " - layer1.bias grad norm: 0.0006201521609909832\n",
            " - layer2.weight grad norm: 0.001871087122708559\n",
            " - layer2.bias grad norm: 0.0005953441723249853\n",
            " - layer2_input.weight grad norm: 0.508192241191864\n",
            " - layer2_input.bias grad norm: 0.0005953441723249853\n",
            " - layer3.weight grad norm: 0.0025851752143353224\n",
            " - layer3.bias grad norm: 0.0005446043214760721\n",
            " - layer3_input.weight grad norm: 0.47006624937057495\n",
            " - layer3_input.bias grad norm: 0.0005446043214760721\n",
            " - layer4.weight grad norm: 0.0030628186650574207\n",
            " - layer4.bias grad norm: 0.0005720257759094238\n",
            " - layer4_input.weight grad norm: 0.48822489380836487\n",
            " - layer4_input.bias grad norm: 0.0005720257759094238\n",
            " - layer5.weight grad norm: 0.012287596240639687\n",
            " - layer5.bias grad norm: 0.005346593912690878\n",
            "Gradients at iteration 614:\n",
            " - layer1.weight grad norm: 0.5481358766555786\n",
            " - layer1.bias grad norm: 0.000636471901088953\n",
            " - layer2.weight grad norm: 0.0018412919016554952\n",
            " - layer2.bias grad norm: 0.0005734768928959966\n",
            " - layer2_input.weight grad norm: 0.4962378144264221\n",
            " - layer2_input.bias grad norm: 0.0005734768928959966\n",
            " - layer3.weight grad norm: 0.002569710137322545\n",
            " - layer3.bias grad norm: 0.0005556780961342156\n",
            " - layer3_input.weight grad norm: 0.4829980432987213\n",
            " - layer3_input.bias grad norm: 0.0005556780961342156\n",
            " - layer4.weight grad norm: 0.002984486287459731\n",
            " - layer4.bias grad norm: 0.0005372632876969874\n",
            " - layer4_input.weight grad norm: 0.4688577950000763\n",
            " - layer4_input.bias grad norm: 0.0005372632876969874\n",
            " - layer5.weight grad norm: 0.011487236246466637\n",
            " - layer5.bias grad norm: 0.005225314758718014\n",
            "Gradients at iteration 615:\n",
            " - layer1.weight grad norm: 0.5743842720985413\n",
            " - layer1.bias grad norm: 0.0006781714037060738\n",
            " - layer2.weight grad norm: 0.0018240209901705384\n",
            " - layer2.bias grad norm: 0.0005606168997474015\n",
            " - layer2_input.weight grad norm: 0.48300814628601074\n",
            " - layer2_input.bias grad norm: 0.0005606168997474015\n",
            " - layer3.weight grad norm: 0.0025142645463347435\n",
            " - layer3.bias grad norm: 0.0005232927505858243\n",
            " - layer3_input.weight grad norm: 0.45458731055259705\n",
            " - layer3_input.bias grad norm: 0.0005232927505858243\n",
            " - layer4.weight grad norm: 0.002958758268505335\n",
            " - layer4.bias grad norm: 0.0005621560267172754\n",
            " - layer4_input.weight grad norm: 0.47953933477401733\n",
            " - layer4_input.bias grad norm: 0.0005621560267172754\n",
            " - layer5.weight grad norm: 0.01143674273043871\n",
            " - layer5.bias grad norm: 0.0051758685149252415\n",
            "Gradients at iteration 616:\n",
            " - layer1.weight grad norm: 0.5527076721191406\n",
            " - layer1.bias grad norm: 0.0006458834977820516\n",
            " - layer2.weight grad norm: 0.0019337841076776385\n",
            " - layer2.bias grad norm: 0.0005639828741550446\n",
            " - layer2_input.weight grad norm: 0.49248677492141724\n",
            " - layer2_input.bias grad norm: 0.0005639828741550446\n",
            " - layer3.weight grad norm: 0.0026307636871933937\n",
            " - layer3.bias grad norm: 0.0005761008360423148\n",
            " - layer3_input.weight grad norm: 0.49644139409065247\n",
            " - layer3_input.bias grad norm: 0.0005761008360423148\n",
            " - layer4.weight grad norm: 0.0030829082243144512\n",
            " - layer4.bias grad norm: 0.0005237334407866001\n",
            " - layer4_input.weight grad norm: 0.45312270522117615\n",
            " - layer4_input.bias grad norm: 0.0005237334407866001\n",
            " - layer5.weight grad norm: 0.012054064311087132\n",
            " - layer5.bias grad norm: 0.005399269517511129\n",
            "Gradients at iteration 617:\n",
            " - layer1.weight grad norm: 0.5105158686637878\n",
            " - layer1.bias grad norm: 0.0005936786765232682\n",
            " - layer2.weight grad norm: 0.0018365518189966679\n",
            " - layer2.bias grad norm: 0.0006189441191963851\n",
            " - layer2_input.weight grad norm: 0.5268064141273499\n",
            " - layer2_input.bias grad norm: 0.0006189441191963851\n",
            " - layer3.weight grad norm: 0.0025011629331856966\n",
            " - layer3.bias grad norm: 0.0005991907673887908\n",
            " - layer3_input.weight grad norm: 0.5058716535568237\n",
            " - layer3_input.bias grad norm: 0.0005991907673887908\n",
            " - layer4.weight grad norm: 0.0030039302073419094\n",
            " - layer4.bias grad norm: 0.0005253386334516108\n",
            " - layer4_input.weight grad norm: 0.4536329209804535\n",
            " - layer4_input.bias grad norm: 0.0005253386334516108\n",
            " - layer5.weight grad norm: 0.010570590384304523\n",
            " - layer5.bias grad norm: 0.005169680342078209\n",
            "Gradients at iteration 618:\n",
            " - layer1.weight grad norm: 0.5210270881652832\n",
            " - layer1.bias grad norm: 0.0006080488092266023\n",
            " - layer2.weight grad norm: 0.0018122688634321094\n",
            " - layer2.bias grad norm: 0.0005696035223081708\n",
            " - layer2_input.weight grad norm: 0.48927581310272217\n",
            " - layer2_input.bias grad norm: 0.0005696035223081708\n",
            " - layer3.weight grad norm: 0.0025400288868695498\n",
            " - layer3.bias grad norm: 0.0005459895473904908\n",
            " - layer3_input.weight grad norm: 0.4704478085041046\n",
            " - layer3_input.bias grad norm: 0.0005459895473904908\n",
            " - layer4.weight grad norm: 0.003013049950823188\n",
            " - layer4.bias grad norm: 0.000608942355029285\n",
            " - layer4_input.weight grad norm: 0.5173315405845642\n",
            " - layer4_input.bias grad norm: 0.000608942355029285\n",
            " - layer5.weight grad norm: 0.01177329383790493\n",
            " - layer5.bias grad norm: 0.00519458157941699\n",
            "Gradients at iteration 619:\n",
            " - layer1.weight grad norm: 0.5347773432731628\n",
            " - layer1.bias grad norm: 0.0006264870171435177\n",
            " - layer2.weight grad norm: 0.0018465748289600015\n",
            " - layer2.bias grad norm: 0.0006237372290343046\n",
            " - layer2_input.weight grad norm: 0.5341005921363831\n",
            " - layer2_input.bias grad norm: 0.0006237372290343046\n",
            " - layer3.weight grad norm: 0.002518645953387022\n",
            " - layer3.bias grad norm: 0.000523050723131746\n",
            " - layer3_input.weight grad norm: 0.45720604062080383\n",
            " - layer3_input.bias grad norm: 0.000523050723131746\n",
            " - layer4.weight grad norm: 0.0029480692464858294\n",
            " - layer4.bias grad norm: 0.0005483152344822884\n",
            " - layer4_input.weight grad norm: 0.46854913234710693\n",
            " - layer4_input.bias grad norm: 0.0005483152344822884\n",
            " - layer5.weight grad norm: 0.011232691816985607\n",
            " - layer5.bias grad norm: 0.005194473080337048\n",
            "Gradients at iteration 620:\n",
            " - layer1.weight grad norm: 0.5290848612785339\n",
            " - layer1.bias grad norm: 0.0006160176126286387\n",
            " - layer2.weight grad norm: 0.001764392596669495\n",
            " - layer2.bias grad norm: 0.0005621719756163657\n",
            " - layer2_input.weight grad norm: 0.48530280590057373\n",
            " - layer2_input.bias grad norm: 0.0005621719756163657\n",
            " - layer3.weight grad norm: 0.0024746533017605543\n",
            " - layer3.bias grad norm: 0.0005487173330038786\n",
            " - layer3_input.weight grad norm: 0.4701848328113556\n",
            " - layer3_input.bias grad norm: 0.0005487173330038786\n",
            " - layer4.weight grad norm: 0.0029619825072586536\n",
            " - layer4.bias grad norm: 0.0005988436168991029\n",
            " - layer4_input.weight grad norm: 0.5131118297576904\n",
            " - layer4_input.bias grad norm: 0.0005988436168991029\n",
            " - layer5.weight grad norm: 0.012114297598600388\n",
            " - layer5.bias grad norm: 0.005065175238996744\n",
            "Gradients at iteration 621:\n",
            " - layer1.weight grad norm: 0.5246465802192688\n",
            " - layer1.bias grad norm: 0.0006136338924989104\n",
            " - layer2.weight grad norm: 0.001899904222227633\n",
            " - layer2.bias grad norm: 0.0005601780139841139\n",
            " - layer2_input.weight grad norm: 0.48512035608291626\n",
            " - layer2_input.bias grad norm: 0.0005601780139841139\n",
            " - layer3.weight grad norm: 0.002608901122584939\n",
            " - layer3.bias grad norm: 0.0006174655864015222\n",
            " - layer3_input.weight grad norm: 0.5275453329086304\n",
            " - layer3_input.bias grad norm: 0.0006174655864015222\n",
            " - layer4.weight grad norm: 0.0030839256942272186\n",
            " - layer4.bias grad norm: 0.0005319559713825583\n",
            " - layer4_input.weight grad norm: 0.4592556655406952\n",
            " - layer4_input.bias grad norm: 0.0005319559713825583\n",
            " - layer5.weight grad norm: 0.011541604995727539\n",
            " - layer5.bias grad norm: 0.0053732614032924175\n",
            "Gradients at iteration 622:\n",
            " - layer1.weight grad norm: 0.5070958137512207\n",
            " - layer1.bias grad norm: 0.0005872966139577329\n",
            " - layer2.weight grad norm: 0.0018173613352701068\n",
            " - layer2.bias grad norm: 0.0005571143701672554\n",
            " - layer2_input.weight grad norm: 0.484875351190567\n",
            " - layer2_input.bias grad norm: 0.0005571143701672554\n",
            " - layer3.weight grad norm: 0.0025387664791196585\n",
            " - layer3.bias grad norm: 0.0005798445199616253\n",
            " - layer3_input.weight grad norm: 0.49744024872779846\n",
            " - layer3_input.bias grad norm: 0.0005798445199616253\n",
            " - layer4.weight grad norm: 0.0030455810483545065\n",
            " - layer4.bias grad norm: 0.0006000360008329153\n",
            " - layer4_input.weight grad norm: 0.510027289390564\n",
            " - layer4_input.bias grad norm: 0.0006000360008329153\n",
            " - layer5.weight grad norm: 0.011249958537518978\n",
            " - layer5.bias grad norm: 0.005216986406594515\n",
            "Gradients at iteration 623:\n",
            " - layer1.weight grad norm: 0.5608595609664917\n",
            " - layer1.bias grad norm: 0.0006566851516254246\n",
            " - layer2.weight grad norm: 0.0018406483577564359\n",
            " - layer2.bias grad norm: 0.0005778846680186689\n",
            " - layer2_input.weight grad norm: 0.4997471570968628\n",
            " - layer2_input.bias grad norm: 0.0005778846680186689\n",
            " - layer3.weight grad norm: 0.0025526906829327345\n",
            " - layer3.bias grad norm: 0.0005621768068522215\n",
            " - layer3_input.weight grad norm: 0.48635154962539673\n",
            " - layer3_input.bias grad norm: 0.0005621768068522215\n",
            " - layer4.weight grad norm: 0.003060622839257121\n",
            " - layer4.bias grad norm: 0.0005095889791846275\n",
            " - layer4_input.weight grad norm: 0.4460611343383789\n",
            " - layer4_input.bias grad norm: 0.0005095889791846275\n",
            " - layer5.weight grad norm: 0.011458640918135643\n",
            " - layer5.bias grad norm: 0.0053100986406207085\n",
            "Gradients at iteration 624:\n",
            " - layer1.weight grad norm: 0.5427870154380798\n",
            " - layer1.bias grad norm: 0.0006325304275378585\n",
            " - layer2.weight grad norm: 0.001778741949237883\n",
            " - layer2.bias grad norm: 0.0005502983112819493\n",
            " - layer2_input.weight grad norm: 0.48000261187553406\n",
            " - layer2_input.bias grad norm: 0.0005502983112819493\n",
            " - layer3.weight grad norm: 0.002444452838972211\n",
            " - layer3.bias grad norm: 0.0005782913067378104\n",
            " - layer3_input.weight grad norm: 0.49889707565307617\n",
            " - layer3_input.bias grad norm: 0.0005782913067378104\n",
            " - layer4.weight grad norm: 0.0029152389615774155\n",
            " - layer4.bias grad norm: 0.0005470667383633554\n",
            " - layer4_input.weight grad norm: 0.47529223561286926\n",
            " - layer4_input.bias grad norm: 0.0005470667383633554\n",
            " - layer5.weight grad norm: 0.011545587331056595\n",
            " - layer5.bias grad norm: 0.005046670325100422\n",
            "Gradients at iteration 625:\n",
            " - layer1.weight grad norm: 0.529378354549408\n",
            " - layer1.bias grad norm: 0.0006188048864714801\n",
            " - layer2.weight grad norm: 0.0018137827282771468\n",
            " - layer2.bias grad norm: 0.0005333587760105729\n",
            " - layer2_input.weight grad norm: 0.4658075273036957\n",
            " - layer2_input.bias grad norm: 0.0005333587760105729\n",
            " - layer3.weight grad norm: 0.002538102213293314\n",
            " - layer3.bias grad norm: 0.0005785489338450134\n",
            " - layer3_input.weight grad norm: 0.4992513060569763\n",
            " - layer3_input.bias grad norm: 0.0005785489338450134\n",
            " - layer4.weight grad norm: 0.0030101207084953785\n",
            " - layer4.bias grad norm: 0.0005875814240425825\n",
            " - layer4_input.weight grad norm: 0.5033379197120667\n",
            " - layer4_input.bias grad norm: 0.0005875814240425825\n",
            " - layer5.weight grad norm: 0.011535456404089928\n",
            " - layer5.bias grad norm: 0.005191329400986433\n",
            "Gradients at iteration 626:\n",
            " - layer1.weight grad norm: 0.5110944509506226\n",
            " - layer1.bias grad norm: 0.0005894889472983778\n",
            " - layer2.weight grad norm: 0.0018568012164905667\n",
            " - layer2.bias grad norm: 0.0005443768459372222\n",
            " - layer2_input.weight grad norm: 0.47244730591773987\n",
            " - layer2_input.bias grad norm: 0.0005443768459372222\n",
            " - layer3.weight grad norm: 0.0025059061590582132\n",
            " - layer3.bias grad norm: 0.0006137750460766256\n",
            " - layer3_input.weight grad norm: 0.5210190415382385\n",
            " - layer3_input.bias grad norm: 0.0006137750460766256\n",
            " - layer4.weight grad norm: 0.003014971036463976\n",
            " - layer4.bias grad norm: 0.0005773004959337413\n",
            " - layer4_input.weight grad norm: 0.49389004707336426\n",
            " - layer4_input.bias grad norm: 0.0005773004959337413\n",
            " - layer5.weight grad norm: 0.011799262836575508\n",
            " - layer5.bias grad norm: 0.005241868086159229\n",
            "Gradients at iteration 627:\n",
            " - layer1.weight grad norm: 0.551598072052002\n",
            " - layer1.bias grad norm: 0.0006459573633037508\n",
            " - layer2.weight grad norm: 0.0018479927675798535\n",
            " - layer2.bias grad norm: 0.0005774497985839844\n",
            " - layer2_input.weight grad norm: 0.4967658817768097\n",
            " - layer2_input.bias grad norm: 0.0005774497985839844\n",
            " - layer3.weight grad norm: 0.002493353094905615\n",
            " - layer3.bias grad norm: 0.0005943971336819232\n",
            " - layer3_input.weight grad norm: 0.5081164836883545\n",
            " - layer3_input.bias grad norm: 0.0005943971336819232\n",
            " - layer4.weight grad norm: 0.002934863558039069\n",
            " - layer4.bias grad norm: 0.0004996430943720043\n",
            " - layer4_input.weight grad norm: 0.4366017282009125\n",
            " - layer4_input.bias grad norm: 0.0004996430943720043\n",
            " - layer5.weight grad norm: 0.010589332319796085\n",
            " - layer5.bias grad norm: 0.00520461518317461\n",
            "Gradients at iteration 628:\n",
            " - layer1.weight grad norm: 0.49259158968925476\n",
            " - layer1.bias grad norm: 0.0005620770389214158\n",
            " - layer2.weight grad norm: 0.0018543705809861422\n",
            " - layer2.bias grad norm: 0.0005502469139173627\n",
            " - layer2_input.weight grad norm: 0.4787557125091553\n",
            " - layer2_input.bias grad norm: 0.0005502469139173627\n",
            " - layer3.weight grad norm: 0.0025485516525804996\n",
            " - layer3.bias grad norm: 0.0006328346207737923\n",
            " - layer3_input.weight grad norm: 0.5394403338432312\n",
            " - layer3_input.bias grad norm: 0.0006328346207737923\n",
            " - layer4.weight grad norm: 0.003047885373234749\n",
            " - layer4.bias grad norm: 0.0005605441401712596\n",
            " - layer4_input.weight grad norm: 0.486752986907959\n",
            " - layer4_input.bias grad norm: 0.0005605441401712596\n",
            " - layer5.weight grad norm: 0.013145008124411106\n",
            " - layer5.bias grad norm: 0.005267491564154625\n",
            "Gradients at iteration 629:\n",
            " - layer1.weight grad norm: 0.547848641872406\n",
            " - layer1.bias grad norm: 0.000633616466075182\n",
            " - layer2.weight grad norm: 0.0018612989224493504\n",
            " - layer2.bias grad norm: 0.0006079578306525946\n",
            " - layer2_input.weight grad norm: 0.5241075754165649\n",
            " - layer2_input.bias grad norm: 0.0006079578306525946\n",
            " - layer3.weight grad norm: 0.0025399490259587765\n",
            " - layer3.bias grad norm: 0.0005249182577244937\n",
            " - layer3_input.weight grad norm: 0.46291178464889526\n",
            " - layer3_input.bias grad norm: 0.0005249182577244937\n",
            " - layer4.weight grad norm: 0.003042836906388402\n",
            " - layer4.bias grad norm: 0.000524389382917434\n",
            " - layer4_input.weight grad norm: 0.4590185284614563\n",
            " - layer4_input.bias grad norm: 0.000524389382917434\n",
            " - layer5.weight grad norm: 0.011766752228140831\n",
            " - layer5.bias grad norm: 0.0052926745265722275\n",
            "Gradients at iteration 630:\n",
            " - layer1.weight grad norm: 0.5286327004432678\n",
            " - layer1.bias grad norm: 0.0006107122753746808\n",
            " - layer2.weight grad norm: 0.0018505547195672989\n",
            " - layer2.bias grad norm: 0.0005785002722404897\n",
            " - layer2_input.weight grad norm: 0.5017164945602417\n",
            " - layer2_input.bias grad norm: 0.0005785002722404897\n",
            " - layer3.weight grad norm: 0.0025535253807902336\n",
            " - layer3.bias grad norm: 0.0006001893780194223\n",
            " - layer3_input.weight grad norm: 0.5169134736061096\n",
            " - layer3_input.bias grad norm: 0.0006001893780194223\n",
            " - layer4.weight grad norm: 0.003040364943444729\n",
            " - layer4.bias grad norm: 0.0005108146579004824\n",
            " - layer4_input.weight grad norm: 0.44883787631988525\n",
            " - layer4_input.bias grad norm: 0.0005108146579004824\n",
            " - layer5.weight grad norm: 0.011138970032334328\n",
            " - layer5.bias grad norm: 0.005238410551100969\n",
            "Gradients at iteration 631:\n",
            " - layer1.weight grad norm: 0.4978586733341217\n",
            " - layer1.bias grad norm: 0.0005697824526578188\n",
            " - layer2.weight grad norm: 0.001893393462523818\n",
            " - layer2.bias grad norm: 0.0006135029252618551\n",
            " - layer2_input.weight grad norm: 0.5263690948486328\n",
            " - layer2_input.bias grad norm: 0.0006135029252618551\n",
            " - layer3.weight grad norm: 0.0026671490631997585\n",
            " - layer3.bias grad norm: 0.0005540041020140052\n",
            " - layer3_input.weight grad norm: 0.4814266562461853\n",
            " - layer3_input.bias grad norm: 0.0005540041020140052\n",
            " - layer4.weight grad norm: 0.0031208491418510675\n",
            " - layer4.bias grad norm: 0.000574686739128083\n",
            " - layer4_input.weight grad norm: 0.49306100606918335\n",
            " - layer4_input.bias grad norm: 0.000574686739128083\n",
            " - layer5.weight grad norm: 0.01180543377995491\n",
            " - layer5.bias grad norm: 0.005412114318460226\n",
            "Gradients at iteration 632:\n",
            " - layer1.weight grad norm: 0.5070744752883911\n",
            " - layer1.bias grad norm: 0.0005771828582510352\n",
            " - layer2.weight grad norm: 0.0017911060713231564\n",
            " - layer2.bias grad norm: 0.0006263679242692888\n",
            " - layer2_input.weight grad norm: 0.5404824018478394\n",
            " - layer2_input.bias grad norm: 0.0006263679242692888\n",
            " - layer3.weight grad norm: 0.002495007123798132\n",
            " - layer3.bias grad norm: 0.0005897568771615624\n",
            " - layer3_input.weight grad norm: 0.5086306929588318\n",
            " - layer3_input.bias grad norm: 0.0005897568771615624\n",
            " - layer4.weight grad norm: 0.003000295953825116\n",
            " - layer4.bias grad norm: 0.0004971260786987841\n",
            " - layer4_input.weight grad norm: 0.4380601942539215\n",
            " - layer4_input.bias grad norm: 0.0004971260786987841\n",
            " - layer5.weight grad norm: 0.010260018520057201\n",
            " - layer5.bias grad norm: 0.0051191262900829315\n",
            "Gradients at iteration 633:\n",
            " - layer1.weight grad norm: 0.5582119226455688\n",
            " - layer1.bias grad norm: 0.0006519678863696754\n",
            " - layer2.weight grad norm: 0.001845259452238679\n",
            " - layer2.bias grad norm: 0.0005706686060875654\n",
            " - layer2_input.weight grad norm: 0.4935380518436432\n",
            " - layer2_input.bias grad norm: 0.0005706686060875654\n",
            " - layer3.weight grad norm: 0.0025183327961713076\n",
            " - layer3.bias grad norm: 0.0005558337434194982\n",
            " - layer3_input.weight grad norm: 0.4786258041858673\n",
            " - layer3_input.bias grad norm: 0.0005558337434194982\n",
            " - layer4.weight grad norm: 0.0030114452820271254\n",
            " - layer4.bias grad norm: 0.0005363356322050095\n",
            " - layer4_input.weight grad norm: 0.4642658233642578\n",
            " - layer4_input.bias grad norm: 0.0005363356322050095\n",
            " - layer5.weight grad norm: 0.012062429450452328\n",
            " - layer5.bias grad norm: 0.0052510579116642475\n",
            "Gradients at iteration 634:\n",
            " - layer1.weight grad norm: 0.5355719327926636\n",
            " - layer1.bias grad norm: 0.0006247428245842457\n",
            " - layer2.weight grad norm: 0.0018526772037148476\n",
            " - layer2.bias grad norm: 0.0005814431933686137\n",
            " - layer2_input.weight grad norm: 0.49565795063972473\n",
            " - layer2_input.bias grad norm: 0.0005814431933686137\n",
            " - layer3.weight grad norm: 0.0025713208597153425\n",
            " - layer3.bias grad norm: 0.0005783470696769655\n",
            " - layer3_input.weight grad norm: 0.4986903667449951\n",
            " - layer3_input.bias grad norm: 0.0005783470696769655\n",
            " - layer4.weight grad norm: 0.0029699408914893866\n",
            " - layer4.bias grad norm: 0.0005422437097877264\n",
            " - layer4_input.weight grad norm: 0.46755388379096985\n",
            " - layer4_input.bias grad norm: 0.0005422437097877264\n",
            " - layer5.weight grad norm: 0.011783646419644356\n",
            " - layer5.bias grad norm: 0.005198448430746794\n",
            "Gradients at iteration 635:\n",
            " - layer1.weight grad norm: 0.5198422074317932\n",
            " - layer1.bias grad norm: 0.00060463254339993\n",
            " - layer2.weight grad norm: 0.0017634277464821935\n",
            " - layer2.bias grad norm: 0.0006134612485766411\n",
            " - layer2_input.weight grad norm: 0.5232925415039062\n",
            " - layer2_input.bias grad norm: 0.0006134612485766411\n",
            " - layer3.weight grad norm: 0.0024755417834967375\n",
            " - layer3.bias grad norm: 0.0005684044444933534\n",
            " - layer3_input.weight grad norm: 0.48721757531166077\n",
            " - layer3_input.bias grad norm: 0.0005684044444933534\n",
            " - layer4.weight grad norm: 0.0029437029734253883\n",
            " - layer4.bias grad norm: 0.0005464204587042332\n",
            " - layer4_input.weight grad norm: 0.4672854244709015\n",
            " - layer4_input.bias grad norm: 0.0005464204587042332\n",
            " - layer5.weight grad norm: 0.012103674001991749\n",
            " - layer5.bias grad norm: 0.005079732742160559\n",
            "Gradients at iteration 636:\n",
            " - layer1.weight grad norm: 0.5053172707557678\n",
            " - layer1.bias grad norm: 0.0005841121892444789\n",
            " - layer2.weight grad norm: 0.001775289885699749\n",
            " - layer2.bias grad norm: 0.0005991627695038915\n",
            " - layer2_input.weight grad norm: 0.5170900821685791\n",
            " - layer2_input.bias grad norm: 0.0005991627695038915\n",
            " - layer3.weight grad norm: 0.0024742265231907368\n",
            " - layer3.bias grad norm: 0.0005761103238910437\n",
            " - layer3_input.weight grad norm: 0.4967517554759979\n",
            " - layer3_input.bias grad norm: 0.0005761103238910437\n",
            " - layer4.weight grad norm: 0.0028980772476643324\n",
            " - layer4.bias grad norm: 0.0005583071615546942\n",
            " - layer4_input.weight grad norm: 0.47992804646492004\n",
            " - layer4_input.bias grad norm: 0.0005583071615546942\n",
            " - layer5.weight grad norm: 0.011558227241039276\n",
            " - layer5.bias grad norm: 0.0050321719609200954\n",
            "Gradients at iteration 637:\n",
            " - layer1.weight grad norm: 0.5027700066566467\n",
            " - layer1.bias grad norm: 0.0005694428109563887\n",
            " - layer2.weight grad norm: 0.0019335922552272677\n",
            " - layer2.bias grad norm: 0.0005793056334368885\n",
            " - layer2_input.weight grad norm: 0.5037985444068909\n",
            " - layer2_input.bias grad norm: 0.0005793056334368885\n",
            " - layer3.weight grad norm: 0.002660003723576665\n",
            " - layer3.bias grad norm: 0.0006260510999709368\n",
            " - layer3_input.weight grad norm: 0.5398496389389038\n",
            " - layer3_input.bias grad norm: 0.0006260510999709368\n",
            " - layer4.weight grad norm: 0.0031965787056833506\n",
            " - layer4.bias grad norm: 0.0005070551997050643\n",
            " - layer4_input.weight grad norm: 0.44919878244400024\n",
            " - layer4_input.bias grad norm: 0.0005070551997050643\n",
            " - layer5.weight grad norm: 0.011778407730162144\n",
            " - layer5.bias grad norm: 0.005495571997016668\n",
            "Gradients at iteration 638:\n",
            " - layer1.weight grad norm: 0.5504571795463562\n",
            " - layer1.bias grad norm: 0.0006394053343683481\n",
            " - layer2.weight grad norm: 0.0018943557515740395\n",
            " - layer2.bias grad norm: 0.0005761479842476547\n",
            " - layer2_input.weight grad norm: 0.499315470457077\n",
            " - layer2_input.bias grad norm: 0.0005761479842476547\n",
            " - layer3.weight grad norm: 0.0026263755280524492\n",
            " - layer3.bias grad norm: 0.0004999830853193998\n",
            " - layer3_input.weight grad norm: 0.44074803590774536\n",
            " - layer3_input.bias grad norm: 0.0004999830853193998\n",
            " - layer4.weight grad norm: 0.0030687693506479263\n",
            " - layer4.bias grad norm: 0.0005824753898195922\n",
            " - layer4_input.weight grad norm: 0.503223717212677\n",
            " - layer4_input.bias grad norm: 0.0005824753898195922\n",
            " - layer5.weight grad norm: 0.01171793695539236\n",
            " - layer5.bias grad norm: 0.005348094739019871\n",
            "Gradients at iteration 639:\n",
            " - layer1.weight grad norm: 0.5299515724182129\n",
            " - layer1.bias grad norm: 0.0006190662970766425\n",
            " - layer2.weight grad norm: 0.0018713732715696096\n",
            " - layer2.bias grad norm: 0.0005808018613606691\n",
            " - layer2_input.weight grad norm: 0.5004972219467163\n",
            " - layer2_input.bias grad norm: 0.0005808018613606691\n",
            " - layer3.weight grad norm: 0.0026139335241168737\n",
            " - layer3.bias grad norm: 0.0005799130885861814\n",
            " - layer3_input.weight grad norm: 0.49888181686401367\n",
            " - layer3_input.bias grad norm: 0.0005799130885861814\n",
            " - layer4.weight grad norm: 0.0030526439659297466\n",
            " - layer4.bias grad norm: 0.0005478279781527817\n",
            " - layer4_input.weight grad norm: 0.4685926139354706\n",
            " - layer4_input.bias grad norm: 0.0005478279781527817\n",
            " - layer5.weight grad norm: 0.011897587217390537\n",
            " - layer5.bias grad norm: 0.005308112129569054\n",
            "Gradients at iteration 640:\n",
            " - layer1.weight grad norm: 0.5103305578231812\n",
            " - layer1.bias grad norm: 0.0005991945508867502\n",
            " - layer2.weight grad norm: 0.0018544895574450493\n",
            " - layer2.bias grad norm: 0.0005741413915529847\n",
            " - layer2_input.weight grad norm: 0.4930121898651123\n",
            " - layer2_input.bias grad norm: 0.0005741413915529847\n",
            " - layer3.weight grad norm: 0.0026008649729192257\n",
            " - layer3.bias grad norm: 0.0005846972344443202\n",
            " - layer3_input.weight grad norm: 0.4983810782432556\n",
            " - layer3_input.bias grad norm: 0.0005846972344443202\n",
            " - layer4.weight grad norm: 0.003057314781472087\n",
            " - layer4.bias grad norm: 0.000581863452680409\n",
            " - layer4_input.weight grad norm: 0.49791082739830017\n",
            " - layer4_input.bias grad norm: 0.000581863452680409\n",
            " - layer5.weight grad norm: 0.012366649694740772\n",
            " - layer5.bias grad norm: 0.005291861016303301\n",
            "Gradients at iteration 641:\n",
            " - layer1.weight grad norm: 0.5523566603660583\n",
            " - layer1.bias grad norm: 0.0006474783294834197\n",
            " - layer2.weight grad norm: 0.0018149635288864374\n",
            " - layer2.bias grad norm: 0.0005142799345776439\n",
            " - layer2_input.weight grad norm: 0.45412763953208923\n",
            " - layer2_input.bias grad norm: 0.0005142799345776439\n",
            " - layer3.weight grad norm: 0.0024699948262423277\n",
            " - layer3.bias grad norm: 0.0005870474269613624\n",
            " - layer3_input.weight grad norm: 0.5017081499099731\n",
            " - layer3_input.bias grad norm: 0.0005870474269613624\n",
            " - layer4.weight grad norm: 0.002922921907156706\n",
            " - layer4.bias grad norm: 0.0005658114678226411\n",
            " - layer4_input.weight grad norm: 0.48659440875053406\n",
            " - layer4_input.bias grad norm: 0.0005658114678226411\n",
            " - layer5.weight grad norm: 0.011767247691750526\n",
            " - layer5.bias grad norm: 0.005126469302922487\n",
            "Gradients at iteration 642:\n",
            " - layer1.weight grad norm: 0.5391742587089539\n",
            " - layer1.bias grad norm: 0.0006250706501305103\n",
            " - layer2.weight grad norm: 0.001779208891093731\n",
            " - layer2.bias grad norm: 0.0005616009002551436\n",
            " - layer2_input.weight grad norm: 0.4935876429080963\n",
            " - layer2_input.bias grad norm: 0.0005616009002551436\n",
            " - layer3.weight grad norm: 0.0023918950464576483\n",
            " - layer3.bias grad norm: 0.0005319020128808916\n",
            " - layer3_input.weight grad norm: 0.46761247515678406\n",
            " - layer3_input.bias grad norm: 0.0005319020128808916\n",
            " - layer4.weight grad norm: 0.0028527602553367615\n",
            " - layer4.bias grad norm: 0.0005787478876300156\n",
            " - layer4_input.weight grad norm: 0.49683934450149536\n",
            " - layer4_input.bias grad norm: 0.0005787478876300156\n",
            " - layer5.weight grad norm: 0.010364270769059658\n",
            " - layer5.bias grad norm: 0.004986305255442858\n",
            "Gradients at iteration 643:\n",
            " - layer1.weight grad norm: 0.5047388672828674\n",
            " - layer1.bias grad norm: 0.0005802068044431508\n",
            " - layer2.weight grad norm: 0.001816449803300202\n",
            " - layer2.bias grad norm: 0.000542014604434371\n",
            " - layer2_input.weight grad norm: 0.4739725589752197\n",
            " - layer2_input.bias grad norm: 0.000542014604434371\n",
            " - layer3.weight grad norm: 0.0024591905530542135\n",
            " - layer3.bias grad norm: 0.000632863724604249\n",
            " - layer3_input.weight grad norm: 0.5379858613014221\n",
            " - layer3_input.bias grad norm: 0.000632863724604249\n",
            " - layer4.weight grad norm: 0.0029281643219292164\n",
            " - layer4.bias grad norm: 0.0005562539445236325\n",
            " - layer4_input.weight grad norm: 0.48060598969459534\n",
            " - layer4_input.bias grad norm: 0.0005562539445236325\n",
            " - layer5.weight grad norm: 0.011463338509202003\n",
            " - layer5.bias grad norm: 0.005104227922856808\n",
            "Gradients at iteration 644:\n",
            " - layer1.weight grad norm: 0.5250257253646851\n",
            " - layer1.bias grad norm: 0.0006117908633314073\n",
            " - layer2.weight grad norm: 0.001787106622941792\n",
            " - layer2.bias grad norm: 0.0005928308819420636\n",
            " - layer2_input.weight grad norm: 0.5053533315658569\n",
            " - layer2_input.bias grad norm: 0.0005928308819420636\n",
            " - layer3.weight grad norm: 0.002462561009451747\n",
            " - layer3.bias grad norm: 0.0005863790865987539\n",
            " - layer3_input.weight grad norm: 0.5043413043022156\n",
            " - layer3_input.bias grad norm: 0.0005863790865987539\n",
            " - layer4.weight grad norm: 0.002889444585889578\n",
            " - layer4.bias grad norm: 0.0005357785848900676\n",
            " - layer4_input.weight grad norm: 0.46304598450660706\n",
            " - layer4_input.bias grad norm: 0.0005357785848900676\n",
            " - layer5.weight grad norm: 0.012194070033729076\n",
            " - layer5.bias grad norm: 0.005060380324721336\n",
            "Gradients at iteration 645:\n",
            " - layer1.weight grad norm: 0.5193806290626526\n",
            " - layer1.bias grad norm: 0.0005912402411922812\n",
            " - layer2.weight grad norm: 0.0019062815699726343\n",
            " - layer2.bias grad norm: 0.0006252169841900468\n",
            " - layer2_input.weight grad norm: 0.5406044125556946\n",
            " - layer2_input.bias grad norm: 0.0006252169841900468\n",
            " - layer3.weight grad norm: 0.0025599829386919737\n",
            " - layer3.bias grad norm: 0.0005525770247913897\n",
            " - layer3_input.weight grad norm: 0.48033463954925537\n",
            " - layer3_input.bias grad norm: 0.0005525770247913897\n",
            " - layer4.weight grad norm: 0.0030766604468226433\n",
            " - layer4.bias grad norm: 0.0005209959344938397\n",
            " - layer4_input.weight grad norm: 0.45506730675697327\n",
            " - layer4_input.bias grad norm: 0.0005209959344938397\n",
            " - layer5.weight grad norm: 0.01153680868446827\n",
            " - layer5.bias grad norm: 0.005302196368575096\n",
            "Gradients at iteration 646:\n",
            " - layer1.weight grad norm: 0.5203233957290649\n",
            " - layer1.bias grad norm: 0.0005961701972410083\n",
            " - layer2.weight grad norm: 0.0018290375592187047\n",
            " - layer2.bias grad norm: 0.0005591835943050683\n",
            " - layer2_input.weight grad norm: 0.4898487329483032\n",
            " - layer2_input.bias grad norm: 0.0005591835943050683\n",
            " - layer3.weight grad norm: 0.002479089656844735\n",
            " - layer3.bias grad norm: 0.0005970863858237863\n",
            " - layer3_input.weight grad norm: 0.5138887763023376\n",
            " - layer3_input.bias grad norm: 0.0005970863858237863\n",
            " - layer4.weight grad norm: 0.002978284377604723\n",
            " - layer4.bias grad norm: 0.000545920804142952\n",
            " - layer4_input.weight grad norm: 0.47440165281295776\n",
            " - layer4_input.bias grad norm: 0.000545920804142952\n",
            " - layer5.weight grad norm: 0.011196544393897057\n",
            " - layer5.bias grad norm: 0.005219378974288702\n",
            "Gradients at iteration 647:\n",
            " - layer1.weight grad norm: 0.5154302716255188\n",
            " - layer1.bias grad norm: 0.0005990356439724565\n",
            " - layer2.weight grad norm: 0.0018074695253744721\n",
            " - layer2.bias grad norm: 0.0005715194856747985\n",
            " - layer2_input.weight grad norm: 0.48978087306022644\n",
            " - layer2_input.bias grad norm: 0.0005715194856747985\n",
            " - layer3.weight grad norm: 0.0024497509002685547\n",
            " - layer3.bias grad norm: 0.0006104506319388747\n",
            " - layer3_input.weight grad norm: 0.5206937193870544\n",
            " - layer3_input.bias grad norm: 0.0006104506319388747\n",
            " - layer4.weight grad norm: 0.002931082621216774\n",
            " - layer4.bias grad norm: 0.0005479993997141719\n",
            " - layer4_input.weight grad norm: 0.4723779261112213\n",
            " - layer4_input.bias grad norm: 0.0005479993997141719\n",
            " - layer5.weight grad norm: 0.011720766313374043\n",
            " - layer5.bias grad norm: 0.005085024982690811\n",
            "Gradients at iteration 648:\n",
            " - layer1.weight grad norm: 0.5273481607437134\n",
            " - layer1.bias grad norm: 0.0006120021571405232\n",
            " - layer2.weight grad norm: 0.0018828829051926732\n",
            " - layer2.bias grad norm: 0.0005795066826976836\n",
            " - layer2_input.weight grad norm: 0.5008220672607422\n",
            " - layer2_input.bias grad norm: 0.0005795066826976836\n",
            " - layer3.weight grad norm: 0.0025857482105493546\n",
            " - layer3.bias grad norm: 0.0005512114148586988\n",
            " - layer3_input.weight grad norm: 0.48202213644981384\n",
            " - layer3_input.bias grad norm: 0.0005512114148586988\n",
            " - layer4.weight grad norm: 0.0030270498245954514\n",
            " - layer4.bias grad norm: 0.0005644194898195565\n",
            " - layer4_input.weight grad norm: 0.48842430114746094\n",
            " - layer4_input.bias grad norm: 0.0005644194898195565\n",
            " - layer5.weight grad norm: 0.011308852583169937\n",
            " - layer5.bias grad norm: 0.005286113824695349\n",
            "Gradients at iteration 649:\n",
            " - layer1.weight grad norm: 0.5221900939941406\n",
            " - layer1.bias grad norm: 0.0006058420403860509\n",
            " - layer2.weight grad norm: 0.0018883273005485535\n",
            " - layer2.bias grad norm: 0.0005844919360242784\n",
            " - layer2_input.weight grad norm: 0.5051072239875793\n",
            " - layer2_input.bias grad norm: 0.0005844919360242784\n",
            " - layer3.weight grad norm: 0.002553202910348773\n",
            " - layer3.bias grad norm: 0.0005468860035762191\n",
            " - layer3_input.weight grad norm: 0.47543489933013916\n",
            " - layer3_input.bias grad norm: 0.0005468860035762191\n",
            " - layer4.weight grad norm: 0.003090106649324298\n",
            " - layer4.bias grad norm: 0.000580203253775835\n",
            " - layer4_input.weight grad norm: 0.4959295392036438\n",
            " - layer4_input.bias grad norm: 0.000580203253775835\n",
            " - layer5.weight grad norm: 0.01222932618111372\n",
            " - layer5.bias grad norm: 0.005305817816406488\n",
            "Gradients at iteration 650:\n",
            " - layer1.weight grad norm: 0.5054850578308105\n",
            " - layer1.bias grad norm: 0.000587975257076323\n",
            " - layer2.weight grad norm: 0.001785016036592424\n",
            " - layer2.bias grad norm: 0.0006209787097759545\n",
            " - layer2_input.weight grad norm: 0.5260711908340454\n",
            " - layer2_input.bias grad norm: 0.0006209787097759545\n",
            " - layer3.weight grad norm: 0.002428594045341015\n",
            " - layer3.bias grad norm: 0.0005923759890720248\n",
            " - layer3_input.weight grad norm: 0.5063332915306091\n",
            " - layer3_input.bias grad norm: 0.0005923759890720248\n",
            " - layer4.weight grad norm: 0.002914485288783908\n",
            " - layer4.bias grad norm: 0.0005331685533747077\n",
            " - layer4_input.weight grad norm: 0.4595593214035034\n",
            " - layer4_input.bias grad norm: 0.0005331685533747077\n",
            " - layer5.weight grad norm: 0.010972337797284126\n",
            " - layer5.bias grad norm: 0.005027063190937042\n",
            "Gradients at iteration 651:\n",
            " - layer1.weight grad norm: 0.4901910424232483\n",
            " - layer1.bias grad norm: 0.0005664338241331279\n",
            " - layer2.weight grad norm: 0.00179028301499784\n",
            " - layer2.bias grad norm: 0.00063265924109146\n",
            " - layer2_input.weight grad norm: 0.5385484099388123\n",
            " - layer2_input.bias grad norm: 0.00063265924109146\n",
            " - layer3.weight grad norm: 0.0024795925710350275\n",
            " - layer3.bias grad norm: 0.000531280878931284\n",
            " - layer3_input.weight grad norm: 0.46024876832962036\n",
            " - layer3_input.bias grad norm: 0.000531280878931284\n",
            " - layer4.weight grad norm: 0.0029343219939619303\n",
            " - layer4.bias grad norm: 0.0005994542734697461\n",
            " - layer4_input.weight grad norm: 0.5075921416282654\n",
            " - layer4_input.bias grad norm: 0.0005994542734697461\n",
            " - layer5.weight grad norm: 0.012388668954372406\n",
            " - layer5.bias grad norm: 0.005086895544081926\n",
            "Gradients at iteration 652:\n",
            " - layer1.weight grad norm: 0.5341470241546631\n",
            " - layer1.bias grad norm: 0.0006253956817090511\n",
            " - layer2.weight grad norm: 0.0019208998419344425\n",
            " - layer2.bias grad norm: 0.0005696777370758355\n",
            " - layer2_input.weight grad norm: 0.49356991052627563\n",
            " - layer2_input.bias grad norm: 0.0005696777370758355\n",
            " - layer3.weight grad norm: 0.002631510840728879\n",
            " - layer3.bias grad norm: 0.0005826495471410453\n",
            " - layer3_input.weight grad norm: 0.5000996589660645\n",
            " - layer3_input.bias grad norm: 0.0005826495471410453\n",
            " - layer4.weight grad norm: 0.0031309935729950666\n",
            " - layer4.bias grad norm: 0.0005403397371992469\n",
            " - layer4_input.weight grad norm: 0.4698772132396698\n",
            " - layer4_input.bias grad norm: 0.0005403397371992469\n",
            " - layer5.weight grad norm: 0.011818514205515385\n",
            " - layer5.bias grad norm: 0.005401723552495241\n",
            "Gradients at iteration 653:\n",
            " - layer1.weight grad norm: 0.5072222352027893\n",
            " - layer1.bias grad norm: 0.0005789666320197284\n",
            " - layer2.weight grad norm: 0.0019126470433548093\n",
            " - layer2.bias grad norm: 0.0005896245129406452\n",
            " - layer2_input.weight grad norm: 0.5140563249588013\n",
            " - layer2_input.bias grad norm: 0.0005896245129406452\n",
            " - layer3.weight grad norm: 0.0026492096949368715\n",
            " - layer3.bias grad norm: 0.0006256061024032533\n",
            " - layer3_input.weight grad norm: 0.5358956456184387\n",
            " - layer3_input.bias grad norm: 0.0006256061024032533\n",
            " - layer4.weight grad norm: 0.003114969003945589\n",
            " - layer4.bias grad norm: 0.0004902684595435858\n",
            " - layer4_input.weight grad norm: 0.4371371865272522\n",
            " - layer4_input.bias grad norm: 0.0004902684595435858\n",
            " - layer5.weight grad norm: 0.012108942493796349\n",
            " - layer5.bias grad norm: 0.005418762564659119\n",
            "Gradients at iteration 654:\n",
            " - layer1.weight grad norm: 0.5363720655441284\n",
            " - layer1.bias grad norm: 0.0006247992860153317\n",
            " - layer2.weight grad norm: 0.001768804737366736\n",
            " - layer2.bias grad norm: 0.0005755232414230704\n",
            " - layer2_input.weight grad norm: 0.4945351779460907\n",
            " - layer2_input.bias grad norm: 0.0005755232414230704\n",
            " - layer3.weight grad norm: 0.0024537134449929\n",
            " - layer3.bias grad norm: 0.0005655927816405892\n",
            " - layer3_input.weight grad norm: 0.4858672320842743\n",
            " - layer3_input.bias grad norm: 0.0005655927816405892\n",
            " - layer4.weight grad norm: 0.0028807190246880054\n",
            " - layer4.bias grad norm: 0.0005625673802569509\n",
            " - layer4_input.weight grad norm: 0.48114678263664246\n",
            " - layer4_input.bias grad norm: 0.0005625673802569509\n",
            " - layer5.weight grad norm: 0.011203561909496784\n",
            " - layer5.bias grad norm: 0.005056906025856733\n",
            "Gradients at iteration 655:\n",
            " - layer1.weight grad norm: 0.5335747003555298\n",
            " - layer1.bias grad norm: 0.0006279125809669495\n",
            " - layer2.weight grad norm: 0.0017240600427612662\n",
            " - layer2.bias grad norm: 0.0006203071097843349\n",
            " - layer2_input.weight grad norm: 0.5281622409820557\n",
            " - layer2_input.bias grad norm: 0.0006203071097843349\n",
            " - layer3.weight grad norm: 0.002392236841842532\n",
            " - layer3.bias grad norm: 0.0005638952716253698\n",
            " - layer3_input.weight grad norm: 0.48491647839546204\n",
            " - layer3_input.bias grad norm: 0.0005638952716253698\n",
            " - layer4.weight grad norm: 0.0028191031888127327\n",
            " - layer4.bias grad norm: 0.0005194840487092733\n",
            " - layer4_input.weight grad norm: 0.4483453631401062\n",
            " - layer4_input.bias grad norm: 0.0005194840487092733\n",
            " - layer5.weight grad norm: 0.011908559128642082\n",
            " - layer5.bias grad norm: 0.004927469417452812\n",
            "Gradients at iteration 656:\n",
            " - layer1.weight grad norm: 0.50315922498703\n",
            " - layer1.bias grad norm: 0.0005843238322995603\n",
            " - layer2.weight grad norm: 0.0017787995748221874\n",
            " - layer2.bias grad norm: 0.0006438661366701126\n",
            " - layer2_input.weight grad norm: 0.5415734052658081\n",
            " - layer2_input.bias grad norm: 0.0006438661366701126\n",
            " - layer3.weight grad norm: 0.0024627347011119127\n",
            " - layer3.bias grad norm: 0.0005590405780822039\n",
            " - layer3_input.weight grad norm: 0.4814699590206146\n",
            " - layer3_input.bias grad norm: 0.0005590405780822039\n",
            " - layer4.weight grad norm: 0.002940070116892457\n",
            " - layer4.bias grad norm: 0.000547724193893373\n",
            " - layer4_input.weight grad norm: 0.47066426277160645\n",
            " - layer4_input.bias grad norm: 0.000547724193893373\n",
            " - layer5.weight grad norm: 0.012028600089251995\n",
            " - layer5.bias grad norm: 0.00508153298869729\n",
            "Gradients at iteration 657:\n",
            " - layer1.weight grad norm: 0.4986199140548706\n",
            " - layer1.bias grad norm: 0.0005694807623513043\n",
            " - layer2.weight grad norm: 0.0018595188157632947\n",
            " - layer2.bias grad norm: 0.0006111295660957694\n",
            " - layer2_input.weight grad norm: 0.5263699889183044\n",
            " - layer2_input.bias grad norm: 0.0006111295660957694\n",
            " - layer3.weight grad norm: 0.0025718894321471453\n",
            " - layer3.bias grad norm: 0.0005744033842347562\n",
            " - layer3_input.weight grad norm: 0.49622684717178345\n",
            " - layer3_input.bias grad norm: 0.0005744033842347562\n",
            " - layer4.weight grad norm: 0.0030095656402409077\n",
            " - layer4.bias grad norm: 0.0005491767078638077\n",
            " - layer4_input.weight grad norm: 0.4773668050765991\n",
            " - layer4_input.bias grad norm: 0.0005491767078638077\n",
            " - layer5.weight grad norm: 0.011961894109845161\n",
            " - layer5.bias grad norm: 0.00528915598988533\n",
            "Gradients at iteration 658:\n",
            " - layer1.weight grad norm: 0.5049904584884644\n",
            " - layer1.bias grad norm: 0.0005779662169516087\n",
            " - layer2.weight grad norm: 0.0018661627545952797\n",
            " - layer2.bias grad norm: 0.0005678454181179404\n",
            " - layer2_input.weight grad norm: 0.4930400848388672\n",
            " - layer2_input.bias grad norm: 0.0005678454181179404\n",
            " - layer3.weight grad norm: 0.00254339468665421\n",
            " - layer3.bias grad norm: 0.0006372667849063873\n",
            " - layer3_input.weight grad norm: 0.5446716547012329\n",
            " - layer3_input.bias grad norm: 0.0006372667849063873\n",
            " - layer4.weight grad norm: 0.00306606013327837\n",
            " - layer4.bias grad norm: 0.0005143156158737838\n",
            " - layer4_input.weight grad norm: 0.4527948498725891\n",
            " - layer4_input.bias grad norm: 0.0005143156158737838\n",
            " - layer5.weight grad norm: 0.012509378604590893\n",
            " - layer5.bias grad norm: 0.005247225984930992\n",
            "Gradients at iteration 659:\n",
            " - layer1.weight grad norm: 0.5119050145149231\n",
            " - layer1.bias grad norm: 0.0005895621725358069\n",
            " - layer2.weight grad norm: 0.0018162726191803813\n",
            " - layer2.bias grad norm: 0.0005786585388705134\n",
            " - layer2_input.weight grad norm: 0.5037140250205994\n",
            " - layer2_input.bias grad norm: 0.0005786585388705134\n",
            " - layer3.weight grad norm: 0.0024492868687957525\n",
            " - layer3.bias grad norm: 0.0005656036664731801\n",
            " - layer3_input.weight grad norm: 0.4878523349761963\n",
            " - layer3_input.bias grad norm: 0.0005656036664731801\n",
            " - layer4.weight grad norm: 0.002944663865491748\n",
            " - layer4.bias grad norm: 0.0005749183474108577\n",
            " - layer4_input.weight grad norm: 0.4960246980190277\n",
            " - layer4_input.bias grad norm: 0.0005749183474108577\n",
            " - layer5.weight grad norm: 0.01179119385778904\n",
            " - layer5.bias grad norm: 0.0050950609147548676\n",
            "Gradients at iteration 660:\n",
            " - layer1.weight grad norm: 0.5304529070854187\n",
            " - layer1.bias grad norm: 0.0006132283597253263\n",
            " - layer2.weight grad norm: 0.0018430533818900585\n",
            " - layer2.bias grad norm: 0.0005616866983473301\n",
            " - layer2_input.weight grad norm: 0.4905172884464264\n",
            " - layer2_input.bias grad norm: 0.0005616866983473301\n",
            " - layer3.weight grad norm: 0.002541132038459182\n",
            " - layer3.bias grad norm: 0.0005618628347292542\n",
            " - layer3_input.weight grad norm: 0.4888637959957123\n",
            " - layer3_input.bias grad norm: 0.0005618628347292542\n",
            " - layer4.weight grad norm: 0.00298226997256279\n",
            " - layer4.bias grad norm: 0.0005682146293111145\n",
            " - layer4_input.weight grad norm: 0.48872873187065125\n",
            " - layer4_input.bias grad norm: 0.0005682146293111145\n",
            " - layer5.weight grad norm: 0.011004840023815632\n",
            " - layer5.bias grad norm: 0.0051727332174777985\n",
            "Gradients at iteration 661:\n",
            " - layer1.weight grad norm: 0.5199508666992188\n",
            " - layer1.bias grad norm: 0.0006000911234878004\n",
            " - layer2.weight grad norm: 0.0018828307511284947\n",
            " - layer2.bias grad norm: 0.0005368423298932612\n",
            " - layer2_input.weight grad norm: 0.4758186936378479\n",
            " - layer2_input.bias grad norm: 0.0005368423298932612\n",
            " - layer3.weight grad norm: 0.002576816128566861\n",
            " - layer3.bias grad norm: 0.0005815934855490923\n",
            " - layer3_input.weight grad norm: 0.5037952065467834\n",
            " - layer3_input.bias grad norm: 0.0005815934855490923\n",
            " - layer4.weight grad norm: 0.0030586174689233303\n",
            " - layer4.bias grad norm: 0.0005724921938963234\n",
            " - layer4_input.weight grad norm: 0.49922817945480347\n",
            " - layer4_input.bias grad norm: 0.0005724921938963234\n",
            " - layer5.weight grad norm: 0.012635414488613605\n",
            " - layer5.bias grad norm: 0.005286556668579578\n",
            "Gradients at iteration 662:\n",
            " - layer1.weight grad norm: 0.5040969252586365\n",
            " - layer1.bias grad norm: 0.0005843037506565452\n",
            " - layer2.weight grad norm: 0.0017528361640870571\n",
            " - layer2.bias grad norm: 0.0006080280872993171\n",
            " - layer2_input.weight grad norm: 0.5215123891830444\n",
            " - layer2_input.bias grad norm: 0.0006080280872993171\n",
            " - layer3.weight grad norm: 0.0024226950481534004\n",
            " - layer3.bias grad norm: 0.0006127759115770459\n",
            " - layer3_input.weight grad norm: 0.5215169191360474\n",
            " - layer3_input.bias grad norm: 0.0006127759115770459\n",
            " - layer4.weight grad norm: 0.0028212571050971746\n",
            " - layer4.bias grad norm: 0.0005178788560442626\n",
            " - layer4_input.weight grad norm: 0.4491657316684723\n",
            " - layer4_input.bias grad norm: 0.0005178788560442626\n",
            " - layer5.weight grad norm: 0.011739472858607769\n",
            " - layer5.bias grad norm: 0.004930444527417421\n",
            "Gradients at iteration 663:\n",
            " - layer1.weight grad norm: 0.5598955154418945\n",
            " - layer1.bias grad norm: 0.0006557508022524416\n",
            " - layer2.weight grad norm: 0.0017867671558633447\n",
            " - layer2.bias grad norm: 0.0005941627896390855\n",
            " - layer2_input.weight grad norm: 0.5128885507583618\n",
            " - layer2_input.bias grad norm: 0.0005941627896390855\n",
            " - layer3.weight grad norm: 0.0024611179251223803\n",
            " - layer3.bias grad norm: 0.0005103283328935504\n",
            " - layer3_input.weight grad norm: 0.446047306060791\n",
            " - layer3_input.bias grad norm: 0.0005103283328935504\n",
            " - layer4.weight grad norm: 0.002985420636832714\n",
            " - layer4.bias grad norm: 0.0005484745488502085\n",
            " - layer4_input.weight grad norm: 0.4736146032810211\n",
            " - layer4_input.bias grad norm: 0.0005484745488502085\n",
            " - layer5.weight grad norm: 0.012111610732972622\n",
            " - layer5.bias grad norm: 0.0051284353248775005\n",
            "Gradients at iteration 664:\n",
            " - layer1.weight grad norm: 0.5434789061546326\n",
            " - layer1.bias grad norm: 0.0006336469086818397\n",
            " - layer2.weight grad norm: 0.001785901840776205\n",
            " - layer2.bias grad norm: 0.0005800578510388732\n",
            " - layer2_input.weight grad norm: 0.501423716545105\n",
            " - layer2_input.bias grad norm: 0.0005800578510388732\n",
            " - layer3.weight grad norm: 0.002445380436256528\n",
            " - layer3.bias grad norm: 0.0005624269833788276\n",
            " - layer3_input.weight grad norm: 0.48627686500549316\n",
            " - layer3_input.bias grad norm: 0.0005624269833788276\n",
            " - layer4.weight grad norm: 0.0029062372632324696\n",
            " - layer4.bias grad norm: 0.0005395193002186716\n",
            " - layer4_input.weight grad norm: 0.4653346538543701\n",
            " - layer4_input.bias grad norm: 0.0005395193002186716\n",
            " - layer5.weight grad norm: 0.012574163265526295\n",
            " - layer5.bias grad norm: 0.005044674500823021\n",
            "Gradients at iteration 665:\n",
            " - layer1.weight grad norm: 0.5042964816093445\n",
            " - layer1.bias grad norm: 0.0005703408387489617\n",
            " - layer2.weight grad norm: 0.0019452765118330717\n",
            " - layer2.bias grad norm: 0.0005615205154754221\n",
            " - layer2_input.weight grad norm: 0.4966331124305725\n",
            " - layer2_input.bias grad norm: 0.0005615205154754221\n",
            " - layer3.weight grad norm: 0.002647014334797859\n",
            " - layer3.bias grad norm: 0.0005750406999140978\n",
            " - layer3_input.weight grad norm: 0.5017633438110352\n",
            " - layer3_input.bias grad norm: 0.0005750406999140978\n",
            " - layer4.weight grad norm: 0.003124853363260627\n",
            " - layer4.bias grad norm: 0.0005686656222678721\n",
            " - layer4_input.weight grad norm: 0.497052401304245\n",
            " - layer4_input.bias grad norm: 0.0005686656222678721\n",
            " - layer5.weight grad norm: 0.012637613341212273\n",
            " - layer5.bias grad norm: 0.005531469825655222\n",
            "Gradients at iteration 666:\n",
            " - layer1.weight grad norm: 0.5173604488372803\n",
            " - layer1.bias grad norm: 0.0005967392353340983\n",
            " - layer2.weight grad norm: 0.0017858691280707717\n",
            " - layer2.bias grad norm: 0.0005801290972158313\n",
            " - layer2_input.weight grad norm: 0.501093327999115\n",
            " - layer2_input.bias grad norm: 0.0005801290972158313\n",
            " - layer3.weight grad norm: 0.002463976386934519\n",
            " - layer3.bias grad norm: 0.0006020063883624971\n",
            " - layer3_input.weight grad norm: 0.5192986726760864\n",
            " - layer3_input.bias grad norm: 0.0006020063883624971\n",
            " - layer4.weight grad norm: 0.002899821847677231\n",
            " - layer4.bias grad norm: 0.0005280256737023592\n",
            " - layer4_input.weight grad norm: 0.4597674608230591\n",
            " - layer4_input.bias grad norm: 0.0005280256737023592\n",
            " - layer5.weight grad norm: 0.011881419457495213\n",
            " - layer5.bias grad norm: 0.005044133868068457\n",
            "Gradients at iteration 667:\n",
            " - layer1.weight grad norm: 0.5537717938423157\n",
            " - layer1.bias grad norm: 0.0006445198669098318\n",
            " - layer2.weight grad norm: 0.0018980178283527493\n",
            " - layer2.bias grad norm: 0.0005469584721140563\n",
            " - layer2_input.weight grad norm: 0.47561195492744446\n",
            " - layer2_input.bias grad norm: 0.0005469584721140563\n",
            " - layer3.weight grad norm: 0.0026313522830605507\n",
            " - layer3.bias grad norm: 0.0005123973824083805\n",
            " - layer3_input.weight grad norm: 0.44747886061668396\n",
            " - layer3_input.bias grad norm: 0.0005123973824083805\n",
            " - layer4.weight grad norm: 0.0031788479536771774\n",
            " - layer4.bias grad norm: 0.0006027847994118929\n",
            " - layer4_input.weight grad norm: 0.5164147019386292\n",
            " - layer4_input.bias grad norm: 0.0006027847994118929\n",
            " - layer5.weight grad norm: 0.012503652833402157\n",
            " - layer5.bias grad norm: 0.005433789454400539\n",
            "Gradients at iteration 668:\n",
            " - layer1.weight grad norm: 0.4496568441390991\n",
            " - layer1.bias grad norm: 0.0005061191623099148\n",
            " - layer2.weight grad norm: 0.001817359239794314\n",
            " - layer2.bias grad norm: 0.0005837228964082897\n",
            " - layer2_input.weight grad norm: 0.5076080560684204\n",
            " - layer2_input.bias grad norm: 0.0005837228964082897\n",
            " - layer3.weight grad norm: 0.0024959503207355738\n",
            " - layer3.bias grad norm: 0.0006067511276341975\n",
            " - layer3_input.weight grad norm: 0.5212418437004089\n",
            " - layer3_input.bias grad norm: 0.0006067511276341975\n",
            " - layer4.weight grad norm: 0.0029666863847523928\n",
            " - layer4.bias grad norm: 0.0006038285209797323\n",
            " - layer4_input.weight grad norm: 0.5179492831230164\n",
            " - layer4_input.bias grad norm: 0.0006038285209797323\n",
            " - layer5.weight grad norm: 0.011441688053309917\n",
            " - layer5.bias grad norm: 0.005150964483618736\n",
            "Gradients at iteration 669:\n",
            " - layer1.weight grad norm: 0.5581218600273132\n",
            " - layer1.bias grad norm: 0.0006500824238173664\n",
            " - layer2.weight grad norm: 0.0017913589254021645\n",
            " - layer2.bias grad norm: 0.0006095356657169759\n",
            " - layer2_input.weight grad norm: 0.5254592299461365\n",
            " - layer2_input.bias grad norm: 0.0006095356657169759\n",
            " - layer3.weight grad norm: 0.002420790959149599\n",
            " - layer3.bias grad norm: 0.0005474399658851326\n",
            " - layer3_input.weight grad norm: 0.47385406494140625\n",
            " - layer3_input.bias grad norm: 0.0005474399658851326\n",
            " - layer4.weight grad norm: 0.0029264818876981735\n",
            " - layer4.bias grad norm: 0.0004939569043926895\n",
            " - layer4_input.weight grad norm: 0.43323397636413574\n",
            " - layer4_input.bias grad norm: 0.0004939569043926895\n",
            " - layer5.weight grad norm: 0.010864806361496449\n",
            " - layer5.bias grad norm: 0.005024351179599762\n",
            "Gradients at iteration 670:\n",
            " - layer1.weight grad norm: 0.48968684673309326\n",
            " - layer1.bias grad norm: 0.0005615268019028008\n",
            " - layer2.weight grad norm: 0.0018343606498092413\n",
            " - layer2.bias grad norm: 0.0005983677692711353\n",
            " - layer2_input.weight grad norm: 0.5157658457756042\n",
            " - layer2_input.bias grad norm: 0.0005983677692711353\n",
            " - layer3.weight grad norm: 0.002512328326702118\n",
            " - layer3.bias grad norm: 0.0005847796564921737\n",
            " - layer3_input.weight grad norm: 0.5050622224807739\n",
            " - layer3_input.bias grad norm: 0.0005847796564921737\n",
            " - layer4.weight grad norm: 0.003024164354428649\n",
            " - layer4.bias grad norm: 0.0005649648373946548\n",
            " - layer4_input.weight grad norm: 0.48878487944602966\n",
            " - layer4_input.bias grad norm: 0.0005649648373946548\n",
            " - layer5.weight grad norm: 0.01207090076059103\n",
            " - layer5.bias grad norm: 0.005197315476834774\n",
            "Gradients at iteration 671:\n",
            " - layer1.weight grad norm: 0.47403278946876526\n",
            " - layer1.bias grad norm: 0.0005352271837182343\n",
            " - layer2.weight grad norm: 0.0018327806610614061\n",
            " - layer2.bias grad norm: 0.0006325356080196798\n",
            " - layer2_input.weight grad norm: 0.5427746176719666\n",
            " - layer2_input.bias grad norm: 0.0006325356080196798\n",
            " - layer3.weight grad norm: 0.002533629536628723\n",
            " - layer3.bias grad norm: 0.0006018663989380002\n",
            " - layer3_input.weight grad norm: 0.5155084133148193\n",
            " - layer3_input.bias grad norm: 0.0006018663989380002\n",
            " - layer4.weight grad norm: 0.0030345928389579058\n",
            " - layer4.bias grad norm: 0.000532045029103756\n",
            " - layer4_input.weight grad norm: 0.4634063243865967\n",
            " - layer4_input.bias grad norm: 0.000532045029103756\n",
            " - layer5.weight grad norm: 0.012030602432787418\n",
            " - layer5.bias grad norm: 0.00528683653101325\n",
            "Gradients at iteration 672:\n",
            " - layer1.weight grad norm: 0.5665812492370605\n",
            " - layer1.bias grad norm: 0.0006572777638211846\n",
            " - layer2.weight grad norm: 0.0017869395669549704\n",
            " - layer2.bias grad norm: 0.0005497637903317809\n",
            " - layer2_input.weight grad norm: 0.4823662042617798\n",
            " - layer2_input.bias grad norm: 0.0005497637903317809\n",
            " - layer3.weight grad norm: 0.002470308681949973\n",
            " - layer3.bias grad norm: 0.0005685884971171618\n",
            " - layer3_input.weight grad norm: 0.4918093979358673\n",
            " - layer3_input.bias grad norm: 0.0005685884971171618\n",
            " - layer4.weight grad norm: 0.0029676584526896477\n",
            " - layer4.bias grad norm: 0.0005230134702287614\n",
            " - layer4_input.weight grad norm: 0.4519561231136322\n",
            " - layer4_input.bias grad norm: 0.0005230134702287614\n",
            " - layer5.weight grad norm: 0.010993211530148983\n",
            " - layer5.bias grad norm: 0.00515996478497982\n",
            "Gradients at iteration 673:\n",
            " - layer1.weight grad norm: 0.5194900035858154\n",
            " - layer1.bias grad norm: 0.0005997649859637022\n",
            " - layer2.weight grad norm: 0.001781709841452539\n",
            " - layer2.bias grad norm: 0.0005690142279490829\n",
            " - layer2_input.weight grad norm: 0.4929850399494171\n",
            " - layer2_input.bias grad norm: 0.0005690142279490829\n",
            " - layer3.weight grad norm: 0.0024361598771065474\n",
            " - layer3.bias grad norm: 0.0005322607466951013\n",
            " - layer3_input.weight grad norm: 0.46567201614379883\n",
            " - layer3_input.bias grad norm: 0.0005322607466951013\n",
            " - layer4.weight grad norm: 0.002924602944403887\n",
            " - layer4.bias grad norm: 0.0006091492250561714\n",
            " - layer4_input.weight grad norm: 0.5196462869644165\n",
            " - layer4_input.bias grad norm: 0.0006091492250561714\n",
            " - layer5.weight grad norm: 0.012943328358232975\n",
            " - layer5.bias grad norm: 0.0050652590580284595\n",
            "Gradients at iteration 674:\n",
            " - layer1.weight grad norm: 0.49397093057632446\n",
            " - layer1.bias grad norm: 0.0005661780596710742\n",
            " - layer2.weight grad norm: 0.0019074729643762112\n",
            " - layer2.bias grad norm: 0.0006053370889276266\n",
            " - layer2_input.weight grad norm: 0.5218062996864319\n",
            " - layer2_input.bias grad norm: 0.0006053370889276266\n",
            " - layer3.weight grad norm: 0.0025605722330510616\n",
            " - layer3.bias grad norm: 0.0005883167032152414\n",
            " - layer3_input.weight grad norm: 0.5094989538192749\n",
            " - layer3_input.bias grad norm: 0.0005883167032152414\n",
            " - layer4.weight grad norm: 0.0030903571750968695\n",
            " - layer4.bias grad norm: 0.0005440447712317109\n",
            " - layer4_input.weight grad norm: 0.4732135534286499\n",
            " - layer4_input.bias grad norm: 0.0005440447712317109\n",
            " - layer5.weight grad norm: 0.01183103397488594\n",
            " - layer5.bias grad norm: 0.005344786215573549\n",
            "Gradients at iteration 675:\n",
            " - layer1.weight grad norm: 0.529300332069397\n",
            " - layer1.bias grad norm: 0.0006101175677031279\n",
            " - layer2.weight grad norm: 0.0018066392512992024\n",
            " - layer2.bias grad norm: 0.0006246206467039883\n",
            " - layer2_input.weight grad norm: 0.5372708439826965\n",
            " - layer2_input.bias grad norm: 0.0006246206467039883\n",
            " - layer3.weight grad norm: 0.002509454730898142\n",
            " - layer3.bias grad norm: 0.0005376270855776966\n",
            " - layer3_input.weight grad norm: 0.47062432765960693\n",
            " - layer3_input.bias grad norm: 0.0005376270855776966\n",
            " - layer4.weight grad norm: 0.002975242678076029\n",
            " - layer4.bias grad norm: 0.0005201362655498087\n",
            " - layer4_input.weight grad norm: 0.4577406644821167\n",
            " - layer4_input.bias grad norm: 0.0005201362655498087\n",
            " - layer5.weight grad norm: 0.010941014625132084\n",
            " - layer5.bias grad norm: 0.005197450518608093\n",
            "Gradients at iteration 676:\n",
            " - layer1.weight grad norm: 0.5019353628158569\n",
            " - layer1.bias grad norm: 0.0005730405100621283\n",
            " - layer2.weight grad norm: 0.0019153195898979902\n",
            " - layer2.bias grad norm: 0.0005414012703113258\n",
            " - layer2_input.weight grad norm: 0.47147566080093384\n",
            " - layer2_input.bias grad norm: 0.0005414012703113258\n",
            " - layer3.weight grad norm: 0.002647731453180313\n",
            " - layer3.bias grad norm: 0.0006381887360475957\n",
            " - layer3_input.weight grad norm: 0.5437729954719543\n",
            " - layer3_input.bias grad norm: 0.0006381887360475957\n",
            " - layer4.weight grad norm: 0.0030621595215052366\n",
            " - layer4.bias grad norm: 0.0005537488032132387\n",
            " - layer4_input.weight grad norm: 0.47948357462882996\n",
            " - layer4_input.bias grad norm: 0.0005537488032132387\n",
            " - layer5.weight grad norm: 0.011224419809877872\n",
            " - layer5.bias grad norm: 0.00544363260269165\n",
            "Gradients at iteration 677:\n",
            " - layer1.weight grad norm: 0.5625371932983398\n",
            " - layer1.bias grad norm: 0.0006639204220846295\n",
            " - layer2.weight grad norm: 0.001890807063318789\n",
            " - layer2.bias grad norm: 0.0005481830448843539\n",
            " - layer2_input.weight grad norm: 0.4745921790599823\n",
            " - layer2_input.bias grad norm: 0.0005481830448843539\n",
            " - layer3.weight grad norm: 0.002564092632383108\n",
            " - layer3.bias grad norm: 0.0005885033751837909\n",
            " - layer3_input.weight grad norm: 0.5031842589378357\n",
            " - layer3_input.bias grad norm: 0.0005885033751837909\n",
            " - layer4.weight grad norm: 0.003028267528861761\n",
            " - layer4.bias grad norm: 0.0005244620842859149\n",
            " - layer4_input.weight grad norm: 0.45272910594940186\n",
            " - layer4_input.bias grad norm: 0.0005244620842859149\n",
            " - layer5.weight grad norm: 0.010290422476828098\n",
            " - layer5.bias grad norm: 0.00534967752173543\n",
            "Gradients at iteration 678:\n",
            " - layer1.weight grad norm: 0.4930480718612671\n",
            " - layer1.bias grad norm: 0.00056586938444525\n",
            " - layer2.weight grad norm: 0.001824307139031589\n",
            " - layer2.bias grad norm: 0.0005648531368933618\n",
            " - layer2_input.weight grad norm: 0.48893609642982483\n",
            " - layer2_input.bias grad norm: 0.0005648531368933618\n",
            " - layer3.weight grad norm: 0.0025142948143184185\n",
            " - layer3.bias grad norm: 0.0006044787005521357\n",
            " - layer3_input.weight grad norm: 0.5169975757598877\n",
            " - layer3_input.bias grad norm: 0.0006044787005521357\n",
            " - layer4.weight grad norm: 0.0029578940011560917\n",
            " - layer4.bias grad norm: 0.0005828227149322629\n",
            " - layer4_input.weight grad norm: 0.5003762245178223\n",
            " - layer4_input.bias grad norm: 0.0005828227149322629\n",
            " - layer5.weight grad norm: 0.011610702611505985\n",
            " - layer5.bias grad norm: 0.005153735633939505\n",
            "Gradients at iteration 679:\n",
            " - layer1.weight grad norm: 0.5355768799781799\n",
            " - layer1.bias grad norm: 0.0006157237221486866\n",
            " - layer2.weight grad norm: 0.0018891127547249198\n",
            " - layer2.bias grad norm: 0.0006010096403770149\n",
            " - layer2_input.weight grad norm: 0.5171201229095459\n",
            " - layer2_input.bias grad norm: 0.0006010096403770149\n",
            " - layer3.weight grad norm: 0.002621931256726384\n",
            " - layer3.bias grad norm: 0.0005685296491719782\n",
            " - layer3_input.weight grad norm: 0.49294134974479675\n",
            " - layer3_input.bias grad norm: 0.0005685296491719782\n",
            " - layer4.weight grad norm: 0.0030828388407826424\n",
            " - layer4.bias grad norm: 0.0005136600811965764\n",
            " - layer4_input.weight grad norm: 0.45005548000335693\n",
            " - layer4_input.bias grad norm: 0.0005136600811965764\n",
            " - layer5.weight grad norm: 0.01233337726444006\n",
            " - layer5.bias grad norm: 0.005374306347221136\n",
            "Gradients at iteration 680:\n",
            " - layer1.weight grad norm: 0.5297543406486511\n",
            " - layer1.bias grad norm: 0.00062395358690992\n",
            " - layer2.weight grad norm: 0.0017649271758273244\n",
            " - layer2.bias grad norm: 0.0005910207983106375\n",
            " - layer2_input.weight grad norm: 0.5033506751060486\n",
            " - layer2_input.bias grad norm: 0.0005910207983106375\n",
            " - layer3.weight grad norm: 0.0024149655364453793\n",
            " - layer3.bias grad norm: 0.0006214418681338429\n",
            " - layer3_input.weight grad norm: 0.525775671005249\n",
            " - layer3_input.bias grad norm: 0.0006214418681338429\n",
            " - layer4.weight grad norm: 0.002889293245971203\n",
            " - layer4.bias grad norm: 0.0004974655457772315\n",
            " - layer4_input.weight grad norm: 0.4351975619792938\n",
            " - layer4_input.bias grad norm: 0.0004974655457772315\n",
            " - layer5.weight grad norm: 0.010820269584655762\n",
            " - layer5.bias grad norm: 0.004983340390026569\n",
            "Gradients at iteration 681:\n",
            " - layer1.weight grad norm: 0.5029769539833069\n",
            " - layer1.bias grad norm: 0.000581399304792285\n",
            " - layer2.weight grad norm: 0.0017863899702206254\n",
            " - layer2.bias grad norm: 0.00062472780700773\n",
            " - layer2_input.weight grad norm: 0.5359953045845032\n",
            " - layer2_input.bias grad norm: 0.00062472780700773\n",
            " - layer3.weight grad norm: 0.002442128024995327\n",
            " - layer3.bias grad norm: 0.0005117355613037944\n",
            " - layer3_input.weight grad norm: 0.4454496502876282\n",
            " - layer3_input.bias grad norm: 0.0005117355613037944\n",
            " - layer4.weight grad norm: 0.0029051455203443766\n",
            " - layer4.bias grad norm: 0.0006009325734339654\n",
            " - layer4_input.weight grad norm: 0.5109853744506836\n",
            " - layer4_input.bias grad norm: 0.0006009325734339654\n",
            " - layer5.weight grad norm: 0.012101775035262108\n",
            " - layer5.bias grad norm: 0.005032307934015989\n",
            "Gradients at iteration 682:\n",
            " - layer1.weight grad norm: 0.5003033876419067\n",
            " - layer1.bias grad norm: 0.0005818881909362972\n",
            " - layer2.weight grad norm: 0.0018172322306782007\n",
            " - layer2.bias grad norm: 0.0006237014895305037\n",
            " - layer2_input.weight grad norm: 0.5314456224441528\n",
            " - layer2_input.bias grad norm: 0.0006237014895305037\n",
            " - layer3.weight grad norm: 0.0025076826568692923\n",
            " - layer3.bias grad norm: 0.0005773917073383927\n",
            " - layer3_input.weight grad norm: 0.4957507848739624\n",
            " - layer3_input.bias grad norm: 0.0005773917073383927\n",
            " - layer4.weight grad norm: 0.0029934595804661512\n",
            " - layer4.bias grad norm: 0.0005477667436935008\n",
            " - layer4_input.weight grad norm: 0.47042566537857056\n",
            " - layer4_input.bias grad norm: 0.0005477667436935008\n",
            " - layer5.weight grad norm: 0.012046064250171185\n",
            " - layer5.bias grad norm: 0.005176784470677376\n",
            "Gradients at iteration 683:\n",
            " - layer1.weight grad norm: 0.5221467018127441\n",
            " - layer1.bias grad norm: 0.0006046818452887237\n",
            " - layer2.weight grad norm: 0.0017930822214111686\n",
            " - layer2.bias grad norm: 0.0005447319708764553\n",
            " - layer2_input.weight grad norm: 0.4747912883758545\n",
            " - layer2_input.bias grad norm: 0.0005447319708764553\n",
            " - layer3.weight grad norm: 0.002453204244375229\n",
            " - layer3.bias grad norm: 0.0005892444751225412\n",
            " - layer3_input.weight grad norm: 0.5057847499847412\n",
            " - layer3_input.bias grad norm: 0.0005892444751225412\n",
            " - layer4.weight grad norm: 0.0029013720341026783\n",
            " - layer4.bias grad norm: 0.0005764918751083314\n",
            " - layer4_input.weight grad norm: 0.4959215521812439\n",
            " - layer4_input.bias grad norm: 0.0005764918751083314\n",
            " - layer5.weight grad norm: 0.011570178903639317\n",
            " - layer5.bias grad norm: 0.005080750677734613\n",
            "Gradients at iteration 684:\n",
            " - layer1.weight grad norm: 0.5472744703292847\n",
            " - layer1.bias grad norm: 0.0006390597554855049\n",
            " - layer2.weight grad norm: 0.0018776841461658478\n",
            " - layer2.bias grad norm: 0.0005583063466474414\n",
            " - layer2_input.weight grad norm: 0.48446914553642273\n",
            " - layer2_input.bias grad norm: 0.0005583063466474414\n",
            " - layer3.weight grad norm: 0.0025904783979058266\n",
            " - layer3.bias grad norm: 0.000577140657696873\n",
            " - layer3_input.weight grad norm: 0.4917553663253784\n",
            " - layer3_input.bias grad norm: 0.000577140657696873\n",
            " - layer4.weight grad norm: 0.003064746968448162\n",
            " - layer4.bias grad norm: 0.0005499400431290269\n",
            " - layer4_input.weight grad norm: 0.47303467988967896\n",
            " - layer4_input.bias grad norm: 0.0005499400431290269\n",
            " - layer5.weight grad norm: 0.01204006839543581\n",
            " - layer5.bias grad norm: 0.0053090970031917095\n",
            "Gradients at iteration 685:\n",
            " - layer1.weight grad norm: 0.5278300046920776\n",
            " - layer1.bias grad norm: 0.0006201947689987719\n",
            " - layer2.weight grad norm: 0.0018853323999792337\n",
            " - layer2.bias grad norm: 0.0006073767435736954\n",
            " - layer2_input.weight grad norm: 0.5139595866203308\n",
            " - layer2_input.bias grad norm: 0.0006073767435736954\n",
            " - layer3.weight grad norm: 0.002549896016716957\n",
            " - layer3.bias grad norm: 0.0005346508114598691\n",
            " - layer3_input.weight grad norm: 0.4564248025417328\n",
            " - layer3_input.bias grad norm: 0.0005346508114598691\n",
            " - layer4.weight grad norm: 0.003056938061490655\n",
            " - layer4.bias grad norm: 0.0005932667409069836\n",
            " - layer4_input.weight grad norm: 0.4987119436264038\n",
            " - layer4_input.bias grad norm: 0.0005932667409069836\n",
            " - layer5.weight grad norm: 0.012406832538545132\n",
            " - layer5.bias grad norm: 0.005307357758283615\n",
            "Gradients at iteration 686:\n",
            " - layer1.weight grad norm: 0.49830764532089233\n",
            " - layer1.bias grad norm: 0.0005671472754329443\n",
            " - layer2.weight grad norm: 0.0018458031117916107\n",
            " - layer2.bias grad norm: 0.0006248772260732949\n",
            " - layer2_input.weight grad norm: 0.5366970300674438\n",
            " - layer2_input.bias grad norm: 0.0006248772260732949\n",
            " - layer3.weight grad norm: 0.0025156240444630384\n",
            " - layer3.bias grad norm: 0.0005944540607742965\n",
            " - layer3_input.weight grad norm: 0.5084301829338074\n",
            " - layer3_input.bias grad norm: 0.0005944540607742965\n",
            " - layer4.weight grad norm: 0.0030371160246431828\n",
            " - layer4.bias grad norm: 0.0005215970450080931\n",
            " - layer4_input.weight grad norm: 0.45272892713546753\n",
            " - layer4_input.bias grad norm: 0.0005215970450080931\n",
            " - layer5.weight grad norm: 0.011492249555885792\n",
            " - layer5.bias grad norm: 0.005249535199254751\n",
            "Gradients at iteration 687:\n",
            " - layer1.weight grad norm: 0.5075992941856384\n",
            " - layer1.bias grad norm: 0.000584630121011287\n",
            " - layer2.weight grad norm: 0.0018338954541832209\n",
            " - layer2.bias grad norm: 0.0005332592409104109\n",
            " - layer2_input.weight grad norm: 0.46963611245155334\n",
            " - layer2_input.bias grad norm: 0.0005332592409104109\n",
            " - layer3.weight grad norm: 0.0025136820040643215\n",
            " - layer3.bias grad norm: 0.0006127695087343454\n",
            " - layer3_input.weight grad norm: 0.5226608514785767\n",
            " - layer3_input.bias grad norm: 0.0006127695087343454\n",
            " - layer4.weight grad norm: 0.0029571938794106245\n",
            " - layer4.bias grad norm: 0.0005764456582255661\n",
            " - layer4_input.weight grad norm: 0.4984174370765686\n",
            " - layer4_input.bias grad norm: 0.0005764456582255661\n",
            " - layer5.weight grad norm: 0.011960349045693874\n",
            " - layer5.bias grad norm: 0.005190609022974968\n",
            "Gradients at iteration 688:\n",
            " - layer1.weight grad norm: 0.5361307263374329\n",
            " - layer1.bias grad norm: 0.00062899524345994\n",
            " - layer2.weight grad norm: 0.001853225752711296\n",
            " - layer2.bias grad norm: 0.000506572425365448\n",
            " - layer2_input.weight grad norm: 0.4505205750465393\n",
            " - layer2_input.bias grad norm: 0.000506572425365448\n",
            " - layer3.weight grad norm: 0.00253726402297616\n",
            " - layer3.bias grad norm: 0.0005892907502129674\n",
            " - layer3_input.weight grad norm: 0.5075719356536865\n",
            " - layer3_input.bias grad norm: 0.0005892907502129674\n",
            " - layer4.weight grad norm: 0.0030022435821592808\n",
            " - layer4.bias grad norm: 0.0005866201245225966\n",
            " - layer4_input.weight grad norm: 0.5017828941345215\n",
            " - layer4_input.bias grad norm: 0.0005866201245225966\n",
            " - layer5.weight grad norm: 0.011463475413620472\n",
            " - layer5.bias grad norm: 0.005220414604991674\n",
            "Gradients at iteration 689:\n",
            " - layer1.weight grad norm: 0.5517950057983398\n",
            " - layer1.bias grad norm: 0.0006465438636951149\n",
            " - layer2.weight grad norm: 0.0018197910394519567\n",
            " - layer2.bias grad norm: 0.0005905695725232363\n",
            " - layer2_input.weight grad norm: 0.5082514882087708\n",
            " - layer2_input.bias grad norm: 0.0005905695725232363\n",
            " - layer3.weight grad norm: 0.002470431150868535\n",
            " - layer3.bias grad norm: 0.000541502726264298\n",
            " - layer3_input.weight grad norm: 0.46481403708457947\n",
            " - layer3_input.bias grad norm: 0.000541502726264298\n",
            " - layer4.weight grad norm: 0.0029804909136146307\n",
            " - layer4.bias grad norm: 0.0005456679500639439\n",
            " - layer4_input.weight grad norm: 0.47008901834487915\n",
            " - layer4_input.bias grad norm: 0.0005456679500639439\n",
            " - layer5.weight grad norm: 0.010929114185273647\n",
            " - layer5.bias grad norm: 0.005181786138564348\n",
            "Gradients at iteration 690:\n",
            " - layer1.weight grad norm: 0.5098029971122742\n",
            " - layer1.bias grad norm: 0.0005844156839884818\n",
            " - layer2.weight grad norm: 0.0019269860349595547\n",
            " - layer2.bias grad norm: 0.0005669596139341593\n",
            " - layer2_input.weight grad norm: 0.49768349528312683\n",
            " - layer2_input.bias grad norm: 0.0005669596139341593\n",
            " - layer3.weight grad norm: 0.0026772376149892807\n",
            " - layer3.bias grad norm: 0.00052505056373775\n",
            " - layer3_input.weight grad norm: 0.46284618973731995\n",
            " - layer3_input.bias grad norm: 0.00052505056373775\n",
            " - layer4.weight grad norm: 0.0031566289253532887\n",
            " - layer4.bias grad norm: 0.0006175253656692803\n",
            " - layer4_input.weight grad norm: 0.5272454023361206\n",
            " - layer4_input.bias grad norm: 0.0006175253656692803\n",
            " - layer5.weight grad norm: 0.012030914425849915\n",
            " - layer5.bias grad norm: 0.0054613566026091576\n",
            "Gradients at iteration 691:\n",
            " - layer1.weight grad norm: 0.5273032188415527\n",
            " - layer1.bias grad norm: 0.0006055128760635853\n",
            " - layer2.weight grad norm: 0.0020326385274529457\n",
            " - layer2.bias grad norm: 0.0005584745085798204\n",
            " - layer2_input.weight grad norm: 0.4888395667076111\n",
            " - layer2_input.bias grad norm: 0.0005584745085798204\n",
            " - layer3.weight grad norm: 0.002742831828072667\n",
            " - layer3.bias grad norm: 0.0005803678068332374\n",
            " - layer3_input.weight grad norm: 0.49821633100509644\n",
            " - layer3_input.bias grad norm: 0.0005803678068332374\n",
            " - layer4.weight grad norm: 0.003265456994995475\n",
            " - layer4.bias grad norm: 0.000554941943846643\n",
            " - layer4_input.weight grad norm: 0.4842962920665741\n",
            " - layer4_input.bias grad norm: 0.000554941943846643\n",
            " - layer5.weight grad norm: 0.012943713925778866\n",
            " - layer5.bias grad norm: 0.005720230285078287\n",
            "Gradients at iteration 692:\n",
            " - layer1.weight grad norm: 0.5489078164100647\n",
            " - layer1.bias grad norm: 0.0006377440877258778\n",
            " - layer2.weight grad norm: 0.001842381083406508\n",
            " - layer2.bias grad norm: 0.0006013922393321991\n",
            " - layer2_input.weight grad norm: 0.5163159966468811\n",
            " - layer2_input.bias grad norm: 0.0006013922393321991\n",
            " - layer3.weight grad norm: 0.0025516077876091003\n",
            " - layer3.bias grad norm: 0.0006016106344759464\n",
            " - layer3_input.weight grad norm: 0.5120357871055603\n",
            " - layer3_input.bias grad norm: 0.0006016106344759464\n",
            " - layer4.weight grad norm: 0.0030512227676808834\n",
            " - layer4.bias grad norm: 0.0004639874678105116\n",
            " - layer4_input.weight grad norm: 0.4120182394981384\n",
            " - layer4_input.bias grad norm: 0.0004639874678105116\n",
            " - layer5.weight grad norm: 0.01136147603392601\n",
            " - layer5.bias grad norm: 0.005269474349915981\n",
            "Gradients at iteration 693:\n",
            " - layer1.weight grad norm: 0.5353119373321533\n",
            " - layer1.bias grad norm: 0.0006190260755829513\n",
            " - layer2.weight grad norm: 0.0018155097495764494\n",
            " - layer2.bias grad norm: 0.0005601135781034827\n",
            " - layer2_input.weight grad norm: 0.48600155115127563\n",
            " - layer2_input.bias grad norm: 0.0005601135781034827\n",
            " - layer3.weight grad norm: 0.00249328906647861\n",
            " - layer3.bias grad norm: 0.0006040100124664605\n",
            " - layer3_input.weight grad norm: 0.5159845352172852\n",
            " - layer3_input.bias grad norm: 0.0006040100124664605\n",
            " - layer4.weight grad norm: 0.002980178454890847\n",
            " - layer4.bias grad norm: 0.0005296118906699121\n",
            " - layer4_input.weight grad norm: 0.45914629101753235\n",
            " - layer4_input.bias grad norm: 0.0005296118906699121\n",
            " - layer5.weight grad norm: 0.01186002604663372\n",
            " - layer5.bias grad norm: 0.0051862518303096294\n",
            "Gradients at iteration 694:\n",
            " - layer1.weight grad norm: 0.4902622699737549\n",
            " - layer1.bias grad norm: 0.0005586298648267984\n",
            " - layer2.weight grad norm: 0.0018587001832202077\n",
            " - layer2.bias grad norm: 0.0005596140399575233\n",
            " - layer2_input.weight grad norm: 0.48965978622436523\n",
            " - layer2_input.bias grad norm: 0.0005596140399575233\n",
            " - layer3.weight grad norm: 0.0025642316322773695\n",
            " - layer3.bias grad norm: 0.0006267090793699026\n",
            " - layer3_input.weight grad norm: 0.5346996784210205\n",
            " - layer3_input.bias grad norm: 0.0006267090793699026\n",
            " - layer4.weight grad norm: 0.0029957301449030638\n",
            " - layer4.bias grad norm: 0.0005596490809693933\n",
            " - layer4_input.weight grad norm: 0.48350173234939575\n",
            " - layer4_input.bias grad norm: 0.0005596490809693933\n",
            " - layer5.weight grad norm: 0.012228221632540226\n",
            " - layer5.bias grad norm: 0.00525313476100564\n",
            "Gradients at iteration 695:\n",
            " - layer1.weight grad norm: 0.5151740908622742\n",
            " - layer1.bias grad norm: 0.0006008968339301646\n",
            " - layer2.weight grad norm: 0.0018275940092280507\n",
            " - layer2.bias grad norm: 0.0005655742133967578\n",
            " - layer2_input.weight grad norm: 0.4873267412185669\n",
            " - layer2_input.bias grad norm: 0.0005655742133967578\n",
            " - layer3.weight grad norm: 0.002459378680214286\n",
            " - layer3.bias grad norm: 0.0005446993745863438\n",
            " - layer3_input.weight grad norm: 0.4700031280517578\n",
            " - layer3_input.bias grad norm: 0.0005446993745863438\n",
            " - layer4.weight grad norm: 0.0030082808807492256\n",
            " - layer4.bias grad norm: 0.0006197537295520306\n",
            " - layer4_input.weight grad norm: 0.5253390669822693\n",
            " - layer4_input.bias grad norm: 0.0006197537295520306\n",
            " - layer5.weight grad norm: 0.013296266086399555\n",
            " - layer5.bias grad norm: 0.0051621850579977036\n",
            "Gradients at iteration 696:\n",
            " - layer1.weight grad norm: 0.48944905400276184\n",
            " - layer1.bias grad norm: 0.0005647279904223979\n",
            " - layer2.weight grad norm: 0.001790219801478088\n",
            " - layer2.bias grad norm: 0.0006145802908577025\n",
            " - layer2_input.weight grad norm: 0.5267972946166992\n",
            " - layer2_input.bias grad norm: 0.0006145802908577025\n",
            " - layer3.weight grad norm: 0.002438777359202504\n",
            " - layer3.bias grad norm: 0.0005791546427644789\n",
            " - layer3_input.weight grad norm: 0.4968598186969757\n",
            " - layer3_input.bias grad norm: 0.0005791546427644789\n",
            " - layer4.weight grad norm: 0.0029151870403438807\n",
            " - layer4.bias grad norm: 0.0005653166444972157\n",
            " - layer4_input.weight grad norm: 0.4856596291065216\n",
            " - layer4_input.bias grad norm: 0.0005653166444972157\n",
            " - layer5.weight grad norm: 0.0119914710521698\n",
            " - layer5.bias grad norm: 0.005048113409429789\n",
            "Gradients at iteration 697:\n",
            " - layer1.weight grad norm: 0.5248813629150391\n",
            " - layer1.bias grad norm: 0.000601285370066762\n",
            " - layer2.weight grad norm: 0.0018266703700646758\n",
            " - layer2.bias grad norm: 0.0005896251532249153\n",
            " - layer2_input.weight grad norm: 0.5134384632110596\n",
            " - layer2_input.bias grad norm: 0.0005896251532249153\n",
            " - layer3.weight grad norm: 0.002543848706409335\n",
            " - layer3.bias grad norm: 0.0005133624072186649\n",
            " - layer3_input.weight grad norm: 0.45485514402389526\n",
            " - layer3_input.bias grad norm: 0.0005133624072186649\n",
            " - layer4.weight grad norm: 0.0029512825421988964\n",
            " - layer4.bias grad norm: 0.000576675811316818\n",
            " - layer4_input.weight grad norm: 0.5038007497787476\n",
            " - layer4_input.bias grad norm: 0.000576675811316818\n",
            " - layer5.weight grad norm: 0.011182990856468678\n",
            " - layer5.bias grad norm: 0.005130826495587826\n",
            "Gradients at iteration 698:\n",
            " - layer1.weight grad norm: 0.5440337061882019\n",
            " - layer1.bias grad norm: 0.0006317269871942699\n",
            " - layer2.weight grad norm: 0.0018390727927908301\n",
            " - layer2.bias grad norm: 0.0005524432053789496\n",
            " - layer2_input.weight grad norm: 0.4811326563358307\n",
            " - layer2_input.bias grad norm: 0.0005524432053789496\n",
            " - layer3.weight grad norm: 0.002435544738546014\n",
            " - layer3.bias grad norm: 0.0005876702489331365\n",
            " - layer3_input.weight grad norm: 0.5032163858413696\n",
            " - layer3_input.bias grad norm: 0.0005876702489331365\n",
            " - layer4.weight grad norm: 0.002917855279520154\n",
            " - layer4.bias grad norm: 0.0005429942393675447\n",
            " - layer4_input.weight grad norm: 0.4681110382080078\n",
            " - layer4_input.bias grad norm: 0.0005429942393675447\n",
            " - layer5.weight grad norm: 0.011729340068995953\n",
            " - layer5.bias grad norm: 0.0051150149665772915\n",
            "Gradients at iteration 699:\n",
            " - layer1.weight grad norm: 0.523922324180603\n",
            " - layer1.bias grad norm: 0.0006101069739088416\n",
            " - layer2.weight grad norm: 0.0017766787204891443\n",
            " - layer2.bias grad norm: 0.0005821564118377864\n",
            " - layer2_input.weight grad norm: 0.49948716163635254\n",
            " - layer2_input.bias grad norm: 0.0005821564118377864\n",
            " - layer3.weight grad norm: 0.0024806400761008263\n",
            " - layer3.bias grad norm: 0.0006133558927103877\n",
            " - layer3_input.weight grad norm: 0.522838830947876\n",
            " - layer3_input.bias grad norm: 0.0006133558927103877\n",
            " - layer4.weight grad norm: 0.0028858219739049673\n",
            " - layer4.bias grad norm: 0.0005184978363104165\n",
            " - layer4_input.weight grad norm: 0.44995561242103577\n",
            " - layer4_input.bias grad norm: 0.0005184978363104165\n",
            " - layer5.weight grad norm: 0.012308238074183464\n",
            " - layer5.bias grad norm: 0.005095484666526318\n",
            "Gradients at iteration 700:\n",
            " - layer1.weight grad norm: 0.5027293562889099\n",
            " - layer1.bias grad norm: 0.0005841006641276181\n",
            " - layer2.weight grad norm: 0.0018486195476725698\n",
            " - layer2.bias grad norm: 0.0006046370835974813\n",
            " - layer2_input.weight grad norm: 0.5199034810066223\n",
            " - layer2_input.bias grad norm: 0.0006046370835974813\n",
            " - layer3.weight grad norm: 0.002507065422832966\n",
            " - layer3.bias grad norm: 0.0005710504483431578\n",
            " - layer3_input.weight grad norm: 0.49319368600845337\n",
            " - layer3_input.bias grad norm: 0.0005710504483431578\n",
            " - layer4.weight grad norm: 0.0030343541875481606\n",
            " - layer4.bias grad norm: 0.0005650697858072817\n",
            " - layer4_input.weight grad norm: 0.48325127363204956\n",
            " - layer4_input.bias grad norm: 0.0005650697858072817\n",
            " - layer5.weight grad norm: 0.011992779560387135\n",
            " - layer5.bias grad norm: 0.005170237738639116\n",
            "It: 700, Loss: 5.358e+13, Y0: -3.015, Time: 1.58, Learning Rate: 1.000e-03\n",
            "Gradients at iteration 701:\n",
            " - layer1.weight grad norm: 0.527805507183075\n",
            " - layer1.bias grad norm: 0.0006144430371932685\n",
            " - layer2.weight grad norm: 0.0018452092772349715\n",
            " - layer2.bias grad norm: 0.0005794546450488269\n",
            " - layer2_input.weight grad norm: 0.4963372051715851\n",
            " - layer2_input.bias grad norm: 0.0005794546450488269\n",
            " - layer3.weight grad norm: 0.0025067024398595095\n",
            " - layer3.bias grad norm: 0.0005847493303008378\n",
            " - layer3_input.weight grad norm: 0.5047088861465454\n",
            " - layer3_input.bias grad norm: 0.0005847493303008378\n",
            " - layer4.weight grad norm: 0.0029978780075907707\n",
            " - layer4.bias grad norm: 0.0005476666265167296\n",
            " - layer4_input.weight grad norm: 0.46918603777885437\n",
            " - layer4_input.bias grad norm: 0.0005476666265167296\n",
            " - layer5.weight grad norm: 0.012495770119130611\n",
            " - layer5.bias grad norm: 0.0052063134498894215\n",
            "Gradients at iteration 702:\n",
            " - layer1.weight grad norm: 0.5537440180778503\n",
            " - layer1.bias grad norm: 0.0006555411964654922\n",
            " - layer2.weight grad norm: 0.0018106236821040511\n",
            " - layer2.bias grad norm: 0.0005802332307212055\n",
            " - layer2_input.weight grad norm: 0.4976806044578552\n",
            " - layer2_input.bias grad norm: 0.0005802332307212055\n",
            " - layer3.weight grad norm: 0.002480199793353677\n",
            " - layer3.bias grad norm: 0.0005996575928293169\n",
            " - layer3_input.weight grad norm: 0.5102140307426453\n",
            " - layer3_input.bias grad norm: 0.0005996575928293169\n",
            " - layer4.weight grad norm: 0.00296974228695035\n",
            " - layer4.bias grad norm: 0.0004973619361408055\n",
            " - layer4_input.weight grad norm: 0.43031564354896545\n",
            " - layer4_input.bias grad norm: 0.0004973619361408055\n",
            " - layer5.weight grad norm: 0.012023978866636753\n",
            " - layer5.bias grad norm: 0.005153525620698929\n",
            "Gradients at iteration 703:\n",
            " - layer1.weight grad norm: 0.5049262642860413\n",
            " - layer1.bias grad norm: 0.0005778636550530791\n",
            " - layer2.weight grad norm: 0.0018418199615553021\n",
            " - layer2.bias grad norm: 0.0005718964384868741\n",
            " - layer2_input.weight grad norm: 0.49918246269226074\n",
            " - layer2_input.bias grad norm: 0.0005718964384868741\n",
            " - layer3.weight grad norm: 0.0025113869924098253\n",
            " - layer3.bias grad norm: 0.0005851424066349864\n",
            " - layer3_input.weight grad norm: 0.5019634366035461\n",
            " - layer3_input.bias grad norm: 0.0005851424066349864\n",
            " - layer4.weight grad norm: 0.0030128471553325653\n",
            " - layer4.bias grad norm: 0.0005736342864111066\n",
            " - layer4_input.weight grad norm: 0.49366238713264465\n",
            " - layer4_input.bias grad norm: 0.0005736342864111066\n",
            " - layer5.weight grad norm: 0.012183546088635921\n",
            " - layer5.bias grad norm: 0.0051897428929805756\n",
            "Gradients at iteration 704:\n",
            " - layer1.weight grad norm: 0.47717738151550293\n",
            " - layer1.bias grad norm: 0.0005398011999204755\n",
            " - layer2.weight grad norm: 0.001824608538299799\n",
            " - layer2.bias grad norm: 0.0006362613057717681\n",
            " - layer2_input.weight grad norm: 0.5406813621520996\n",
            " - layer2_input.bias grad norm: 0.0006362613057717681\n",
            " - layer3.weight grad norm: 0.0025074370205402374\n",
            " - layer3.bias grad norm: 0.0005505592562258244\n",
            " - layer3_input.weight grad norm: 0.4787258803844452\n",
            " - layer3_input.bias grad norm: 0.0005505592562258244\n",
            " - layer4.weight grad norm: 0.002969049382954836\n",
            " - layer4.bias grad norm: 0.0005873770569451153\n",
            " - layer4_input.weight grad norm: 0.5005925893783569\n",
            " - layer4_input.bias grad norm: 0.0005873770569451153\n",
            " - layer5.weight grad norm: 0.012106439098715782\n",
            " - layer5.bias grad norm: 0.005147615913301706\n",
            "Gradients at iteration 705:\n",
            " - layer1.weight grad norm: 0.5191115140914917\n",
            " - layer1.bias grad norm: 0.0006009656353853643\n",
            " - layer2.weight grad norm: 0.001767816487699747\n",
            " - layer2.bias grad norm: 0.0005781569052487612\n",
            " - layer2_input.weight grad norm: 0.5015625953674316\n",
            " - layer2_input.bias grad norm: 0.0005781569052487612\n",
            " - layer3.weight grad norm: 0.0024037561379373074\n",
            " - layer3.bias grad norm: 0.0005864446866326034\n",
            " - layer3_input.weight grad norm: 0.5063717365264893\n",
            " - layer3_input.bias grad norm: 0.0005864446866326034\n",
            " - layer4.weight grad norm: 0.002865765243768692\n",
            " - layer4.bias grad norm: 0.00053872523130849\n",
            " - layer4_input.weight grad norm: 0.4715476334095001\n",
            " - layer4_input.bias grad norm: 0.00053872523130849\n",
            " - layer5.weight grad norm: 0.012037196196615696\n",
            " - layer5.bias grad norm: 0.004946984816342592\n",
            "Gradients at iteration 706:\n",
            " - layer1.weight grad norm: 0.520506739616394\n",
            " - layer1.bias grad norm: 0.0006048199720680714\n",
            " - layer2.weight grad norm: 0.0017935049254447222\n",
            " - layer2.bias grad norm: 0.0005622105090878904\n",
            " - layer2_input.weight grad norm: 0.4899933934211731\n",
            " - layer2_input.bias grad norm: 0.0005622105090878904\n",
            " - layer3.weight grad norm: 0.0024715051986277103\n",
            " - layer3.bias grad norm: 0.0005620422307401896\n",
            " - layer3_input.weight grad norm: 0.4873332977294922\n",
            " - layer3_input.bias grad norm: 0.0005620422307401896\n",
            " - layer4.weight grad norm: 0.0029490224551409483\n",
            " - layer4.bias grad norm: 0.000582439883146435\n",
            " - layer4_input.weight grad norm: 0.5012984871864319\n",
            " - layer4_input.bias grad norm: 0.000582439883146435\n",
            " - layer5.weight grad norm: 0.011790764518082142\n",
            " - layer5.bias grad norm: 0.005098544526845217\n",
            "Gradients at iteration 707:\n",
            " - layer1.weight grad norm: 0.5495198369026184\n",
            " - layer1.bias grad norm: 0.0006432324298657477\n",
            " - layer2.weight grad norm: 0.001817590557038784\n",
            " - layer2.bias grad norm: 0.0005802069790661335\n",
            " - layer2_input.weight grad norm: 0.5023725032806396\n",
            " - layer2_input.bias grad norm: 0.0005802069790661335\n",
            " - layer3.weight grad norm: 0.002517347689718008\n",
            " - layer3.bias grad norm: 0.0006119527970440686\n",
            " - layer3_input.weight grad norm: 0.5236133337020874\n",
            " - layer3_input.bias grad norm: 0.0006119527970440686\n",
            " - layer4.weight grad norm: 0.0029277398716658354\n",
            " - layer4.bias grad norm: 0.00046558096073567867\n",
            " - layer4_input.weight grad norm: 0.41387510299682617\n",
            " - layer4_input.bias grad norm: 0.00046558096073567867\n",
            " - layer5.weight grad norm: 0.011772720143198967\n",
            " - layer5.bias grad norm: 0.0052109332755208015\n",
            "Gradients at iteration 708:\n",
            " - layer1.weight grad norm: 0.4929110109806061\n",
            " - layer1.bias grad norm: 0.0005671890103258193\n",
            " - layer2.weight grad norm: 0.0019179561641067266\n",
            " - layer2.bias grad norm: 0.0006390921771526337\n",
            " - layer2_input.weight grad norm: 0.5444827675819397\n",
            " - layer2_input.bias grad norm: 0.0006390921771526337\n",
            " - layer3.weight grad norm: 0.002606209833174944\n",
            " - layer3.bias grad norm: 0.0005461297114379704\n",
            " - layer3_input.weight grad norm: 0.476052850484848\n",
            " - layer3_input.bias grad norm: 0.0005461297114379704\n",
            " - layer4.weight grad norm: 0.003135660896077752\n",
            " - layer4.bias grad norm: 0.000555877631995827\n",
            " - layer4_input.weight grad norm: 0.4834403097629547\n",
            " - layer4_input.bias grad norm: 0.000555877631995827\n",
            " - layer5.weight grad norm: 0.013559883460402489\n",
            " - layer5.bias grad norm: 0.0054624988697469234\n",
            "Gradients at iteration 709:\n",
            " - layer1.weight grad norm: 0.5097514986991882\n",
            " - layer1.bias grad norm: 0.0005922687123529613\n",
            " - layer2.weight grad norm: 0.001806622720323503\n",
            " - layer2.bias grad norm: 0.0005955345695838332\n",
            " - layer2_input.weight grad norm: 0.5104758739471436\n",
            " - layer2_input.bias grad norm: 0.0005955345695838332\n",
            " - layer3.weight grad norm: 0.0024859043769538403\n",
            " - layer3.bias grad norm: 0.0005344628589227796\n",
            " - layer3_input.weight grad norm: 0.4649340510368347\n",
            " - layer3_input.bias grad norm: 0.0005344628589227796\n",
            " - layer4.weight grad norm: 0.0029067094437777996\n",
            " - layer4.bias grad norm: 0.0006028595380485058\n",
            " - layer4_input.weight grad norm: 0.5130549669265747\n",
            " - layer4_input.bias grad norm: 0.0006028595380485058\n",
            " - layer5.weight grad norm: 0.0114896260201931\n",
            " - layer5.bias grad norm: 0.005148745141923428\n",
            "Gradients at iteration 710:\n",
            " - layer1.weight grad norm: 0.5387434959411621\n",
            " - layer1.bias grad norm: 0.0006336175720207393\n",
            " - layer2.weight grad norm: 0.0018316005589440465\n",
            " - layer2.bias grad norm: 0.0005755507736466825\n",
            " - layer2_input.weight grad norm: 0.4987863004207611\n",
            " - layer2_input.bias grad norm: 0.0005755507736466825\n",
            " - layer3.weight grad norm: 0.002515180502086878\n",
            " - layer3.bias grad norm: 0.0005511248600669205\n",
            " - layer3_input.weight grad norm: 0.47595837712287903\n",
            " - layer3_input.bias grad norm: 0.0005511248600669205\n",
            " - layer4.weight grad norm: 0.002974105067551136\n",
            " - layer4.bias grad norm: 0.0005641033640131354\n",
            " - layer4_input.weight grad norm: 0.48399055004119873\n",
            " - layer4_input.bias grad norm: 0.0005641033640131354\n",
            " - layer5.weight grad norm: 0.011682109907269478\n",
            " - layer5.bias grad norm: 0.005205109715461731\n",
            "Gradients at iteration 711:\n",
            " - layer1.weight grad norm: 0.5223278403282166\n",
            " - layer1.bias grad norm: 0.0005973173538222909\n",
            " - layer2.weight grad norm: 0.001941072172485292\n",
            " - layer2.bias grad norm: 0.0006309908931143582\n",
            " - layer2_input.weight grad norm: 0.5391504764556885\n",
            " - layer2_input.bias grad norm: 0.0006309908931143582\n",
            " - layer3.weight grad norm: 0.0026331846602261066\n",
            " - layer3.bias grad norm: 0.0005697948508895934\n",
            " - layer3_input.weight grad norm: 0.492494136095047\n",
            " - layer3_input.bias grad norm: 0.0005697948508895934\n",
            " - layer4.weight grad norm: 0.003112095408141613\n",
            " - layer4.bias grad norm: 0.0005006866413168609\n",
            " - layer4_input.weight grad norm: 0.4401722550392151\n",
            " - layer4_input.bias grad norm: 0.0005006866413168609\n",
            " - layer5.weight grad norm: 0.011678460985422134\n",
            " - layer5.bias grad norm: 0.005398920737206936\n",
            "Gradients at iteration 712:\n",
            " - layer1.weight grad norm: 0.49276649951934814\n",
            " - layer1.bias grad norm: 0.0005551327485591173\n",
            " - layer2.weight grad norm: 0.001876571448519826\n",
            " - layer2.bias grad norm: 0.0006051904638297856\n",
            " - layer2_input.weight grad norm: 0.525736927986145\n",
            " - layer2_input.bias grad norm: 0.0006051904638297856\n",
            " - layer3.weight grad norm: 0.002555011073127389\n",
            " - layer3.bias grad norm: 0.0005149635835550725\n",
            " - layer3_input.weight grad norm: 0.4586213529109955\n",
            " - layer3_input.bias grad norm: 0.0005149635835550725\n",
            " - layer4.weight grad norm: 0.0029995038639754057\n",
            " - layer4.bias grad norm: 0.0006034252583049238\n",
            " - layer4_input.weight grad norm: 0.5198798775672913\n",
            " - layer4_input.bias grad norm: 0.0006034252583049238\n",
            " - layer5.weight grad norm: 0.011153417639434338\n",
            " - layer5.bias grad norm: 0.005246502347290516\n",
            "Gradients at iteration 713:\n",
            " - layer1.weight grad norm: 0.537786602973938\n",
            " - layer1.bias grad norm: 0.0006257641944102943\n",
            " - layer2.weight grad norm: 0.0018244782695546746\n",
            " - layer2.bias grad norm: 0.0005848376313224435\n",
            " - layer2_input.weight grad norm: 0.5048145651817322\n",
            " - layer2_input.bias grad norm: 0.0005848376313224435\n",
            " - layer3.weight grad norm: 0.0025053340941667557\n",
            " - layer3.bias grad norm: 0.0005816381308250129\n",
            " - layer3_input.weight grad norm: 0.502327561378479\n",
            " - layer3_input.bias grad norm: 0.0005816381308250129\n",
            " - layer4.weight grad norm: 0.002989172702655196\n",
            " - layer4.bias grad norm: 0.0005145370960235596\n",
            " - layer4_input.weight grad norm: 0.45103010535240173\n",
            " - layer4_input.bias grad norm: 0.0005145370960235596\n",
            " - layer5.weight grad norm: 0.011788283474743366\n",
            " - layer5.bias grad norm: 0.005183147266507149\n",
            "Gradients at iteration 714:\n",
            " - layer1.weight grad norm: 0.5304564833641052\n",
            " - layer1.bias grad norm: 0.0006082114414311945\n",
            " - layer2.weight grad norm: 0.0018027088372036815\n",
            " - layer2.bias grad norm: 0.000572676828596741\n",
            " - layer2_input.weight grad norm: 0.49960798025131226\n",
            " - layer2_input.bias grad norm: 0.000572676828596741\n",
            " - layer3.weight grad norm: 0.0024470926728099585\n",
            " - layer3.bias grad norm: 0.0005781935760751367\n",
            " - layer3_input.weight grad norm: 0.4998679459095001\n",
            " - layer3_input.bias grad norm: 0.0005781935760751367\n",
            " - layer4.weight grad norm: 0.002938207471743226\n",
            " - layer4.bias grad norm: 0.0005373012972995639\n",
            " - layer4_input.weight grad norm: 0.46790146827697754\n",
            " - layer4_input.bias grad norm: 0.0005373012972995639\n",
            " - layer5.weight grad norm: 0.012723292224109173\n",
            " - layer5.bias grad norm: 0.00509527325630188\n",
            "Gradients at iteration 715:\n",
            " - layer1.weight grad norm: 0.5132879614830017\n",
            " - layer1.bias grad norm: 0.0005944144795648754\n",
            " - layer2.weight grad norm: 0.0018298368668183684\n",
            " - layer2.bias grad norm: 0.0005544613231904805\n",
            " - layer2_input.weight grad norm: 0.4812702536582947\n",
            " - layer2_input.bias grad norm: 0.0005544613231904805\n",
            " - layer3.weight grad norm: 0.0025465907528996468\n",
            " - layer3.bias grad norm: 0.0005772382137365639\n",
            " - layer3_input.weight grad norm: 0.49195247888565063\n",
            " - layer3_input.bias grad norm: 0.0005772382137365639\n",
            " - layer4.weight grad norm: 0.003050648607313633\n",
            " - layer4.bias grad norm: 0.0006061122403480113\n",
            " - layer4_input.weight grad norm: 0.5125522613525391\n",
            " - layer4_input.bias grad norm: 0.0006061122403480113\n",
            " - layer5.weight grad norm: 0.011768355034291744\n",
            " - layer5.bias grad norm: 0.0052179270423948765\n",
            "Gradients at iteration 716:\n",
            " - layer1.weight grad norm: 0.5334026217460632\n",
            " - layer1.bias grad norm: 0.0006167328683659434\n",
            " - layer2.weight grad norm: 0.0017137234099209309\n",
            " - layer2.bias grad norm: 0.000602887652348727\n",
            " - layer2_input.weight grad norm: 0.5220975279808044\n",
            " - layer2_input.bias grad norm: 0.000602887652348727\n",
            " - layer3.weight grad norm: 0.0024114162661135197\n",
            " - layer3.bias grad norm: 0.0005833014729432762\n",
            " - layer3_input.weight grad norm: 0.5047591924667358\n",
            " - layer3_input.bias grad norm: 0.0005833014729432762\n",
            " - layer4.weight grad norm: 0.0027657959144562483\n",
            " - layer4.bias grad norm: 0.0004935244796797633\n",
            " - layer4_input.weight grad norm: 0.4335308074951172\n",
            " - layer4_input.bias grad norm: 0.0004935244796797633\n",
            " - layer5.weight grad norm: 0.011055688373744488\n",
            " - layer5.bias grad norm: 0.004913864191621542\n",
            "Gradients at iteration 717:\n",
            " - layer1.weight grad norm: 0.5330018401145935\n",
            " - layer1.bias grad norm: 0.0006215591565705836\n",
            " - layer2.weight grad norm: 0.001793266856111586\n",
            " - layer2.bias grad norm: 0.0005196723504923284\n",
            " - layer2_input.weight grad norm: 0.454672634601593\n",
            " - layer2_input.bias grad norm: 0.0005196723504923284\n",
            " - layer3.weight grad norm: 0.002531282138079405\n",
            " - layer3.bias grad norm: 0.0005953496438451111\n",
            " - layer3_input.weight grad norm: 0.5082668662071228\n",
            " - layer3_input.bias grad norm: 0.0005953496438451111\n",
            " - layer4.weight grad norm: 0.002939801197499037\n",
            " - layer4.bias grad norm: 0.0005872678593732417\n",
            " - layer4_input.weight grad norm: 0.5006577968597412\n",
            " - layer4_input.bias grad norm: 0.0005872678593732417\n",
            " - layer5.weight grad norm: 0.01189932320266962\n",
            " - layer5.bias grad norm: 0.005111394915729761\n",
            "Gradients at iteration 718:\n",
            " - layer1.weight grad norm: 0.5900538563728333\n",
            " - layer1.bias grad norm: 0.0007019208278506994\n",
            " - layer2.weight grad norm: 0.0017953738570213318\n",
            " - layer2.bias grad norm: 0.0005620820447802544\n",
            " - layer2_input.weight grad norm: 0.48517367243766785\n",
            " - layer2_input.bias grad norm: 0.0005620820447802544\n",
            " - layer3.weight grad norm: 0.0024372683838009834\n",
            " - layer3.bias grad norm: 0.0005386657430790365\n",
            " - layer3_input.weight grad norm: 0.46725985407829285\n",
            " - layer3_input.bias grad norm: 0.0005386657430790365\n",
            " - layer4.weight grad norm: 0.0028560757637023926\n",
            " - layer4.bias grad norm: 0.0005156220286153257\n",
            " - layer4_input.weight grad norm: 0.44488194584846497\n",
            " - layer4_input.bias grad norm: 0.0005156220286153257\n",
            " - layer5.weight grad norm: 0.012106206268072128\n",
            " - layer5.bias grad norm: 0.005028349347412586\n",
            "Gradients at iteration 719:\n",
            " - layer1.weight grad norm: 0.48921266198158264\n",
            " - layer1.bias grad norm: 0.0005615915870293975\n",
            " - layer2.weight grad norm: 0.001781961997039616\n",
            " - layer2.bias grad norm: 0.0005275746807456017\n",
            " - layer2_input.weight grad norm: 0.4618849754333496\n",
            " - layer2_input.bias grad norm: 0.0005275746807456017\n",
            " - layer3.weight grad norm: 0.0024928508792072535\n",
            " - layer3.bias grad norm: 0.000631426926702261\n",
            " - layer3_input.weight grad norm: 0.5357158184051514\n",
            " - layer3_input.bias grad norm: 0.000631426926702261\n",
            " - layer4.weight grad norm: 0.0028996197506785393\n",
            " - layer4.bias grad norm: 0.0005998415872454643\n",
            " - layer4_input.weight grad norm: 0.5100650787353516\n",
            " - layer4_input.bias grad norm: 0.0005998415872454643\n",
            " - layer5.weight grad norm: 0.011355939321219921\n",
            " - layer5.bias grad norm: 0.005120201501995325\n",
            "Gradients at iteration 720:\n",
            " - layer1.weight grad norm: 0.5336223244667053\n",
            " - layer1.bias grad norm: 0.0006281821988523006\n",
            " - layer2.weight grad norm: 0.0018280670046806335\n",
            " - layer2.bias grad norm: 0.0005943183205090463\n",
            " - layer2_input.weight grad norm: 0.5075719356536865\n",
            " - layer2_input.bias grad norm: 0.0005943183205090463\n",
            " - layer3.weight grad norm: 0.0024876093957573175\n",
            " - layer3.bias grad norm: 0.000578536419197917\n",
            " - layer3_input.weight grad norm: 0.492946058511734\n",
            " - layer3_input.bias grad norm: 0.000578536419197917\n",
            " - layer4.weight grad norm: 0.002954127499833703\n",
            " - layer4.bias grad norm: 0.0005391519516706467\n",
            " - layer4_input.weight grad norm: 0.4630681276321411\n",
            " - layer4_input.bias grad norm: 0.0005391519516706467\n",
            " - layer5.weight grad norm: 0.011942158453166485\n",
            " - layer5.bias grad norm: 0.005171528086066246\n",
            "Gradients at iteration 721:\n",
            " - layer1.weight grad norm: 0.48474887013435364\n",
            " - layer1.bias grad norm: 0.000557563325855881\n",
            " - layer2.weight grad norm: 0.0017933109775185585\n",
            " - layer2.bias grad norm: 0.0005837605567649007\n",
            " - layer2_input.weight grad norm: 0.5045568943023682\n",
            " - layer2_input.bias grad norm: 0.0005837605567649007\n",
            " - layer3.weight grad norm: 0.0024366741999983788\n",
            " - layer3.bias grad norm: 0.0005892727058380842\n",
            " - layer3_input.weight grad norm: 0.5073822736740112\n",
            " - layer3_input.bias grad norm: 0.0005892727058380842\n",
            " - layer4.weight grad norm: 0.0028638881631195545\n",
            " - layer4.bias grad norm: 0.0005847571301274002\n",
            " - layer4_input.weight grad norm: 0.5028377771377563\n",
            " - layer4_input.bias grad norm: 0.0005847571301274002\n",
            " - layer5.weight grad norm: 0.010646381415426731\n",
            " - layer5.bias grad norm: 0.005027152597904205\n",
            "Gradients at iteration 722:\n",
            " - layer1.weight grad norm: 0.5641573071479797\n",
            " - layer1.bias grad norm: 0.0006632367731072009\n",
            " - layer2.weight grad norm: 0.0018184054642915726\n",
            " - layer2.bias grad norm: 0.0005724487127736211\n",
            " - layer2_input.weight grad norm: 0.4935969412326813\n",
            " - layer2_input.bias grad norm: 0.0005724487127736211\n",
            " - layer3.weight grad norm: 0.002467705635353923\n",
            " - layer3.bias grad norm: 0.0005542036378756166\n",
            " - layer3_input.weight grad norm: 0.4756193161010742\n",
            " - layer3_input.bias grad norm: 0.0005542036378756166\n",
            " - layer4.weight grad norm: 0.0029238136485219\n",
            " - layer4.bias grad norm: 0.0005308968829922378\n",
            " - layer4_input.weight grad norm: 0.4601006805896759\n",
            " - layer4_input.bias grad norm: 0.0005308968829922378\n",
            " - layer5.weight grad norm: 0.011656348593533039\n",
            " - layer5.bias grad norm: 0.005129589699208736\n",
            "Gradients at iteration 723:\n",
            " - layer1.weight grad norm: 0.45020368695259094\n",
            " - layer1.bias grad norm: 0.0005137168918736279\n",
            " - layer2.weight grad norm: 0.0017168873455375433\n",
            " - layer2.bias grad norm: 0.0006089738453738391\n",
            " - layer2_input.weight grad norm: 0.5199726819992065\n",
            " - layer2_input.bias grad norm: 0.0006089738453738391\n",
            " - layer3.weight grad norm: 0.0023705465719103813\n",
            " - layer3.bias grad norm: 0.0006350787007249892\n",
            " - layer3_input.weight grad norm: 0.5355076789855957\n",
            " - layer3_input.bias grad norm: 0.0006350787007249892\n",
            " - layer4.weight grad norm: 0.002833104692399502\n",
            " - layer4.bias grad norm: 0.000574879115447402\n",
            " - layer4_input.weight grad norm: 0.48990148305892944\n",
            " - layer4_input.bias grad norm: 0.000574879115447402\n",
            " - layer5.weight grad norm: 0.011393893510103226\n",
            " - layer5.bias grad norm: 0.004928675014525652\n",
            "Gradients at iteration 724:\n",
            " - layer1.weight grad norm: 0.49864041805267334\n",
            " - layer1.bias grad norm: 0.0005620542215183377\n",
            " - layer2.weight grad norm: 0.0018473236123099923\n",
            " - layer2.bias grad norm: 0.0005739252083003521\n",
            " - layer2_input.weight grad norm: 0.4977158308029175\n",
            " - layer2_input.bias grad norm: 0.0005739252083003521\n",
            " - layer3.weight grad norm: 0.0025994814932346344\n",
            " - layer3.bias grad norm: 0.000574553559999913\n",
            " - layer3_input.weight grad norm: 0.4958045780658722\n",
            " - layer3_input.bias grad norm: 0.000574553559999913\n",
            " - layer4.weight grad norm: 0.003042657859623432\n",
            " - layer4.bias grad norm: 0.0005930129555054009\n",
            " - layer4_input.weight grad norm: 0.5075656175613403\n",
            " - layer4_input.bias grad norm: 0.0005930129555054009\n",
            " - layer5.weight grad norm: 0.011906730942428112\n",
            " - layer5.bias grad norm: 0.005310490261763334\n",
            "Gradients at iteration 725:\n",
            " - layer1.weight grad norm: 0.48976758122444153\n",
            " - layer1.bias grad norm: 0.0005689657409675419\n",
            " - layer2.weight grad norm: 0.0017297693993896246\n",
            " - layer2.bias grad norm: 0.0005966033786535263\n",
            " - layer2_input.weight grad norm: 0.5053464770317078\n",
            " - layer2_input.bias grad norm: 0.0005966033786535263\n",
            " - layer3.weight grad norm: 0.00238855741918087\n",
            " - layer3.bias grad norm: 0.000594825018197298\n",
            " - layer3_input.weight grad norm: 0.5044081211090088\n",
            " - layer3_input.bias grad norm: 0.000594825018197298\n",
            " - layer4.weight grad norm: 0.0028422479517757893\n",
            " - layer4.bias grad norm: 0.000589197501540184\n",
            " - layer4_input.weight grad norm: 0.5001536011695862\n",
            " - layer4_input.bias grad norm: 0.000589197501540184\n",
            " - layer5.weight grad norm: 0.011305299587547779\n",
            " - layer5.bias grad norm: 0.004952868912369013\n",
            "Gradients at iteration 726:\n",
            " - layer1.weight grad norm: 0.5073619484901428\n",
            " - layer1.bias grad norm: 0.0005989191122353077\n",
            " - layer2.weight grad norm: 0.0018240798963233829\n",
            " - layer2.bias grad norm: 0.0005788957350887358\n",
            " - layer2_input.weight grad norm: 0.4951249957084656\n",
            " - layer2_input.bias grad norm: 0.0005788957350887358\n",
            " - layer3.weight grad norm: 0.0024674111045897007\n",
            " - layer3.bias grad norm: 0.0006068709190003574\n",
            " - layer3_input.weight grad norm: 0.5171098113059998\n",
            " - layer3_input.bias grad norm: 0.0006068709190003574\n",
            " - layer4.weight grad norm: 0.0029035003390163183\n",
            " - layer4.bias grad norm: 0.0005603507743217051\n",
            " - layer4_input.weight grad norm: 0.47945085167884827\n",
            " - layer4_input.bias grad norm: 0.0005603507743217051\n",
            " - layer5.weight grad norm: 0.010612044483423233\n",
            " - layer5.bias grad norm: 0.005159458145499229\n",
            "Gradients at iteration 727:\n",
            " - layer1.weight grad norm: 0.5650768876075745\n",
            " - layer1.bias grad norm: 0.000648624321911484\n",
            " - layer2.weight grad norm: 0.0018169996328651905\n",
            " - layer2.bias grad norm: 0.0005454050260595977\n",
            " - layer2_input.weight grad norm: 0.4813358187675476\n",
            " - layer2_input.bias grad norm: 0.0005454050260595977\n",
            " - layer3.weight grad norm: 0.002483476884663105\n",
            " - layer3.bias grad norm: 0.000556489045266062\n",
            " - layer3_input.weight grad norm: 0.4861864447593689\n",
            " - layer3_input.bias grad norm: 0.000556489045266062\n",
            " - layer4.weight grad norm: 0.002953117247670889\n",
            " - layer4.bias grad norm: 0.0005260015605017543\n",
            " - layer4_input.weight grad norm: 0.4609142243862152\n",
            " - layer4_input.bias grad norm: 0.0005260015605017543\n",
            " - layer5.weight grad norm: 0.011726778000593185\n",
            " - layer5.bias grad norm: 0.005164552479982376\n",
            "Gradients at iteration 728:\n",
            " - layer1.weight grad norm: 0.5403214693069458\n",
            " - layer1.bias grad norm: 0.0006291166646406054\n",
            " - layer2.weight grad norm: 0.0019016759470105171\n",
            " - layer2.bias grad norm: 0.0006105482461862266\n",
            " - layer2_input.weight grad norm: 0.5251103043556213\n",
            " - layer2_input.bias grad norm: 0.0006105482461862266\n",
            " - layer3.weight grad norm: 0.002594506833702326\n",
            " - layer3.bias grad norm: 0.0005412555765360594\n",
            " - layer3_input.weight grad norm: 0.4713209867477417\n",
            " - layer3_input.bias grad norm: 0.0005412555765360594\n",
            " - layer4.weight grad norm: 0.0030778765212744474\n",
            " - layer4.bias grad norm: 0.0005232940311543643\n",
            " - layer4_input.weight grad norm: 0.45824095606803894\n",
            " - layer4_input.bias grad norm: 0.0005232940311543643\n",
            " - layer5.weight grad norm: 0.011504611931741238\n",
            " - layer5.bias grad norm: 0.005405574571341276\n",
            "Gradients at iteration 729:\n",
            " - layer1.weight grad norm: 0.5596683621406555\n",
            " - layer1.bias grad norm: 0.0006526854122057557\n",
            " - layer2.weight grad norm: 0.0018767702858895063\n",
            " - layer2.bias grad norm: 0.0005994686507619917\n",
            " - layer2_input.weight grad norm: 0.5208339691162109\n",
            " - layer2_input.bias grad norm: 0.0005994686507619917\n",
            " - layer3.weight grad norm: 0.0025450866669416428\n",
            " - layer3.bias grad norm: 0.0005399608053267002\n",
            " - layer3_input.weight grad norm: 0.46899744868278503\n",
            " - layer3_input.bias grad norm: 0.0005399608053267002\n",
            " - layer4.weight grad norm: 0.0030250754207372665\n",
            " - layer4.bias grad norm: 0.0004957300843670964\n",
            " - layer4_input.weight grad norm: 0.4419863522052765\n",
            " - layer4_input.bias grad norm: 0.0004957300843670964\n",
            " - layer5.weight grad norm: 0.011944142170250416\n",
            " - layer5.bias grad norm: 0.005349398590624332\n",
            "Gradients at iteration 730:\n",
            " - layer1.weight grad norm: 0.5124710202217102\n",
            " - layer1.bias grad norm: 0.0005907178856432438\n",
            " - layer2.weight grad norm: 0.0018730483716353774\n",
            " - layer2.bias grad norm: 0.0006054468685761094\n",
            " - layer2_input.weight grad norm: 0.5213156938552856\n",
            " - layer2_input.bias grad norm: 0.0006054468685761094\n",
            " - layer3.weight grad norm: 0.0025387639179825783\n",
            " - layer3.bias grad norm: 0.0005826441338285804\n",
            " - layer3_input.weight grad norm: 0.5047223567962646\n",
            " - layer3_input.bias grad norm: 0.0005826441338285804\n",
            " - layer4.weight grad norm: 0.0030212944839149714\n",
            " - layer4.bias grad norm: 0.0005242176703177392\n",
            " - layer4_input.weight grad norm: 0.45899778604507446\n",
            " - layer4_input.bias grad norm: 0.0005242176703177392\n",
            " - layer5.weight grad norm: 0.011414378881454468\n",
            " - layer5.bias grad norm: 0.0053028236143291\n",
            "Gradients at iteration 731:\n",
            " - layer1.weight grad norm: 0.5690327286720276\n",
            " - layer1.bias grad norm: 0.0006665894761681557\n",
            " - layer2.weight grad norm: 0.0017595108365640044\n",
            " - layer2.bias grad norm: 0.000569473544601351\n",
            " - layer2_input.weight grad norm: 0.4952929615974426\n",
            " - layer2_input.bias grad norm: 0.000569473544601351\n",
            " - layer3.weight grad norm: 0.002480369294062257\n",
            " - layer3.bias grad norm: 0.0005200569285079837\n",
            " - layer3_input.weight grad norm: 0.4544382095336914\n",
            " - layer3_input.bias grad norm: 0.0005200569285079837\n",
            " - layer4.weight grad norm: 0.0029135635122656822\n",
            " - layer4.bias grad norm: 0.0005444530979730189\n",
            " - layer4_input.weight grad norm: 0.47347649931907654\n",
            " - layer4_input.bias grad norm: 0.0005444530979730189\n",
            " - layer5.weight grad norm: 0.012136021628975868\n",
            " - layer5.bias grad norm: 0.005054207518696785\n",
            "Gradients at iteration 732:\n",
            " - layer1.weight grad norm: 0.5473472476005554\n",
            " - layer1.bias grad norm: 0.0006372865755110979\n",
            " - layer2.weight grad norm: 0.0017918639350682497\n",
            " - layer2.bias grad norm: 0.0005615392001345754\n",
            " - layer2_input.weight grad norm: 0.4901583790779114\n",
            " - layer2_input.bias grad norm: 0.0005615392001345754\n",
            " - layer3.weight grad norm: 0.0024612394627183676\n",
            " - layer3.bias grad norm: 0.0005565665196627378\n",
            " - layer3_input.weight grad norm: 0.48287925124168396\n",
            " - layer3_input.bias grad norm: 0.0005565665196627378\n",
            " - layer4.weight grad norm: 0.002941783983260393\n",
            " - layer4.bias grad norm: 0.0005504856817424297\n",
            " - layer4_input.weight grad norm: 0.47624263167381287\n",
            " - layer4_input.bias grad norm: 0.0005504856817424297\n",
            " - layer5.weight grad norm: 0.011403854005038738\n",
            " - layer5.bias grad norm: 0.005112475715577602\n",
            "Gradients at iteration 733:\n",
            " - layer1.weight grad norm: 0.4749332368373871\n",
            " - layer1.bias grad norm: 0.0005350781138986349\n",
            " - layer2.weight grad norm: 0.0018181903287768364\n",
            " - layer2.bias grad norm: 0.0006627601687796414\n",
            " - layer2_input.weight grad norm: 0.561618447303772\n",
            " - layer2_input.bias grad norm: 0.0006627601687796414\n",
            " - layer3.weight grad norm: 0.0024598187301307917\n",
            " - layer3.bias grad norm: 0.0005893433117307723\n",
            " - layer3_input.weight grad norm: 0.5058940649032593\n",
            " - layer3_input.bias grad norm: 0.0005893433117307723\n",
            " - layer4.weight grad norm: 0.0029351285193115473\n",
            " - layer4.bias grad norm: 0.0005156465922482312\n",
            " - layer4_input.weight grad norm: 0.4504653513431549\n",
            " - layer4_input.bias grad norm: 0.0005156465922482312\n",
            " - layer5.weight grad norm: 0.011338753625750542\n",
            " - layer5.bias grad norm: 0.005133898463100195\n",
            "Gradients at iteration 734:\n",
            " - layer1.weight grad norm: 0.511615514755249\n",
            " - layer1.bias grad norm: 0.0005848835571669042\n",
            " - layer2.weight grad norm: 0.0018272678134962916\n",
            " - layer2.bias grad norm: 0.0005761734209954739\n",
            " - layer2_input.weight grad norm: 0.5007337331771851\n",
            " - layer2_input.bias grad norm: 0.0005761734209954739\n",
            " - layer3.weight grad norm: 0.0024830002803355455\n",
            " - layer3.bias grad norm: 0.0006220659124664962\n",
            " - layer3_input.weight grad norm: 0.5265865921974182\n",
            " - layer3_input.bias grad norm: 0.0006220659124664962\n",
            " - layer4.weight grad norm: 0.0029684274923056364\n",
            " - layer4.bias grad norm: 0.0005256054573692381\n",
            " - layer4_input.weight grad norm: 0.4582902193069458\n",
            " - layer4_input.bias grad norm: 0.0005256054573692381\n",
            " - layer5.weight grad norm: 0.012002374045550823\n",
            " - layer5.bias grad norm: 0.00522181112319231\n",
            "Gradients at iteration 735:\n",
            " - layer1.weight grad norm: 0.48814669251441956\n",
            " - layer1.bias grad norm: 0.000562572677154094\n",
            " - layer2.weight grad norm: 0.0017849517753347754\n",
            " - layer2.bias grad norm: 0.0005587810301221907\n",
            " - layer2_input.weight grad norm: 0.48213571310043335\n",
            " - layer2_input.bias grad norm: 0.0005587810301221907\n",
            " - layer3.weight grad norm: 0.0024599668104201555\n",
            " - layer3.bias grad norm: 0.0005858195363543928\n",
            " - layer3_input.weight grad norm: 0.5014190077781677\n",
            " - layer3_input.bias grad norm: 0.0005858195363543928\n",
            " - layer4.weight grad norm: 0.0028733715880662203\n",
            " - layer4.bias grad norm: 0.0006263143732212484\n",
            " - layer4_input.weight grad norm: 0.526949405670166\n",
            " - layer4_input.bias grad norm: 0.0006263143732212484\n",
            " - layer5.weight grad norm: 0.010763844475150108\n",
            " - layer5.bias grad norm: 0.005061677191406488\n",
            "Gradients at iteration 736:\n",
            " - layer1.weight grad norm: 0.5322681069374084\n",
            " - layer1.bias grad norm: 0.0006154647562652826\n",
            " - layer2.weight grad norm: 0.001922460040077567\n",
            " - layer2.bias grad norm: 0.000599954160861671\n",
            " - layer2_input.weight grad norm: 0.5187241435050964\n",
            " - layer2_input.bias grad norm: 0.000599954160861671\n",
            " - layer3.weight grad norm: 0.0025742389261722565\n",
            " - layer3.bias grad norm: 0.0005636833957396448\n",
            " - layer3_input.weight grad norm: 0.48749563097953796\n",
            " - layer3_input.bias grad norm: 0.0005636833957396448\n",
            " - layer4.weight grad norm: 0.003163969377055764\n",
            " - layer4.bias grad norm: 0.0005234759883023798\n",
            " - layer4_input.weight grad norm: 0.4579926133155823\n",
            " - layer4_input.bias grad norm: 0.0005234759883023798\n",
            " - layer5.weight grad norm: 0.012440946884453297\n",
            " - layer5.bias grad norm: 0.005419567693024874\n",
            "Gradients at iteration 737:\n",
            " - layer1.weight grad norm: 0.5280471444129944\n",
            " - layer1.bias grad norm: 0.0006109654204919934\n",
            " - layer2.weight grad norm: 0.0017745173536241055\n",
            " - layer2.bias grad norm: 0.0005428686272352934\n",
            " - layer2_input.weight grad norm: 0.471466064453125\n",
            " - layer2_input.bias grad norm: 0.0005428686272352934\n",
            " - layer3.weight grad norm: 0.002424052217975259\n",
            " - layer3.bias grad norm: 0.0006077151629142463\n",
            " - layer3_input.weight grad norm: 0.5179933905601501\n",
            " - layer3_input.bias grad norm: 0.0006077151629142463\n",
            " - layer4.weight grad norm: 0.00286300596781075\n",
            " - layer4.bias grad norm: 0.0005612735403701663\n",
            " - layer4_input.weight grad norm: 0.4800093173980713\n",
            " - layer4_input.bias grad norm: 0.0005612735403701663\n",
            " - layer5.weight grad norm: 0.010734560899436474\n",
            " - layer5.bias grad norm: 0.005004792008548975\n",
            "Gradients at iteration 738:\n",
            " - layer1.weight grad norm: 0.5801389813423157\n",
            " - layer1.bias grad norm: 0.0006823246367275715\n",
            " - layer2.weight grad norm: 0.0018268073908984661\n",
            " - layer2.bias grad norm: 0.000606937101110816\n",
            " - layer2_input.weight grad norm: 0.5182836055755615\n",
            " - layer2_input.bias grad norm: 0.000606937101110816\n",
            " - layer3.weight grad norm: 0.002526952652260661\n",
            " - layer3.bias grad norm: 0.0005598307470791042\n",
            " - layer3_input.weight grad norm: 0.47719091176986694\n",
            " - layer3_input.bias grad norm: 0.0005598307470791042\n",
            " - layer4.weight grad norm: 0.0030003711581230164\n",
            " - layer4.bias grad norm: 0.0004614158533513546\n",
            " - layer4_input.weight grad norm: 0.40855512022972107\n",
            " - layer4_input.bias grad norm: 0.0004614158533513546\n",
            " - layer5.weight grad norm: 0.012025503441691399\n",
            " - layer5.bias grad norm: 0.005169068928807974\n",
            "Gradients at iteration 739:\n",
            " - layer1.weight grad norm: 0.5069922804832458\n",
            " - layer1.bias grad norm: 0.0005832241731695831\n",
            " - layer2.weight grad norm: 0.0019399675074964762\n",
            " - layer2.bias grad norm: 0.0006187467370182276\n",
            " - layer2_input.weight grad norm: 0.5262563228607178\n",
            " - layer2_input.bias grad norm: 0.0006187467370182276\n",
            " - layer3.weight grad norm: 0.002657487289980054\n",
            " - layer3.bias grad norm: 0.0005953681538812816\n",
            " - layer3_input.weight grad norm: 0.5021657943725586\n",
            " - layer3_input.bias grad norm: 0.0005953681538812816\n",
            " - layer4.weight grad norm: 0.0031512596178799868\n",
            " - layer4.bias grad norm: 0.0005349395796656609\n",
            " - layer4_input.weight grad norm: 0.46221378445625305\n",
            " - layer4_input.bias grad norm: 0.0005349395796656609\n",
            " - layer5.weight grad norm: 0.012181454338133335\n",
            " - layer5.bias grad norm: 0.005441341083496809\n",
            "Gradients at iteration 740:\n",
            " - layer1.weight grad norm: 0.5111505389213562\n",
            " - layer1.bias grad norm: 0.0005886628641746938\n",
            " - layer2.weight grad norm: 0.0018766819266602397\n",
            " - layer2.bias grad norm: 0.0005935665685683489\n",
            " - layer2_input.weight grad norm: 0.5167449116706848\n",
            " - layer2_input.bias grad norm: 0.0005935665685683489\n",
            " - layer3.weight grad norm: 0.002574757905676961\n",
            " - layer3.bias grad norm: 0.0005627883947454393\n",
            " - layer3_input.weight grad norm: 0.486711323261261\n",
            " - layer3_input.bias grad norm: 0.0005627883947454393\n",
            " - layer4.weight grad norm: 0.0030004414729774\n",
            " - layer4.bias grad norm: 0.0005607414641417563\n",
            " - layer4_input.weight grad norm: 0.48435965180397034\n",
            " - layer4_input.bias grad norm: 0.0005607414641417563\n",
            " - layer5.weight grad norm: 0.012569162994623184\n",
            " - layer5.bias grad norm: 0.005311363376677036\n",
            "Gradients at iteration 741:\n",
            " - layer1.weight grad norm: 0.48071447014808655\n",
            " - layer1.bias grad norm: 0.0005467191804200411\n",
            " - layer2.weight grad norm: 0.0018525186460465193\n",
            " - layer2.bias grad norm: 0.0006410662899725139\n",
            " - layer2_input.weight grad norm: 0.5496901869773865\n",
            " - layer2_input.bias grad norm: 0.0006410662899725139\n",
            " - layer3.weight grad norm: 0.00255976221524179\n",
            " - layer3.bias grad norm: 0.0005199495353735983\n",
            " - layer3_input.weight grad norm: 0.4598422348499298\n",
            " - layer3_input.bias grad norm: 0.0005199495353735983\n",
            " - layer4.weight grad norm: 0.0030557874124497175\n",
            " - layer4.bias grad norm: 0.0005886864964850247\n",
            " - layer4_input.weight grad norm: 0.505070686340332\n",
            " - layer4_input.bias grad norm: 0.0005886864964850247\n",
            " - layer5.weight grad norm: 0.012378431856632233\n",
            " - layer5.bias grad norm: 0.0052855247631669044\n",
            "Gradients at iteration 742:\n",
            " - layer1.weight grad norm: 0.5275920033454895\n",
            " - layer1.bias grad norm: 0.0006068199872970581\n",
            " - layer2.weight grad norm: 0.00184606050606817\n",
            " - layer2.bias grad norm: 0.0005631927051581442\n",
            " - layer2_input.weight grad norm: 0.4931049644947052\n",
            " - layer2_input.bias grad norm: 0.0005631927051581442\n",
            " - layer3.weight grad norm: 0.002550731645897031\n",
            " - layer3.bias grad norm: 0.000543526781257242\n",
            " - layer3_input.weight grad norm: 0.47520700097084045\n",
            " - layer3_input.bias grad norm: 0.000543526781257242\n",
            " - layer4.weight grad norm: 0.002980087185278535\n",
            " - layer4.bias grad norm: 0.0005818370846100152\n",
            " - layer4_input.weight grad norm: 0.5024673938751221\n",
            " - layer4_input.bias grad norm: 0.0005818370846100152\n",
            " - layer5.weight grad norm: 0.012270137667655945\n",
            " - layer5.bias grad norm: 0.005218476057052612\n",
            "Gradients at iteration 743:\n",
            " - layer1.weight grad norm: 0.5070891380310059\n",
            " - layer1.bias grad norm: 0.0005826930282637477\n",
            " - layer2.weight grad norm: 0.0018132722470909357\n",
            " - layer2.bias grad norm: 0.0006360002444125712\n",
            " - layer2_input.weight grad norm: 0.5399219989776611\n",
            " - layer2_input.bias grad norm: 0.0006360002444125712\n",
            " - layer3.weight grad norm: 0.00245859962888062\n",
            " - layer3.bias grad norm: 0.0005286810337565839\n",
            " - layer3_input.weight grad norm: 0.4584825932979584\n",
            " - layer3_input.bias grad norm: 0.0005286810337565839\n",
            " - layer4.weight grad norm: 0.0029398901388049126\n",
            " - layer4.bias grad norm: 0.0005707464297302067\n",
            " - layer4_input.weight grad norm: 0.49088171124458313\n",
            " - layer4_input.bias grad norm: 0.0005707464297302067\n",
            " - layer5.weight grad norm: 0.011260700412094593\n",
            " - layer5.bias grad norm: 0.005138487555086613\n",
            "Gradients at iteration 744:\n",
            " - layer1.weight grad norm: 0.5320820212364197\n",
            " - layer1.bias grad norm: 0.0006206710240803659\n",
            " - layer2.weight grad norm: 0.0017909869784489274\n",
            " - layer2.bias grad norm: 0.0005505169392563403\n",
            " - layer2_input.weight grad norm: 0.47817182540893555\n",
            " - layer2_input.bias grad norm: 0.0005505169392563403\n",
            " - layer3.weight grad norm: 0.002490732353180647\n",
            " - layer3.bias grad norm: 0.0006123374332673848\n",
            " - layer3_input.weight grad norm: 0.5170390009880066\n",
            " - layer3_input.bias grad norm: 0.0006123374332673848\n",
            " - layer4.weight grad norm: 0.00293664145283401\n",
            " - layer4.bias grad norm: 0.0005483205313794315\n",
            " - layer4_input.weight grad norm: 0.46983930468559265\n",
            " - layer4_input.bias grad norm: 0.0005483205313794315\n",
            " - layer5.weight grad norm: 0.010749436914920807\n",
            " - layer5.bias grad norm: 0.005125026684254408\n",
            "Gradients at iteration 745:\n",
            " - layer1.weight grad norm: 0.5079784989356995\n",
            " - layer1.bias grad norm: 0.0005784460227005184\n",
            " - layer2.weight grad norm: 0.0018166177906095982\n",
            " - layer2.bias grad norm: 0.0005416314816102386\n",
            " - layer2_input.weight grad norm: 0.4772796630859375\n",
            " - layer2_input.bias grad norm: 0.0005416314816102386\n",
            " - layer3.weight grad norm: 0.0025506233796477318\n",
            " - layer3.bias grad norm: 0.0006424757302738726\n",
            " - layer3_input.weight grad norm: 0.5463377833366394\n",
            " - layer3_input.bias grad norm: 0.0006424757302738726\n",
            " - layer4.weight grad norm: 0.0029668230563402176\n",
            " - layer4.bias grad norm: 0.0005327780963853002\n",
            " - layer4_input.weight grad norm: 0.46421077847480774\n",
            " - layer4_input.bias grad norm: 0.0005327780963853002\n",
            " - layer5.weight grad norm: 0.011700297705829144\n",
            " - layer5.bias grad norm: 0.00523983221501112\n",
            "Gradients at iteration 746:\n",
            " - layer1.weight grad norm: 0.5275397300720215\n",
            " - layer1.bias grad norm: 0.0006142434431239963\n",
            " - layer2.weight grad norm: 0.0018940215231850743\n",
            " - layer2.bias grad norm: 0.0005969915073364973\n",
            " - layer2_input.weight grad norm: 0.5098126530647278\n",
            " - layer2_input.bias grad norm: 0.0005969915073364973\n",
            " - layer3.weight grad norm: 0.0026004172395914793\n",
            " - layer3.bias grad norm: 0.0005422732792794704\n",
            " - layer3_input.weight grad norm: 0.4737395942211151\n",
            " - layer3_input.bias grad norm: 0.0005422732792794704\n",
            " - layer4.weight grad norm: 0.003055787645280361\n",
            " - layer4.bias grad norm: 0.0005707444506697357\n",
            " - layer4_input.weight grad norm: 0.48697856068611145\n",
            " - layer4_input.bias grad norm: 0.0005707444506697357\n",
            " - layer5.weight grad norm: 0.012866410426795483\n",
            " - layer5.bias grad norm: 0.005304533988237381\n",
            "Gradients at iteration 747:\n",
            " - layer1.weight grad norm: 0.5099873542785645\n",
            " - layer1.bias grad norm: 0.0005979468114674091\n",
            " - layer2.weight grad norm: 0.00187598611228168\n",
            " - layer2.bias grad norm: 0.0005152294179424644\n",
            " - layer2_input.weight grad norm: 0.4539002478122711\n",
            " - layer2_input.bias grad norm: 0.0005152294179424644\n",
            " - layer3.weight grad norm: 0.0026203307788819075\n",
            " - layer3.bias grad norm: 0.0006331611075438559\n",
            " - layer3_input.weight grad norm: 0.5365676283836365\n",
            " - layer3_input.bias grad norm: 0.0006331611075438559\n",
            " - layer4.weight grad norm: 0.003033270826563239\n",
            " - layer4.bias grad norm: 0.0005820940132252872\n",
            " - layer4_input.weight grad norm: 0.4957613945007324\n",
            " - layer4_input.bias grad norm: 0.0005820940132252872\n",
            " - layer5.weight grad norm: 0.012363553047180176\n",
            " - layer5.bias grad norm: 0.0053414832800626755\n",
            "Gradients at iteration 748:\n",
            " - layer1.weight grad norm: 0.5190967917442322\n",
            " - layer1.bias grad norm: 0.0006003573653288186\n",
            " - layer2.weight grad norm: 0.0018230497371405363\n",
            " - layer2.bias grad norm: 0.0005322282668203115\n",
            " - layer2_input.weight grad norm: 0.46858951449394226\n",
            " - layer2_input.bias grad norm: 0.0005322282668203115\n",
            " - layer3.weight grad norm: 0.002506167395040393\n",
            " - layer3.bias grad norm: 0.000617149518802762\n",
            " - layer3_input.weight grad norm: 0.5286449790000916\n",
            " - layer3_input.bias grad norm: 0.000617149518802762\n",
            " - layer4.weight grad norm: 0.0029418431222438812\n",
            " - layer4.bias grad norm: 0.0005597713752649724\n",
            " - layer4_input.weight grad norm: 0.48096218705177307\n",
            " - layer4_input.bias grad norm: 0.0005597713752649724\n",
            " - layer5.weight grad norm: 0.011201235465705395\n",
            " - layer5.bias grad norm: 0.005138475447893143\n",
            "Gradients at iteration 749:\n",
            " - layer1.weight grad norm: 0.5283545255661011\n",
            " - layer1.bias grad norm: 0.0006124513456597924\n",
            " - layer2.weight grad norm: 0.0018437199760228395\n",
            " - layer2.bias grad norm: 0.0005696087027899921\n",
            " - layer2_input.weight grad norm: 0.4971698522567749\n",
            " - layer2_input.bias grad norm: 0.0005696087027899921\n",
            " - layer3.weight grad norm: 0.0025369985960423946\n",
            " - layer3.bias grad norm: 0.00053798669250682\n",
            " - layer3_input.weight grad norm: 0.4703719913959503\n",
            " - layer3_input.bias grad norm: 0.00053798669250682\n",
            " - layer4.weight grad norm: 0.0030520763248205185\n",
            " - layer4.bias grad norm: 0.0005900795222260058\n",
            " - layer4_input.weight grad norm: 0.5022087693214417\n",
            " - layer4_input.bias grad norm: 0.0005900795222260058\n",
            " - layer5.weight grad norm: 0.012290137819945812\n",
            " - layer5.bias grad norm: 0.005257835146039724\n",
            "Gradients at iteration 750:\n",
            " - layer1.weight grad norm: 0.523316502571106\n",
            " - layer1.bias grad norm: 0.0006011119112372398\n",
            " - layer2.weight grad norm: 0.0019152535824105144\n",
            " - layer2.bias grad norm: 0.0005584238097071648\n",
            " - layer2_input.weight grad norm: 0.49139583110809326\n",
            " - layer2_input.bias grad norm: 0.0005584238097071648\n",
            " - layer3.weight grad norm: 0.0026025401894003153\n",
            " - layer3.bias grad norm: 0.0005527667235583067\n",
            " - layer3_input.weight grad norm: 0.4836254119873047\n",
            " - layer3_input.bias grad norm: 0.0005527667235583067\n",
            " - layer4.weight grad norm: 0.003107986180111766\n",
            " - layer4.bias grad norm: 0.0005807973211631179\n",
            " - layer4_input.weight grad norm: 0.5005521774291992\n",
            " - layer4_input.bias grad norm: 0.0005807973211631179\n",
            " - layer5.weight grad norm: 0.0131208011880517\n",
            " - layer5.bias grad norm: 0.0054154787212610245\n",
            "Gradients at iteration 751:\n",
            " - layer1.weight grad norm: 0.5166288018226624\n",
            " - layer1.bias grad norm: 0.0006031452212482691\n",
            " - layer2.weight grad norm: 0.0018360662506893277\n",
            " - layer2.bias grad norm: 0.0005957795074209571\n",
            " - layer2_input.weight grad norm: 0.5078492760658264\n",
            " - layer2_input.bias grad norm: 0.0005957795074209571\n",
            " - layer3.weight grad norm: 0.0025126547552645206\n",
            " - layer3.bias grad norm: 0.0005839851219207048\n",
            " - layer3_input.weight grad norm: 0.49634483456611633\n",
            " - layer3_input.bias grad norm: 0.0005839851219207048\n",
            " - layer4.weight grad norm: 0.002994065871462226\n",
            " - layer4.bias grad norm: 0.0005595727707259357\n",
            " - layer4_input.weight grad norm: 0.47816136479377747\n",
            " - layer4_input.bias grad norm: 0.0005595727707259357\n",
            " - layer5.weight grad norm: 0.011804687790572643\n",
            " - layer5.bias grad norm: 0.005197871010750532\n",
            "Gradients at iteration 752:\n",
            " - layer1.weight grad norm: 0.4993150234222412\n",
            " - layer1.bias grad norm: 0.0005741237546317279\n",
            " - layer2.weight grad norm: 0.0018362178234383464\n",
            " - layer2.bias grad norm: 0.0006170339183881879\n",
            " - layer2_input.weight grad norm: 0.5264766216278076\n",
            " - layer2_input.bias grad norm: 0.0006170339183881879\n",
            " - layer3.weight grad norm: 0.002491532126441598\n",
            " - layer3.bias grad norm: 0.0005906132864765823\n",
            " - layer3_input.weight grad norm: 0.5043054223060608\n",
            " - layer3_input.bias grad norm: 0.0005906132864765823\n",
            " - layer4.weight grad norm: 0.002979160752147436\n",
            " - layer4.bias grad norm: 0.0005415445775724947\n",
            " - layer4_input.weight grad norm: 0.46796339750289917\n",
            " - layer4_input.bias grad norm: 0.0005415445775724947\n",
            " - layer5.weight grad norm: 0.012068657204508781\n",
            " - layer5.bias grad norm: 0.005163569003343582\n",
            "Gradients at iteration 753:\n",
            " - layer1.weight grad norm: 0.5053415894508362\n",
            " - layer1.bias grad norm: 0.0005843263352289796\n",
            " - layer2.weight grad norm: 0.001823222148232162\n",
            " - layer2.bias grad norm: 0.0006666724220849574\n",
            " - layer2_input.weight grad norm: 0.5655454993247986\n",
            " - layer2_input.bias grad norm: 0.0006666724220849574\n",
            " - layer3.weight grad norm: 0.002446025377139449\n",
            " - layer3.bias grad norm: 0.0005424062255769968\n",
            " - layer3_input.weight grad norm: 0.47420915961265564\n",
            " - layer3_input.bias grad norm: 0.0005424062255769968\n",
            " - layer4.weight grad norm: 0.003005951875820756\n",
            " - layer4.bias grad norm: 0.0005150793003849685\n",
            " - layer4_input.weight grad norm: 0.44689953327178955\n",
            " - layer4_input.bias grad norm: 0.0005150793003849685\n",
            " - layer5.weight grad norm: 0.01212475448846817\n",
            " - layer5.bias grad norm: 0.00519470265135169\n",
            "Gradients at iteration 754:\n",
            " - layer1.weight grad norm: 0.5123059749603271\n",
            " - layer1.bias grad norm: 0.0005934880464337766\n",
            " - layer2.weight grad norm: 0.0018695073667913675\n",
            " - layer2.bias grad norm: 0.0006106648943386972\n",
            " - layer2_input.weight grad norm: 0.5203976631164551\n",
            " - layer2_input.bias grad norm: 0.0006106648943386972\n",
            " - layer3.weight grad norm: 0.0025735863018780947\n",
            " - layer3.bias grad norm: 0.0005745670641772449\n",
            " - layer3_input.weight grad norm: 0.4937311112880707\n",
            " - layer3_input.bias grad norm: 0.0005745670641772449\n",
            " - layer4.weight grad norm: 0.0030421423725783825\n",
            " - layer4.bias grad norm: 0.0005460932152345777\n",
            " - layer4_input.weight grad norm: 0.4719942808151245\n",
            " - layer4_input.bias grad norm: 0.0005460932152345777\n",
            " - layer5.weight grad norm: 0.011392318643629551\n",
            " - layer5.bias grad norm: 0.005332321859896183\n",
            "Gradients at iteration 755:\n",
            " - layer1.weight grad norm: 0.5280036330223083\n",
            " - layer1.bias grad norm: 0.0006116981385275722\n",
            " - layer2.weight grad norm: 0.0018386698793619871\n",
            " - layer2.bias grad norm: 0.0005464966525323689\n",
            " - layer2_input.weight grad norm: 0.4786797761917114\n",
            " - layer2_input.bias grad norm: 0.0005464966525323689\n",
            " - layer3.weight grad norm: 0.0024872508365660906\n",
            " - layer3.bias grad norm: 0.0005776645266450942\n",
            " - layer3_input.weight grad norm: 0.4977928400039673\n",
            " - layer3_input.bias grad norm: 0.0005776645266450942\n",
            " - layer4.weight grad norm: 0.0030133025720715523\n",
            " - layer4.bias grad norm: 0.0005718897446058691\n",
            " - layer4_input.weight grad norm: 0.4940696060657501\n",
            " - layer4_input.bias grad norm: 0.0005718897446058691\n",
            " - layer5.weight grad norm: 0.011272378265857697\n",
            " - layer5.bias grad norm: 0.005232641473412514\n",
            "Gradients at iteration 756:\n",
            " - layer1.weight grad norm: 0.489096999168396\n",
            " - layer1.bias grad norm: 0.0005686518852598965\n",
            " - layer2.weight grad norm: 0.0018622352508828044\n",
            " - layer2.bias grad norm: 0.0006112228729762137\n",
            " - layer2_input.weight grad norm: 0.522729218006134\n",
            " - layer2_input.bias grad norm: 0.0006112228729762137\n",
            " - layer3.weight grad norm: 0.0025672500487416983\n",
            " - layer3.bias grad norm: 0.0005686935619451106\n",
            " - layer3_input.weight grad norm: 0.4896259009838104\n",
            " - layer3_input.bias grad norm: 0.0005686935619451106\n",
            " - layer4.weight grad norm: 0.003023120341822505\n",
            " - layer4.bias grad norm: 0.0005847250577062368\n",
            " - layer4_input.weight grad norm: 0.49757447838783264\n",
            " - layer4_input.bias grad norm: 0.0005847250577062368\n",
            " - layer5.weight grad norm: 0.013222341425716877\n",
            " - layer5.bias grad norm: 0.005299810785800219\n",
            "Gradients at iteration 757:\n",
            " - layer1.weight grad norm: 0.5031417012214661\n",
            " - layer1.bias grad norm: 0.0005817344645038247\n",
            " - layer2.weight grad norm: 0.0019019122701138258\n",
            " - layer2.bias grad norm: 0.0006043098401278257\n",
            " - layer2_input.weight grad norm: 0.5192149877548218\n",
            " - layer2_input.bias grad norm: 0.0006043098401278257\n",
            " - layer3.weight grad norm: 0.0025786219630390406\n",
            " - layer3.bias grad norm: 0.0005750723066739738\n",
            " - layer3_input.weight grad norm: 0.4928225874900818\n",
            " - layer3_input.bias grad norm: 0.0005750723066739738\n",
            " - layer4.weight grad norm: 0.003094278508797288\n",
            " - layer4.bias grad norm: 0.0005640143644995987\n",
            " - layer4_input.weight grad norm: 0.4839398264884949\n",
            " - layer4_input.bias grad norm: 0.0005640143644995987\n",
            " - layer5.weight grad norm: 0.011883369646966457\n",
            " - layer5.bias grad norm: 0.0053920140489935875\n",
            "Gradients at iteration 758:\n",
            " - layer1.weight grad norm: 0.5573940873146057\n",
            " - layer1.bias grad norm: 0.0006526525830850005\n",
            " - layer2.weight grad norm: 0.0018832300556823611\n",
            " - layer2.bias grad norm: 0.0005764773231931031\n",
            " - layer2_input.weight grad norm: 0.4996773302555084\n",
            " - layer2_input.bias grad norm: 0.0005764773231931031\n",
            " - layer3.weight grad norm: 0.0025789884384721518\n",
            " - layer3.bias grad norm: 0.0005492899217642844\n",
            " - layer3_input.weight grad norm: 0.4723585247993469\n",
            " - layer3_input.bias grad norm: 0.0005492899217642844\n",
            " - layer4.weight grad norm: 0.003035416826605797\n",
            " - layer4.bias grad norm: 0.0005368947749957442\n",
            " - layer4_input.weight grad norm: 0.46510013937950134\n",
            " - layer4_input.bias grad norm: 0.0005368947749957442\n",
            " - layer5.weight grad norm: 0.011975167319178581\n",
            " - layer5.bias grad norm: 0.005345984827727079\n",
            "Gradients at iteration 759:\n",
            " - layer1.weight grad norm: 0.5089030861854553\n",
            " - layer1.bias grad norm: 0.0005849697627127171\n",
            " - layer2.weight grad norm: 0.0018414048245176673\n",
            " - layer2.bias grad norm: 0.0006254275212995708\n",
            " - layer2_input.weight grad norm: 0.5314632654190063\n",
            " - layer2_input.bias grad norm: 0.0006254275212995708\n",
            " - layer3.weight grad norm: 0.002545932773500681\n",
            " - layer3.bias grad norm: 0.000551736680790782\n",
            " - layer3_input.weight grad norm: 0.4805968403816223\n",
            " - layer3_input.bias grad norm: 0.000551736680790782\n",
            " - layer4.weight grad norm: 0.0029669366776943207\n",
            " - layer4.bias grad norm: 0.0005484220455400646\n",
            " - layer4_input.weight grad norm: 0.47686630487442017\n",
            " - layer4_input.bias grad norm: 0.0005484220455400646\n",
            " - layer5.weight grad norm: 0.011863638646900654\n",
            " - layer5.bias grad norm: 0.0052910130470991135\n",
            "Gradients at iteration 760:\n",
            " - layer1.weight grad norm: 0.516562819480896\n",
            " - layer1.bias grad norm: 0.0006006890907883644\n",
            " - layer2.weight grad norm: 0.0017851409502327442\n",
            " - layer2.bias grad norm: 0.000595941673964262\n",
            " - layer2_input.weight grad norm: 0.5091537833213806\n",
            " - layer2_input.bias grad norm: 0.000595941673964262\n",
            " - layer3.weight grad norm: 0.002462591975927353\n",
            " - layer3.bias grad norm: 0.0005698752938769758\n",
            " - layer3_input.weight grad norm: 0.48790687322616577\n",
            " - layer3_input.bias grad norm: 0.0005698752938769758\n",
            " - layer4.weight grad norm: 0.0029152953065931797\n",
            " - layer4.bias grad norm: 0.0005684432690031826\n",
            " - layer4_input.weight grad norm: 0.485485702753067\n",
            " - layer4_input.bias grad norm: 0.0005684432690031826\n",
            " - layer5.weight grad norm: 0.011379184201359749\n",
            " - layer5.bias grad norm: 0.00513120973482728\n",
            "Gradients at iteration 761:\n",
            " - layer1.weight grad norm: 0.5182240605354309\n",
            " - layer1.bias grad norm: 0.0006061323801986873\n",
            " - layer2.weight grad norm: 0.0017910037422552705\n",
            " - layer2.bias grad norm: 0.000599151011556387\n",
            " - layer2_input.weight grad norm: 0.5134508609771729\n",
            " - layer2_input.bias grad norm: 0.000599151011556387\n",
            " - layer3.weight grad norm: 0.0024220349732786417\n",
            " - layer3.bias grad norm: 0.0005346499965526164\n",
            " - layer3_input.weight grad norm: 0.46100348234176636\n",
            " - layer3_input.bias grad norm: 0.0005346499965526164\n",
            " - layer4.weight grad norm: 0.002864861162379384\n",
            " - layer4.bias grad norm: 0.0005944072618149221\n",
            " - layer4_input.weight grad norm: 0.5050816535949707\n",
            " - layer4_input.bias grad norm: 0.0005944072618149221\n",
            " - layer5.weight grad norm: 0.011635258793830872\n",
            " - layer5.bias grad norm: 0.005042055156081915\n",
            "Gradients at iteration 762:\n",
            " - layer1.weight grad norm: 0.5055437684059143\n",
            " - layer1.bias grad norm: 0.0005716123851016164\n",
            " - layer2.weight grad norm: 0.0018922545714303851\n",
            " - layer2.bias grad norm: 0.0005840554949827492\n",
            " - layer2_input.weight grad norm: 0.5089752078056335\n",
            " - layer2_input.bias grad norm: 0.0005840554949827492\n",
            " - layer3.weight grad norm: 0.0025699276011437178\n",
            " - layer3.bias grad norm: 0.0005359083879739046\n",
            " - layer3_input.weight grad norm: 0.4706256091594696\n",
            " - layer3_input.bias grad norm: 0.0005359083879739046\n",
            " - layer4.weight grad norm: 0.003089591860771179\n",
            " - layer4.bias grad norm: 0.0005942229181528091\n",
            " - layer4_input.weight grad norm: 0.5135148763656616\n",
            " - layer4_input.bias grad norm: 0.0005942229181528091\n",
            " - layer5.weight grad norm: 0.011550174094736576\n",
            " - layer5.bias grad norm: 0.005333628971129656\n",
            "Gradients at iteration 763:\n",
            " - layer1.weight grad norm: 0.5410771369934082\n",
            " - layer1.bias grad norm: 0.0006361888954415917\n",
            " - layer2.weight grad norm: 0.0018422373104840517\n",
            " - layer2.bias grad norm: 0.0005377134657464921\n",
            " - layer2_input.weight grad norm: 0.46999499201774597\n",
            " - layer2_input.bias grad norm: 0.0005377134657464921\n",
            " - layer3.weight grad norm: 0.002548852004110813\n",
            " - layer3.bias grad norm: 0.000586490030400455\n",
            " - layer3_input.weight grad norm: 0.5048245191574097\n",
            " - layer3_input.bias grad norm: 0.000586490030400455\n",
            " - layer4.weight grad norm: 0.00300130364485085\n",
            " - layer4.bias grad norm: 0.0005576975527219474\n",
            " - layer4_input.weight grad norm: 0.48093271255493164\n",
            " - layer4_input.bias grad norm: 0.0005576975527219474\n",
            " - layer5.weight grad norm: 0.012125794775784016\n",
            " - layer5.bias grad norm: 0.005286848172545433\n",
            "Gradients at iteration 764:\n",
            " - layer1.weight grad norm: 0.5438410639762878\n",
            " - layer1.bias grad norm: 0.0006381460698321462\n",
            " - layer2.weight grad norm: 0.0018473999807611108\n",
            " - layer2.bias grad norm: 0.0005692704580724239\n",
            " - layer2_input.weight grad norm: 0.493755966424942\n",
            " - layer2_input.bias grad norm: 0.0005692704580724239\n",
            " - layer3.weight grad norm: 0.0025384787004441023\n",
            " - layer3.bias grad norm: 0.0005260670441202819\n",
            " - layer3_input.weight grad norm: 0.45777422189712524\n",
            " - layer3_input.bias grad norm: 0.0005260670441202819\n",
            " - layer4.weight grad norm: 0.0029996626544743776\n",
            " - layer4.bias grad norm: 0.0005806435947306454\n",
            " - layer4_input.weight grad norm: 0.5006676316261292\n",
            " - layer4_input.bias grad norm: 0.0005806435947306454\n",
            " - layer5.weight grad norm: 0.012937339022755623\n",
            " - layer5.bias grad norm: 0.005296425893902779\n",
            "Gradients at iteration 765:\n",
            " - layer1.weight grad norm: 0.5381065011024475\n",
            " - layer1.bias grad norm: 0.0006308587035164237\n",
            " - layer2.weight grad norm: 0.0017884051194414496\n",
            " - layer2.bias grad norm: 0.0005529447225853801\n",
            " - layer2_input.weight grad norm: 0.4817828834056854\n",
            " - layer2_input.bias grad norm: 0.0005529447225853801\n",
            " - layer3.weight grad norm: 0.00243476708419621\n",
            " - layer3.bias grad norm: 0.0005845896666869521\n",
            " - layer3_input.weight grad norm: 0.5016065239906311\n",
            " - layer3_input.bias grad norm: 0.0005845896666869521\n",
            " - layer4.weight grad norm: 0.0029463237151503563\n",
            " - layer4.bias grad norm: 0.0005532251670956612\n",
            " - layer4_input.weight grad norm: 0.475952684879303\n",
            " - layer4_input.bias grad norm: 0.0005532251670956612\n",
            " - layer5.weight grad norm: 0.011869086883962154\n",
            " - layer5.bias grad norm: 0.005048621911555529\n",
            "Gradients at iteration 766:\n",
            " - layer1.weight grad norm: 0.5310786962509155\n",
            " - layer1.bias grad norm: 0.0006217979243956506\n",
            " - layer2.weight grad norm: 0.001784560619853437\n",
            " - layer2.bias grad norm: 0.0006278102518990636\n",
            " - layer2_input.weight grad norm: 0.5381633639335632\n",
            " - layer2_input.bias grad norm: 0.0006278102518990636\n",
            " - layer3.weight grad norm: 0.0024190868716686964\n",
            " - layer3.bias grad norm: 0.0005557452677749097\n",
            " - layer3_input.weight grad norm: 0.47907117009162903\n",
            " - layer3_input.bias grad norm: 0.0005557452677749097\n",
            " - layer4.weight grad norm: 0.0028479977045208216\n",
            " - layer4.bias grad norm: 0.0005115634994581342\n",
            " - layer4_input.weight grad norm: 0.445707768201828\n",
            " - layer4_input.bias grad norm: 0.0005115634994581342\n",
            " - layer5.weight grad norm: 0.01126865204423666\n",
            " - layer5.bias grad norm: 0.004953437019139528\n",
            "Gradients at iteration 767:\n",
            " - layer1.weight grad norm: 0.49766790866851807\n",
            " - layer1.bias grad norm: 0.0005778920603916049\n",
            " - layer2.weight grad norm: 0.0018618777394294739\n",
            " - layer2.bias grad norm: 0.0006119452882558107\n",
            " - layer2_input.weight grad norm: 0.5219022035598755\n",
            " - layer2_input.bias grad norm: 0.0006119452882558107\n",
            " - layer3.weight grad norm: 0.002510622376576066\n",
            " - layer3.bias grad norm: 0.0005666734650731087\n",
            " - layer3_input.weight grad norm: 0.48796403408050537\n",
            " - layer3_input.bias grad norm: 0.0005666734650731087\n",
            " - layer4.weight grad norm: 0.0029490741435438395\n",
            " - layer4.bias grad norm: 0.0005755716701969504\n",
            " - layer4_input.weight grad norm: 0.49159032106399536\n",
            " - layer4_input.bias grad norm: 0.0005755716701969504\n",
            " - layer5.weight grad norm: 0.011272150091826916\n",
            " - layer5.bias grad norm: 0.005203398875892162\n",
            "Gradients at iteration 768:\n",
            " - layer1.weight grad norm: 0.5236374735832214\n",
            " - layer1.bias grad norm: 0.0006035651895217597\n",
            " - layer2.weight grad norm: 0.0018644650699570775\n",
            " - layer2.bias grad norm: 0.000579571002162993\n",
            " - layer2_input.weight grad norm: 0.5017262697219849\n",
            " - layer2_input.bias grad norm: 0.000579571002162993\n",
            " - layer3.weight grad norm: 0.0025091334246098995\n",
            " - layer3.bias grad norm: 0.0005622324533760548\n",
            " - layer3_input.weight grad norm: 0.48861318826675415\n",
            " - layer3_input.bias grad norm: 0.0005622324533760548\n",
            " - layer4.weight grad norm: 0.0029625482857227325\n",
            " - layer4.bias grad norm: 0.0005621649324893951\n",
            " - layer4_input.weight grad norm: 0.484941691160202\n",
            " - layer4_input.bias grad norm: 0.0005621649324893951\n",
            " - layer5.weight grad norm: 0.01072166208177805\n",
            " - layer5.bias grad norm: 0.005237724166363478\n",
            "Gradients at iteration 769:\n",
            " - layer1.weight grad norm: 0.5208897590637207\n",
            " - layer1.bias grad norm: 0.0005994458333589137\n",
            " - layer2.weight grad norm: 0.001864417688921094\n",
            " - layer2.bias grad norm: 0.0005163375753909349\n",
            " - layer2_input.weight grad norm: 0.4593888521194458\n",
            " - layer2_input.bias grad norm: 0.0005163375753909349\n",
            " - layer3.weight grad norm: 0.002591025084257126\n",
            " - layer3.bias grad norm: 0.0006163612124510109\n",
            " - layer3_input.weight grad norm: 0.5292148590087891\n",
            " - layer3_input.bias grad norm: 0.0006163612124510109\n",
            " - layer4.weight grad norm: 0.003120102221146226\n",
            " - layer4.bias grad norm: 0.0005615931004285812\n",
            " - layer4_input.weight grad norm: 0.48718422651290894\n",
            " - layer4_input.bias grad norm: 0.0005615931004285812\n",
            " - layer5.weight grad norm: 0.012956724502146244\n",
            " - layer5.bias grad norm: 0.005365387070924044\n",
            "Gradients at iteration 770:\n",
            " - layer1.weight grad norm: 0.5606609582901001\n",
            " - layer1.bias grad norm: 0.0006594028091058135\n",
            " - layer2.weight grad norm: 0.0018620069604367018\n",
            " - layer2.bias grad norm: 0.0006011045770719647\n",
            " - layer2_input.weight grad norm: 0.5125681757926941\n",
            " - layer2_input.bias grad norm: 0.0006011045770719647\n",
            " - layer3.weight grad norm: 0.002541920868679881\n",
            " - layer3.bias grad norm: 0.0005676807486452162\n",
            " - layer3_input.weight grad norm: 0.4854747951030731\n",
            " - layer3_input.bias grad norm: 0.0005676807486452162\n",
            " - layer4.weight grad norm: 0.0030994543340057135\n",
            " - layer4.bias grad norm: 0.0004957239725627005\n",
            " - layer4_input.weight grad norm: 0.4325094223022461\n",
            " - layer4_input.bias grad norm: 0.0004957239725627005\n",
            " - layer5.weight grad norm: 0.011510038748383522\n",
            " - layer5.bias grad norm: 0.005332275293767452\n",
            "Gradients at iteration 771:\n",
            " - layer1.weight grad norm: 0.5609543323516846\n",
            " - layer1.bias grad norm: 0.0006584438378922641\n",
            " - layer2.weight grad norm: 0.0017876632045954466\n",
            " - layer2.bias grad norm: 0.0005639137816615403\n",
            " - layer2_input.weight grad norm: 0.48669376969337463\n",
            " - layer2_input.bias grad norm: 0.0005639137816615403\n",
            " - layer3.weight grad norm: 0.0024622944183647633\n",
            " - layer3.bias grad norm: 0.0005297683528624475\n",
            " - layer3_input.weight grad norm: 0.46171271800994873\n",
            " - layer3_input.bias grad norm: 0.0005297683528624475\n",
            " - layer4.weight grad norm: 0.0029043564572930336\n",
            " - layer4.bias grad norm: 0.0005650563980452716\n",
            " - layer4_input.weight grad norm: 0.48487696051597595\n",
            " - layer4_input.bias grad norm: 0.0005650563980452716\n",
            " - layer5.weight grad norm: 0.011370906606316566\n",
            " - layer5.bias grad norm: 0.005090266931802034\n",
            "Gradients at iteration 772:\n",
            " - layer1.weight grad norm: 0.5142630338668823\n",
            " - layer1.bias grad norm: 0.0005851289024576545\n",
            " - layer2.weight grad norm: 0.0017699019517749548\n",
            " - layer2.bias grad norm: 0.0005197614664211869\n",
            " - layer2_input.weight grad norm: 0.4651416838169098\n",
            " - layer2_input.bias grad norm: 0.0005197614664211869\n",
            " - layer3.weight grad norm: 0.0024301877710968256\n",
            " - layer3.bias grad norm: 0.0006066026398912072\n",
            " - layer3_input.weight grad norm: 0.5268917083740234\n",
            " - layer3_input.bias grad norm: 0.0006066026398912072\n",
            " - layer4.weight grad norm: 0.0028919288888573647\n",
            " - layer4.bias grad norm: 0.0005619984585791826\n",
            " - layer4_input.weight grad norm: 0.49130645394325256\n",
            " - layer4_input.bias grad norm: 0.0005619984585791826\n",
            " - layer5.weight grad norm: 0.011606274172663689\n",
            " - layer5.bias grad norm: 0.005036216229200363\n",
            "Gradients at iteration 773:\n",
            " - layer1.weight grad norm: 0.527259886264801\n",
            " - layer1.bias grad norm: 0.0006081220344640315\n",
            " - layer2.weight grad norm: 0.001869371859356761\n",
            " - layer2.bias grad norm: 0.0005979783018119633\n",
            " - layer2_input.weight grad norm: 0.5163370370864868\n",
            " - layer2_input.bias grad norm: 0.0005979783018119633\n",
            " - layer3.weight grad norm: 0.002481065457686782\n",
            " - layer3.bias grad norm: 0.0005988282500766218\n",
            " - layer3_input.weight grad norm: 0.5166468024253845\n",
            " - layer3_input.bias grad norm: 0.0005988282500766218\n",
            " - layer4.weight grad norm: 0.002973213791847229\n",
            " - layer4.bias grad norm: 0.0004934305907227099\n",
            " - layer4_input.weight grad norm: 0.4338993430137634\n",
            " - layer4_input.bias grad norm: 0.0004934305907227099\n",
            " - layer5.weight grad norm: 0.012345004826784134\n",
            " - layer5.bias grad norm: 0.0052185687236487865\n",
            "Gradients at iteration 774:\n",
            " - layer1.weight grad norm: 0.5368129014968872\n",
            " - layer1.bias grad norm: 0.0006213916931301355\n",
            " - layer2.weight grad norm: 0.001826717983931303\n",
            " - layer2.bias grad norm: 0.0005745295202359557\n",
            " - layer2_input.weight grad norm: 0.49772486090660095\n",
            " - layer2_input.bias grad norm: 0.0005745295202359557\n",
            " - layer3.weight grad norm: 0.002515575382858515\n",
            " - layer3.bias grad norm: 0.0005964688025414944\n",
            " - layer3_input.weight grad norm: 0.5111350417137146\n",
            " - layer3_input.bias grad norm: 0.0005964688025414944\n",
            " - layer4.weight grad norm: 0.003007729770615697\n",
            " - layer4.bias grad norm: 0.0005142419831827283\n",
            " - layer4_input.weight grad norm: 0.4501662254333496\n",
            " - layer4_input.bias grad norm: 0.0005142419831827283\n",
            " - layer5.weight grad norm: 0.012051176279783249\n",
            " - layer5.bias grad norm: 0.005205757915973663\n",
            "Gradients at iteration 775:\n",
            " - layer1.weight grad norm: 0.5200254917144775\n",
            " - layer1.bias grad norm: 0.0006017843261361122\n",
            " - layer2.weight grad norm: 0.0018212361028417945\n",
            " - layer2.bias grad norm: 0.0006191570428200066\n",
            " - layer2_input.weight grad norm: 0.5341234803199768\n",
            " - layer2_input.bias grad norm: 0.0006191570428200066\n",
            " - layer3.weight grad norm: 0.0024506424088031054\n",
            " - layer3.bias grad norm: 0.0005434028571471572\n",
            " - layer3_input.weight grad norm: 0.47412624955177307\n",
            " - layer3_input.bias grad norm: 0.0005434028571471572\n",
            " - layer4.weight grad norm: 0.0029319829773157835\n",
            " - layer4.bias grad norm: 0.0005340306088328362\n",
            " - layer4_input.weight grad norm: 0.46829918026924133\n",
            " - layer4_input.bias grad norm: 0.0005340306088328362\n",
            " - layer5.weight grad norm: 0.011799250729382038\n",
            " - layer5.bias grad norm: 0.0051153372041881084\n",
            "Gradients at iteration 776:\n",
            " - layer1.weight grad norm: 0.5283004641532898\n",
            " - layer1.bias grad norm: 0.0006190367275848985\n",
            " - layer2.weight grad norm: 0.0017804722301661968\n",
            " - layer2.bias grad norm: 0.0006011289660818875\n",
            " - layer2_input.weight grad norm: 0.5120460391044617\n",
            " - layer2_input.bias grad norm: 0.0006011289660818875\n",
            " - layer3.weight grad norm: 0.0024388162419199944\n",
            " - layer3.bias grad norm: 0.0005844187107868493\n",
            " - layer3_input.weight grad norm: 0.4993257522583008\n",
            " - layer3_input.bias grad norm: 0.0005844187107868493\n",
            " - layer4.weight grad norm: 0.002896155696362257\n",
            " - layer4.bias grad norm: 0.0005280536715872586\n",
            " - layer4_input.weight grad norm: 0.45737284421920776\n",
            " - layer4_input.bias grad norm: 0.0005280536715872586\n",
            " - layer5.weight grad norm: 0.012089836411178112\n",
            " - layer5.bias grad norm: 0.005030258093029261\n",
            "Gradients at iteration 777:\n",
            " - layer1.weight grad norm: 0.5474901795387268\n",
            " - layer1.bias grad norm: 0.0006354587967507541\n",
            " - layer2.weight grad norm: 0.0018169042887166142\n",
            " - layer2.bias grad norm: 0.0005888637970201671\n",
            " - layer2_input.weight grad norm: 0.5104295015335083\n",
            " - layer2_input.bias grad norm: 0.0005888637970201671\n",
            " - layer3.weight grad norm: 0.0025107881519943476\n",
            " - layer3.bias grad norm: 0.0005443983245640993\n",
            " - layer3_input.weight grad norm: 0.48097702860832214\n",
            " - layer3_input.bias grad norm: 0.0005443983245640993\n",
            " - layer4.weight grad norm: 0.0029763190541416407\n",
            " - layer4.bias grad norm: 0.0005211788229644299\n",
            " - layer4_input.weight grad norm: 0.45625069737434387\n",
            " - layer4_input.bias grad norm: 0.0005211788229644299\n",
            " - layer5.weight grad norm: 0.012806138955056667\n",
            " - layer5.bias grad norm: 0.005291236564517021\n",
            "Gradients at iteration 778:\n",
            " - layer1.weight grad norm: 0.5116629004478455\n",
            " - layer1.bias grad norm: 0.0005848360015079379\n",
            " - layer2.weight grad norm: 0.0019254878861829638\n",
            " - layer2.bias grad norm: 0.0006510196253657341\n",
            " - layer2_input.weight grad norm: 0.5557834506034851\n",
            " - layer2_input.bias grad norm: 0.0006510196253657341\n",
            " - layer3.weight grad norm: 0.002633563708513975\n",
            " - layer3.bias grad norm: 0.0005464330315589905\n",
            " - layer3_input.weight grad norm: 0.47913363575935364\n",
            " - layer3_input.bias grad norm: 0.0005464330315589905\n",
            " - layer4.weight grad norm: 0.0031377021223306656\n",
            " - layer4.bias grad norm: 0.0005087873432785273\n",
            " - layer4_input.weight grad norm: 0.4466787874698639\n",
            " - layer4_input.bias grad norm: 0.0005087873432785273\n",
            " - layer5.weight grad norm: 0.012756024487316608\n",
            " - layer5.bias grad norm: 0.005423722323030233\n",
            "Gradients at iteration 779:\n",
            " - layer1.weight grad norm: 0.5235193967819214\n",
            " - layer1.bias grad norm: 0.0006122363265603781\n",
            " - layer2.weight grad norm: 0.0017883479595184326\n",
            " - layer2.bias grad norm: 0.0006220665527507663\n",
            " - layer2_input.weight grad norm: 0.5287133455276489\n",
            " - layer2_input.bias grad norm: 0.0006220665527507663\n",
            " - layer3.weight grad norm: 0.002426391001790762\n",
            " - layer3.bias grad norm: 0.0005742096109315753\n",
            " - layer3_input.weight grad norm: 0.4939310550689697\n",
            " - layer3_input.bias grad norm: 0.0005742096109315753\n",
            " - layer4.weight grad norm: 0.0028317321557551622\n",
            " - layer4.bias grad norm: 0.0005239074234850705\n",
            " - layer4_input.weight grad norm: 0.44973334670066833\n",
            " - layer4_input.bias grad norm: 0.0005239074234850705\n",
            " - layer5.weight grad norm: 0.010798431932926178\n",
            " - layer5.bias grad norm: 0.005052290391176939\n",
            "Gradients at iteration 780:\n",
            " - layer1.weight grad norm: 0.527840256690979\n",
            " - layer1.bias grad norm: 0.0006084095221012831\n",
            " - layer2.weight grad norm: 0.0018732765456661582\n",
            " - layer2.bias grad norm: 0.0005343799712136388\n",
            " - layer2_input.weight grad norm: 0.46954482793807983\n",
            " - layer2_input.bias grad norm: 0.0005343799712136388\n",
            " - layer3.weight grad norm: 0.0025494240690022707\n",
            " - layer3.bias grad norm: 0.0005852598114870489\n",
            " - layer3_input.weight grad norm: 0.5077732801437378\n",
            " - layer3_input.bias grad norm: 0.0005852598114870489\n",
            " - layer4.weight grad norm: 0.003049478866159916\n",
            " - layer4.bias grad norm: 0.0005743877845816314\n",
            " - layer4_input.weight grad norm: 0.4928230941295624\n",
            " - layer4_input.bias grad norm: 0.0005743877845816314\n",
            " - layer5.weight grad norm: 0.012411978095769882\n",
            " - layer5.bias grad norm: 0.005334804300218821\n",
            "Gradients at iteration 781:\n",
            " - layer1.weight grad norm: 0.5268502831459045\n",
            " - layer1.bias grad norm: 0.0006174013251438737\n",
            " - layer2.weight grad norm: 0.0017534109065309167\n",
            " - layer2.bias grad norm: 0.0005899707321077585\n",
            " - layer2_input.weight grad norm: 0.5070378184318542\n",
            " - layer2_input.bias grad norm: 0.0005899707321077585\n",
            " - layer3.weight grad norm: 0.002400493249297142\n",
            " - layer3.bias grad norm: 0.0005590327200479805\n",
            " - layer3_input.weight grad norm: 0.48260951042175293\n",
            " - layer3_input.bias grad norm: 0.0005590327200479805\n",
            " - layer4.weight grad norm: 0.0028014290146529675\n",
            " - layer4.bias grad norm: 0.0005604253965429962\n",
            " - layer4_input.weight grad norm: 0.48192474246025085\n",
            " - layer4_input.bias grad norm: 0.0005604253965429962\n",
            " - layer5.weight grad norm: 0.011593375355005264\n",
            " - layer5.bias grad norm: 0.004969885107129812\n",
            "Gradients at iteration 782:\n",
            " - layer1.weight grad norm: 0.4777461886405945\n",
            " - layer1.bias grad norm: 0.0005483044660650194\n",
            " - layer2.weight grad norm: 0.0018512862734496593\n",
            " - layer2.bias grad norm: 0.0006469612126238644\n",
            " - layer2_input.weight grad norm: 0.5465481877326965\n",
            " - layer2_input.bias grad norm: 0.0006469612126238644\n",
            " - layer3.weight grad norm: 0.0025454608257859945\n",
            " - layer3.bias grad norm: 0.0005989633500576019\n",
            " - layer3_input.weight grad norm: 0.5087880492210388\n",
            " - layer3_input.bias grad norm: 0.0005989633500576019\n",
            " - layer4.weight grad norm: 0.0030394336208701134\n",
            " - layer4.bias grad norm: 0.0005443175905384123\n",
            " - layer4_input.weight grad norm: 0.4625783860683441\n",
            " - layer4_input.bias grad norm: 0.0005443175905384123\n",
            " - layer5.weight grad norm: 0.012253398075699806\n",
            " - layer5.bias grad norm: 0.005265987943857908\n",
            "Gradients at iteration 783:\n",
            " - layer1.weight grad norm: 0.48299670219421387\n",
            " - layer1.bias grad norm: 0.0005474773934110999\n",
            " - layer2.weight grad norm: 0.001954751554876566\n",
            " - layer2.bias grad norm: 0.0005988655029796064\n",
            " - layer2_input.weight grad norm: 0.5176371932029724\n",
            " - layer2_input.bias grad norm: 0.0005988655029796064\n",
            " - layer3.weight grad norm: 0.0026754559949040413\n",
            " - layer3.bias grad norm: 0.0005716453888453543\n",
            " - layer3_input.weight grad norm: 0.4976564645767212\n",
            " - layer3_input.bias grad norm: 0.0005716453888453543\n",
            " - layer4.weight grad norm: 0.0031426609493792057\n",
            " - layer4.bias grad norm: 0.0005741656641475856\n",
            " - layer4_input.weight grad norm: 0.5009074807167053\n",
            " - layer4_input.bias grad norm: 0.0005741656641475856\n",
            " - layer5.weight grad norm: 0.01192655973136425\n",
            " - layer5.bias grad norm: 0.005504183005541563\n",
            "Gradients at iteration 784:\n",
            " - layer1.weight grad norm: 0.5442057847976685\n",
            " - layer1.bias grad norm: 0.0006303596310317516\n",
            " - layer2.weight grad norm: 0.0018398351967334747\n",
            " - layer2.bias grad norm: 0.0005132938968017697\n",
            " - layer2_input.weight grad norm: 0.4541851580142975\n",
            " - layer2_input.bias grad norm: 0.0005132938968017697\n",
            " - layer3.weight grad norm: 0.0024810233153402805\n",
            " - layer3.bias grad norm: 0.0006168514373712242\n",
            " - layer3_input.weight grad norm: 0.5269798040390015\n",
            " - layer3_input.bias grad norm: 0.0006168514373712242\n",
            " - layer4.weight grad norm: 0.0029577575623989105\n",
            " - layer4.bias grad norm: 0.0005395460757426918\n",
            " - layer4_input.weight grad norm: 0.46866893768310547\n",
            " - layer4_input.bias grad norm: 0.0005395460757426918\n",
            " - layer5.weight grad norm: 0.012239333242177963\n",
            " - layer5.bias grad norm: 0.005210597533732653\n",
            "Gradients at iteration 785:\n",
            " - layer1.weight grad norm: 0.5051907896995544\n",
            " - layer1.bias grad norm: 0.0005788327543996274\n",
            " - layer2.weight grad norm: 0.0018467804184183478\n",
            " - layer2.bias grad norm: 0.0005645828205160797\n",
            " - layer2_input.weight grad norm: 0.4919709265232086\n",
            " - layer2_input.bias grad norm: 0.0005645828205160797\n",
            " - layer3.weight grad norm: 0.0025792578235268593\n",
            " - layer3.bias grad norm: 0.0005803600652143359\n",
            " - layer3_input.weight grad norm: 0.5015976428985596\n",
            " - layer3_input.bias grad norm: 0.0005803600652143359\n",
            " - layer4.weight grad norm: 0.0030749652069061995\n",
            " - layer4.bias grad norm: 0.0005835524061694741\n",
            " - layer4_input.weight grad norm: 0.5009250640869141\n",
            " - layer4_input.bias grad norm: 0.0005835524061694741\n",
            " - layer5.weight grad norm: 0.013080929405987263\n",
            " - layer5.bias grad norm: 0.005283211823552847\n",
            "Gradients at iteration 786:\n",
            " - layer1.weight grad norm: 0.5379879474639893\n",
            " - layer1.bias grad norm: 0.0006294493214227259\n",
            " - layer2.weight grad norm: 0.0019419582094997168\n",
            " - layer2.bias grad norm: 0.0005768126575276256\n",
            " - layer2_input.weight grad norm: 0.49761298298835754\n",
            " - layer2_input.bias grad norm: 0.0005768126575276256\n",
            " - layer3.weight grad norm: 0.0026896586641669273\n",
            " - layer3.bias grad norm: 0.0005289295222610235\n",
            " - layer3_input.weight grad norm: 0.4611641466617584\n",
            " - layer3_input.bias grad norm: 0.0005289295222610235\n",
            " - layer4.weight grad norm: 0.003190753050148487\n",
            " - layer4.bias grad norm: 0.0005888534360565245\n",
            " - layer4_input.weight grad norm: 0.5000555515289307\n",
            " - layer4_input.bias grad norm: 0.0005888534360565245\n",
            " - layer5.weight grad norm: 0.012934458442032337\n",
            " - layer5.bias grad norm: 0.005606696009635925\n",
            "Gradients at iteration 787:\n",
            " - layer1.weight grad norm: 0.5056598782539368\n",
            " - layer1.bias grad norm: 0.0005861520767211914\n",
            " - layer2.weight grad norm: 0.0017971796914935112\n",
            " - layer2.bias grad norm: 0.0006294185877777636\n",
            " - layer2_input.weight grad norm: 0.5363525748252869\n",
            " - layer2_input.bias grad norm: 0.0006294185877777636\n",
            " - layer3.weight grad norm: 0.0024547777138650417\n",
            " - layer3.bias grad norm: 0.0005558135453611612\n",
            " - layer3_input.weight grad norm: 0.4809589684009552\n",
            " - layer3_input.bias grad norm: 0.0005558135453611612\n",
            " - layer4.weight grad norm: 0.0029419907368719578\n",
            " - layer4.bias grad norm: 0.0005545631283894181\n",
            " - layer4_input.weight grad norm: 0.4745088219642639\n",
            " - layer4_input.bias grad norm: 0.0005545631283894181\n",
            " - layer5.weight grad norm: 0.01037078257650137\n",
            " - layer5.bias grad norm: 0.005092806648463011\n",
            "Gradients at iteration 788:\n",
            " - layer1.weight grad norm: 0.49758103489875793\n",
            " - layer1.bias grad norm: 0.0005730202537961304\n",
            " - layer2.weight grad norm: 0.0018823109567165375\n",
            " - layer2.bias grad norm: 0.0006136861047707498\n",
            " - layer2_input.weight grad norm: 0.5207669138908386\n",
            " - layer2_input.bias grad norm: 0.0006136861047707498\n",
            " - layer3.weight grad norm: 0.0025439211167395115\n",
            " - layer3.bias grad norm: 0.0005865926505066454\n",
            " - layer3_input.weight grad norm: 0.5021224021911621\n",
            " - layer3_input.bias grad norm: 0.0005865926505066454\n",
            " - layer4.weight grad norm: 0.0030211745761334896\n",
            " - layer4.bias grad norm: 0.0005561503930948675\n",
            " - layer4_input.weight grad norm: 0.47843098640441895\n",
            " - layer4_input.bias grad norm: 0.0005561503930948675\n",
            " - layer5.weight grad norm: 0.01196585688740015\n",
            " - layer5.bias grad norm: 0.005220408551394939\n",
            "Gradients at iteration 789:\n",
            " - layer1.weight grad norm: 0.5106659531593323\n",
            " - layer1.bias grad norm: 0.0005859291413798928\n",
            " - layer2.weight grad norm: 0.0018921929877251387\n",
            " - layer2.bias grad norm: 0.0005850440938957036\n",
            " - layer2_input.weight grad norm: 0.5081771016120911\n",
            " - layer2_input.bias grad norm: 0.0005850440938957036\n",
            " - layer3.weight grad norm: 0.0026150003541260958\n",
            " - layer3.bias grad norm: 0.0005441432003863156\n",
            " - layer3_input.weight grad norm: 0.47753727436065674\n",
            " - layer3_input.bias grad norm: 0.0005441432003863156\n",
            " - layer4.weight grad norm: 0.0030501391738653183\n",
            " - layer4.bias grad norm: 0.0005921559641137719\n",
            " - layer4_input.weight grad norm: 0.5027090907096863\n",
            " - layer4_input.bias grad norm: 0.0005921559641137719\n",
            " - layer5.weight grad norm: 0.012923125177621841\n",
            " - layer5.bias grad norm: 0.005367234814912081\n",
            "Gradients at iteration 790:\n",
            " - layer1.weight grad norm: 0.5544666051864624\n",
            " - layer1.bias grad norm: 0.0006344844587147236\n",
            " - layer2.weight grad norm: 0.0018944242037832737\n",
            " - layer2.bias grad norm: 0.0005363435484468937\n",
            " - layer2_input.weight grad norm: 0.47645366191864014\n",
            " - layer2_input.bias grad norm: 0.0005363435484468937\n",
            " - layer3.weight grad norm: 0.0025924844667315483\n",
            " - layer3.bias grad norm: 0.0005595069960691035\n",
            " - layer3_input.weight grad norm: 0.48901867866516113\n",
            " - layer3_input.bias grad norm: 0.0005595069960691035\n",
            " - layer4.weight grad norm: 0.003023390891030431\n",
            " - layer4.bias grad norm: 0.0005449223681353033\n",
            " - layer4_input.weight grad norm: 0.47563421726226807\n",
            " - layer4_input.bias grad norm: 0.0005449223681353033\n",
            " - layer5.weight grad norm: 0.011885463260114193\n",
            " - layer5.bias grad norm: 0.005341331474483013\n",
            "Gradients at iteration 791:\n",
            " - layer1.weight grad norm: 0.5084017515182495\n",
            " - layer1.bias grad norm: 0.0005923648714087903\n",
            " - layer2.weight grad norm: 0.00181175849866122\n",
            " - layer2.bias grad norm: 0.0006348296301439404\n",
            " - layer2_input.weight grad norm: 0.5399895310401917\n",
            " - layer2_input.bias grad norm: 0.0006348296301439404\n",
            " - layer3.weight grad norm: 0.0024934131652116776\n",
            " - layer3.bias grad norm: 0.0005142355221323669\n",
            " - layer3_input.weight grad norm: 0.45453596115112305\n",
            " - layer3_input.bias grad norm: 0.0005142355221323669\n",
            " - layer4.weight grad norm: 0.002932823495939374\n",
            " - layer4.bias grad norm: 0.0005775622557848692\n",
            " - layer4_input.weight grad norm: 0.49311554431915283\n",
            " - layer4_input.bias grad norm: 0.0005775622557848692\n",
            " - layer5.weight grad norm: 0.011211229488253593\n",
            " - layer5.bias grad norm: 0.005212647840380669\n",
            "Gradients at iteration 792:\n",
            " - layer1.weight grad norm: 0.4959830939769745\n",
            " - layer1.bias grad norm: 0.0005660871975123882\n",
            " - layer2.weight grad norm: 0.0018828498432412744\n",
            " - layer2.bias grad norm: 0.0006019746069796383\n",
            " - layer2_input.weight grad norm: 0.5177443623542786\n",
            " - layer2_input.bias grad norm: 0.0006019746069796383\n",
            " - layer3.weight grad norm: 0.002606227993965149\n",
            " - layer3.bias grad norm: 0.0005801263614557683\n",
            " - layer3_input.weight grad norm: 0.5012810826301575\n",
            " - layer3_input.bias grad norm: 0.0005801263614557683\n",
            " - layer4.weight grad norm: 0.0030052666552364826\n",
            " - layer4.bias grad norm: 0.0005608973442576826\n",
            " - layer4_input.weight grad norm: 0.48420700430870056\n",
            " - layer4_input.bias grad norm: 0.0005608973442576826\n",
            " - layer5.weight grad norm: 0.012318090535700321\n",
            " - layer5.bias grad norm: 0.005368788260966539\n",
            "Gradients at iteration 793:\n",
            " - layer1.weight grad norm: 0.5360024571418762\n",
            " - layer1.bias grad norm: 0.0006250488222576678\n",
            " - layer2.weight grad norm: 0.0017604447202757\n",
            " - layer2.bias grad norm: 0.0005990713834762573\n",
            " - layer2_input.weight grad norm: 0.5131092667579651\n",
            " - layer2_input.bias grad norm: 0.0005990713834762573\n",
            " - layer3.weight grad norm: 0.0024578275624662638\n",
            " - layer3.bias grad norm: 0.0005606867489404976\n",
            " - layer3_input.weight grad norm: 0.4833729863166809\n",
            " - layer3_input.bias grad norm: 0.0005606867489404976\n",
            " - layer4.weight grad norm: 0.002903916873037815\n",
            " - layer4.bias grad norm: 0.0005356681067496538\n",
            " - layer4_input.weight grad norm: 0.4643439054489136\n",
            " - layer4_input.bias grad norm: 0.0005356681067496538\n",
            " - layer5.weight grad norm: 0.010490147396922112\n",
            " - layer5.bias grad norm: 0.005058739334344864\n",
            "Gradients at iteration 794:\n",
            " - layer1.weight grad norm: 0.5359684824943542\n",
            " - layer1.bias grad norm: 0.0006270594312809408\n",
            " - layer2.weight grad norm: 0.0018704335670918226\n",
            " - layer2.bias grad norm: 0.0005328911356627941\n",
            " - layer2_input.weight grad norm: 0.46595391631126404\n",
            " - layer2_input.bias grad norm: 0.0005328911356627941\n",
            " - layer3.weight grad norm: 0.0025796042755246162\n",
            " - layer3.bias grad norm: 0.000591615738812834\n",
            " - layer3_input.weight grad norm: 0.5067167282104492\n",
            " - layer3_input.bias grad norm: 0.000591615738812834\n",
            " - layer4.weight grad norm: 0.003040311625227332\n",
            " - layer4.bias grad norm: 0.0005756131140515208\n",
            " - layer4_input.weight grad norm: 0.48855409026145935\n",
            " - layer4_input.bias grad norm: 0.0005756131140515208\n",
            " - layer5.weight grad norm: 0.01130896620452404\n",
            " - layer5.bias grad norm: 0.0053094825707376\n",
            "Gradients at iteration 795:\n",
            " - layer1.weight grad norm: 0.5066207051277161\n",
            " - layer1.bias grad norm: 0.0005926715675741434\n",
            " - layer2.weight grad norm: 0.001824332750402391\n",
            " - layer2.bias grad norm: 0.0005748776020482183\n",
            " - layer2_input.weight grad norm: 0.49150988459587097\n",
            " - layer2_input.bias grad norm: 0.0005748776020482183\n",
            " - layer3.weight grad norm: 0.002455326495692134\n",
            " - layer3.bias grad norm: 0.0005872423062101007\n",
            " - layer3_input.weight grad norm: 0.4983171820640564\n",
            " - layer3_input.bias grad norm: 0.0005872423062101007\n",
            " - layer4.weight grad norm: 0.002891473937779665\n",
            " - layer4.bias grad norm: 0.0005962190916761756\n",
            " - layer4_input.weight grad norm: 0.5032583475112915\n",
            " - layer4_input.bias grad norm: 0.0005962190916761756\n",
            " - layer5.weight grad norm: 0.010877370834350586\n",
            " - layer5.bias grad norm: 0.005113223567605019\n",
            "Gradients at iteration 796:\n",
            " - layer1.weight grad norm: 0.529047429561615\n",
            " - layer1.bias grad norm: 0.0006183261284604669\n",
            " - layer2.weight grad norm: 0.0017984433798119426\n",
            " - layer2.bias grad norm: 0.000606881978455931\n",
            " - layer2_input.weight grad norm: 0.5185832381248474\n",
            " - layer2_input.bias grad norm: 0.000606881978455931\n",
            " - layer3.weight grad norm: 0.0024742416571825743\n",
            " - layer3.bias grad norm: 0.0005231042741797864\n",
            " - layer3_input.weight grad norm: 0.4527662992477417\n",
            " - layer3_input.bias grad norm: 0.0005231042741797864\n",
            " - layer4.weight grad norm: 0.0029168648179620504\n",
            " - layer4.bias grad norm: 0.0005804203683510423\n",
            " - layer4_input.weight grad norm: 0.49598509073257446\n",
            " - layer4_input.bias grad norm: 0.0005804203683510423\n",
            " - layer5.weight grad norm: 0.01162097230553627\n",
            " - layer5.bias grad norm: 0.005160551983863115\n",
            "Gradients at iteration 797:\n",
            " - layer1.weight grad norm: 0.5261245965957642\n",
            " - layer1.bias grad norm: 0.000604353379458189\n",
            " - layer2.weight grad norm: 0.0018102563917636871\n",
            " - layer2.bias grad norm: 0.0005916064255870879\n",
            " - layer2_input.weight grad norm: 0.5105769634246826\n",
            " - layer2_input.bias grad norm: 0.0005916064255870879\n",
            " - layer3.weight grad norm: 0.0024896999821066856\n",
            " - layer3.bias grad norm: 0.0005364127573557198\n",
            " - layer3_input.weight grad norm: 0.47178733348846436\n",
            " - layer3_input.bias grad norm: 0.0005364127573557198\n",
            " - layer4.weight grad norm: 0.002913004020228982\n",
            " - layer4.bias grad norm: 0.0005640439921990037\n",
            " - layer4_input.weight grad norm: 0.4896147847175598\n",
            " - layer4_input.bias grad norm: 0.0005640439921990037\n",
            " - layer5.weight grad norm: 0.012283455580472946\n",
            " - layer5.bias grad norm: 0.0052125160582363605\n",
            "Gradients at iteration 798:\n",
            " - layer1.weight grad norm: 0.523521363735199\n",
            " - layer1.bias grad norm: 0.0005983979790471494\n",
            " - layer2.weight grad norm: 0.0019097933545708656\n",
            " - layer2.bias grad norm: 0.0005732146673835814\n",
            " - layer2_input.weight grad norm: 0.4998639225959778\n",
            " - layer2_input.bias grad norm: 0.0005732146673835814\n",
            " - layer3.weight grad norm: 0.002641430590301752\n",
            " - layer3.bias grad norm: 0.0005768893170170486\n",
            " - layer3_input.weight grad norm: 0.4996873140335083\n",
            " - layer3_input.bias grad norm: 0.0005768893170170486\n",
            " - layer4.weight grad norm: 0.003070549573749304\n",
            " - layer4.bias grad norm: 0.0005414109909906983\n",
            " - layer4_input.weight grad norm: 0.4755862057209015\n",
            " - layer4_input.bias grad norm: 0.0005414109909906983\n",
            " - layer5.weight grad norm: 0.011829263530671597\n",
            " - layer5.bias grad norm: 0.005438027437776327\n",
            "Gradients at iteration 799:\n",
            " - layer1.weight grad norm: 0.5209925174713135\n",
            " - layer1.bias grad norm: 0.0006044817273505032\n",
            " - layer2.weight grad norm: 0.0017579622799530625\n",
            " - layer2.bias grad norm: 0.0006016433471813798\n",
            " - layer2_input.weight grad norm: 0.5174639225006104\n",
            " - layer2_input.bias grad norm: 0.0006016433471813798\n",
            " - layer3.weight grad norm: 0.0024044036399573088\n",
            " - layer3.bias grad norm: 0.0005502544809132814\n",
            " - layer3_input.weight grad norm: 0.47756144404411316\n",
            " - layer3_input.bias grad norm: 0.0005502544809132814\n",
            " - layer4.weight grad norm: 0.002841236302629113\n",
            " - layer4.bias grad norm: 0.0005596221890300512\n",
            " - layer4_input.weight grad norm: 0.48223111033439636\n",
            " - layer4_input.bias grad norm: 0.0005596221890300512\n",
            " - layer5.weight grad norm: 0.011913802474737167\n",
            " - layer5.bias grad norm: 0.004997801501303911\n",
            "Gradients at iteration 800:\n",
            " - layer1.weight grad norm: 0.5417494773864746\n",
            " - layer1.bias grad norm: 0.0006325702997855842\n",
            " - layer2.weight grad norm: 0.0018796826479956508\n",
            " - layer2.bias grad norm: 0.0005693029961548746\n",
            " - layer2_input.weight grad norm: 0.49392104148864746\n",
            " - layer2_input.bias grad norm: 0.0005693029961548746\n",
            " - layer3.weight grad norm: 0.002573128556832671\n",
            " - layer3.bias grad norm: 0.0005459575331769884\n",
            " - layer3_input.weight grad norm: 0.4728311598300934\n",
            " - layer3_input.bias grad norm: 0.0005459575331769884\n",
            " - layer4.weight grad norm: 0.003088522469624877\n",
            " - layer4.bias grad norm: 0.000567565206438303\n",
            " - layer4_input.weight grad norm: 0.48864439129829407\n",
            " - layer4_input.bias grad norm: 0.000567565206438303\n",
            " - layer5.weight grad norm: 0.012483913451433182\n",
            " - layer5.bias grad norm: 0.005382894072681665\n",
            "It: 800, Loss: 5.382e+13, Y0: 0.698, Time: 1.62, Learning Rate: 1.000e-03\n",
            "Gradients at iteration 801:\n",
            " - layer1.weight grad norm: 0.5013936161994934\n",
            " - layer1.bias grad norm: 0.0005795473116450012\n",
            " - layer2.weight grad norm: 0.0018277426715940237\n",
            " - layer2.bias grad norm: 0.0005782069638371468\n",
            " - layer2_input.weight grad norm: 0.49714988470077515\n",
            " - layer2_input.bias grad norm: 0.0005782069638371468\n",
            " - layer3.weight grad norm: 0.0024754442274570465\n",
            " - layer3.bias grad norm: 0.0005869824672117829\n",
            " - layer3_input.weight grad norm: 0.4983504116535187\n",
            " - layer3_input.bias grad norm: 0.0005869824672117829\n",
            " - layer4.weight grad norm: 0.0029540169052779675\n",
            " - layer4.bias grad norm: 0.000591079646255821\n",
            " - layer4_input.weight grad norm: 0.5028943419456482\n",
            " - layer4_input.bias grad norm: 0.000591079646255821\n",
            " - layer5.weight grad norm: 0.011968963779509068\n",
            " - layer5.bias grad norm: 0.0051785740070044994\n",
            "Gradients at iteration 802:\n",
            " - layer1.weight grad norm: 0.5155563950538635\n",
            " - layer1.bias grad norm: 0.0005967667675577104\n",
            " - layer2.weight grad norm: 0.0017244231421500444\n",
            " - layer2.bias grad norm: 0.0006241025403141975\n",
            " - layer2_input.weight grad norm: 0.5333689451217651\n",
            " - layer2_input.bias grad norm: 0.0006241025403141975\n",
            " - layer3.weight grad norm: 0.0023652943782508373\n",
            " - layer3.bias grad norm: 0.0005329970153979957\n",
            " - layer3_input.weight grad norm: 0.45991086959838867\n",
            " - layer3_input.bias grad norm: 0.0005329970153979957\n",
            " - layer4.weight grad norm: 0.0028141357470303774\n",
            " - layer4.bias grad norm: 0.0005708507960662246\n",
            " - layer4_input.weight grad norm: 0.4879003167152405\n",
            " - layer4_input.bias grad norm: 0.0005708507960662246\n",
            " - layer5.weight grad norm: 0.010551827028393745\n",
            " - layer5.bias grad norm: 0.004949295427650213\n",
            "Gradients at iteration 803:\n",
            " - layer1.weight grad norm: 0.5990704894065857\n",
            " - layer1.bias grad norm: 0.0007155160419642925\n",
            " - layer2.weight grad norm: 0.0018260817741975188\n",
            " - layer2.bias grad norm: 0.0005800758372060955\n",
            " - layer2_input.weight grad norm: 0.4999232292175293\n",
            " - layer2_input.bias grad norm: 0.0005800758372060955\n",
            " - layer3.weight grad norm: 0.0024369722232222557\n",
            " - layer3.bias grad norm: 0.0005369866266846657\n",
            " - layer3_input.weight grad norm: 0.4630533456802368\n",
            " - layer3_input.bias grad norm: 0.0005369866266846657\n",
            " - layer4.weight grad norm: 0.0029060414526611567\n",
            " - layer4.bias grad norm: 0.00048217459698207676\n",
            " - layer4_input.weight grad norm: 0.4202253222465515\n",
            " - layer4_input.bias grad norm: 0.00048217459698207676\n",
            " - layer5.weight grad norm: 0.011727720499038696\n",
            " - layer5.bias grad norm: 0.005112651269882917\n",
            "Gradients at iteration 804:\n",
            " - layer1.weight grad norm: 0.5384256839752197\n",
            " - layer1.bias grad norm: 0.0006252028397284448\n",
            " - layer2.weight grad norm: 0.0019638086669147015\n",
            " - layer2.bias grad norm: 0.0005470877513289452\n",
            " - layer2_input.weight grad norm: 0.4794211685657501\n",
            " - layer2_input.bias grad norm: 0.0005470877513289452\n",
            " - layer3.weight grad norm: 0.0026708729565143585\n",
            " - layer3.bias grad norm: 0.0005911347107030451\n",
            " - layer3_input.weight grad norm: 0.5103512406349182\n",
            " - layer3_input.bias grad norm: 0.0005911347107030451\n",
            " - layer4.weight grad norm: 0.0031508994288742542\n",
            " - layer4.bias grad norm: 0.0005384378018788993\n",
            " - layer4_input.weight grad norm: 0.46860599517822266\n",
            " - layer4_input.bias grad norm: 0.0005384378018788993\n",
            " - layer5.weight grad norm: 0.012223097495734692\n",
            " - layer5.bias grad norm: 0.0055199358612298965\n",
            "Gradients at iteration 805:\n",
            " - layer1.weight grad norm: 0.4896645247936249\n",
            " - layer1.bias grad norm: 0.0005537438555620611\n",
            " - layer2.weight grad norm: 0.001861607888713479\n",
            " - layer2.bias grad norm: 0.0006124299252405763\n",
            " - layer2_input.weight grad norm: 0.5292971134185791\n",
            " - layer2_input.bias grad norm: 0.0006124299252405763\n",
            " - layer3.weight grad norm: 0.0025887435767799616\n",
            " - layer3.bias grad norm: 0.0005974586238153279\n",
            " - layer3_input.weight grad norm: 0.5177625417709351\n",
            " - layer3_input.bias grad norm: 0.0005974586238153279\n",
            " - layer4.weight grad norm: 0.003064775140956044\n",
            " - layer4.bias grad norm: 0.0005243143532425165\n",
            " - layer4_input.weight grad norm: 0.4602223038673401\n",
            " - layer4_input.bias grad norm: 0.0005243143532425165\n",
            " - layer5.weight grad norm: 0.011830084025859833\n",
            " - layer5.bias grad norm: 0.005375515203922987\n",
            "Gradients at iteration 806:\n",
            " - layer1.weight grad norm: 0.5510236620903015\n",
            " - layer1.bias grad norm: 0.0006406591855920851\n",
            " - layer2.weight grad norm: 0.0018227651016786695\n",
            " - layer2.bias grad norm: 0.0005664670024998486\n",
            " - layer2_input.weight grad norm: 0.4913848042488098\n",
            " - layer2_input.bias grad norm: 0.0005664670024998486\n",
            " - layer3.weight grad norm: 0.002422611229121685\n",
            " - layer3.bias grad norm: 0.000586728856433183\n",
            " - layer3_input.weight grad norm: 0.5081671476364136\n",
            " - layer3_input.bias grad norm: 0.000586728856433183\n",
            " - layer4.weight grad norm: 0.0028732437640428543\n",
            " - layer4.bias grad norm: 0.0005089280311949551\n",
            " - layer4_input.weight grad norm: 0.44328445196151733\n",
            " - layer4_input.bias grad norm: 0.0005089280311949551\n",
            " - layer5.weight grad norm: 0.011545444838702679\n",
            " - layer5.bias grad norm: 0.005105739459395409\n",
            "Gradients at iteration 807:\n",
            " - layer1.weight grad norm: 0.49152204394340515\n",
            " - layer1.bias grad norm: 0.0005632081301882863\n",
            " - layer2.weight grad norm: 0.0019460242474451661\n",
            " - layer2.bias grad norm: 0.0006118936580605805\n",
            " - layer2_input.weight grad norm: 0.5275405049324036\n",
            " - layer2_input.bias grad norm: 0.0006118936580605805\n",
            " - layer3.weight grad norm: 0.0026035825721919537\n",
            " - layer3.bias grad norm: 0.0005592173547483981\n",
            " - layer3_input.weight grad norm: 0.4870612919330597\n",
            " - layer3_input.bias grad norm: 0.0005592173547483981\n",
            " - layer4.weight grad norm: 0.0031505562365055084\n",
            " - layer4.bias grad norm: 0.0005689021782018244\n",
            " - layer4_input.weight grad norm: 0.4926108121871948\n",
            " - layer4_input.bias grad norm: 0.0005689021782018244\n",
            " - layer5.weight grad norm: 0.012653778307139874\n",
            " - layer5.bias grad norm: 0.005481897387653589\n",
            "Gradients at iteration 808:\n",
            " - layer1.weight grad norm: 0.5055557489395142\n",
            " - layer1.bias grad norm: 0.0005888197338208556\n",
            " - layer2.weight grad norm: 0.0017949127359315753\n",
            " - layer2.bias grad norm: 0.0006314960774034262\n",
            " - layer2_input.weight grad norm: 0.534233808517456\n",
            " - layer2_input.bias grad norm: 0.0006314960774034262\n",
            " - layer3.weight grad norm: 0.0024467608891427517\n",
            " - layer3.bias grad norm: 0.0006040877196937799\n",
            " - layer3_input.weight grad norm: 0.5166463255882263\n",
            " - layer3_input.bias grad norm: 0.0006040877196937799\n",
            " - layer4.weight grad norm: 0.0029442610684782267\n",
            " - layer4.bias grad norm: 0.000503989402204752\n",
            " - layer4_input.weight grad norm: 0.4380643367767334\n",
            " - layer4_input.bias grad norm: 0.000503989402204752\n",
            " - layer5.weight grad norm: 0.011707312427461147\n",
            " - layer5.bias grad norm: 0.005162074230611324\n",
            "Gradients at iteration 809:\n",
            " - layer1.weight grad norm: 0.5317860841751099\n",
            " - layer1.bias grad norm: 0.0006130324909463525\n",
            " - layer2.weight grad norm: 0.0018402053974568844\n",
            " - layer2.bias grad norm: 0.0005686960066668689\n",
            " - layer2_input.weight grad norm: 0.495551735162735\n",
            " - layer2_input.bias grad norm: 0.0005686960066668689\n",
            " - layer3.weight grad norm: 0.0025829665828496218\n",
            " - layer3.bias grad norm: 0.0005859499797224998\n",
            " - layer3_input.weight grad norm: 0.503732442855835\n",
            " - layer3_input.bias grad norm: 0.0005859499797224998\n",
            " - layer4.weight grad norm: 0.0030161396134644747\n",
            " - layer4.bias grad norm: 0.000538223481271416\n",
            " - layer4_input.weight grad norm: 0.4665653109550476\n",
            " - layer4_input.bias grad norm: 0.000538223481271416\n",
            " - layer5.weight grad norm: 0.012350439094007015\n",
            " - layer5.bias grad norm: 0.005335592199116945\n",
            "Gradients at iteration 810:\n",
            " - layer1.weight grad norm: 0.5491788983345032\n",
            " - layer1.bias grad norm: 0.0006294251070357859\n",
            " - layer2.weight grad norm: 0.0019243512069806457\n",
            " - layer2.bias grad norm: 0.0005303487414494157\n",
            " - layer2_input.weight grad norm: 0.47193804383277893\n",
            " - layer2_input.bias grad norm: 0.0005303487414494157\n",
            " - layer3.weight grad norm: 0.0025792052038013935\n",
            " - layer3.bias grad norm: 0.000530063989572227\n",
            " - layer3_input.weight grad norm: 0.4717485010623932\n",
            " - layer3_input.bias grad norm: 0.000530063989572227\n",
            " - layer4.weight grad norm: 0.0030312840826809406\n",
            " - layer4.bias grad norm: 0.0005731068667955697\n",
            " - layer4_input.weight grad norm: 0.5029295682907104\n",
            " - layer4_input.bias grad norm: 0.0005731068667955697\n",
            " - layer5.weight grad norm: 0.011867804452776909\n",
            " - layer5.bias grad norm: 0.005447089672088623\n",
            "Gradients at iteration 811:\n",
            " - layer1.weight grad norm: 0.5242597460746765\n",
            " - layer1.bias grad norm: 0.000613254145719111\n",
            " - layer2.weight grad norm: 0.0018362603150308132\n",
            " - layer2.bias grad norm: 0.0006026828195899725\n",
            " - layer2_input.weight grad norm: 0.5163974761962891\n",
            " - layer2_input.bias grad norm: 0.0006026828195899725\n",
            " - layer3.weight grad norm: 0.002462021540850401\n",
            " - layer3.bias grad norm: 0.0005680914619006217\n",
            " - layer3_input.weight grad norm: 0.4869908094406128\n",
            " - layer3_input.bias grad norm: 0.0005680914619006217\n",
            " - layer4.weight grad norm: 0.0029815214220434427\n",
            " - layer4.bias grad norm: 0.000545959104783833\n",
            " - layer4_input.weight grad norm: 0.4702703356742859\n",
            " - layer4_input.bias grad norm: 0.000545959104783833\n",
            " - layer5.weight grad norm: 0.01112726703286171\n",
            " - layer5.bias grad norm: 0.005186211317777634\n",
            "Gradients at iteration 812:\n",
            " - layer1.weight grad norm: 0.5308364033699036\n",
            " - layer1.bias grad norm: 0.0006126104854047298\n",
            " - layer2.weight grad norm: 0.0018121293978765607\n",
            " - layer2.bias grad norm: 0.0005917644593864679\n",
            " - layer2_input.weight grad norm: 0.5108708739280701\n",
            " - layer2_input.bias grad norm: 0.0005917644593864679\n",
            " - layer3.weight grad norm: 0.002508691279217601\n",
            " - layer3.bias grad norm: 0.000572542252484709\n",
            " - layer3_input.weight grad norm: 0.49307841062545776\n",
            " - layer3_input.bias grad norm: 0.000572542252484709\n",
            " - layer4.weight grad norm: 0.0029294677078723907\n",
            " - layer4.bias grad norm: 0.0005292939022183418\n",
            " - layer4_input.weight grad norm: 0.4625099301338196\n",
            " - layer4_input.bias grad norm: 0.0005292939022183418\n",
            " - layer5.weight grad norm: 0.011558979749679565\n",
            " - layer5.bias grad norm: 0.005272093694657087\n",
            "Gradients at iteration 813:\n",
            " - layer1.weight grad norm: 0.5341177582740784\n",
            " - layer1.bias grad norm: 0.0006248871795833111\n",
            " - layer2.weight grad norm: 0.0017993663204833865\n",
            " - layer2.bias grad norm: 0.0005577091942541301\n",
            " - layer2_input.weight grad norm: 0.48466017842292786\n",
            " - layer2_input.bias grad norm: 0.0005577091942541301\n",
            " - layer3.weight grad norm: 0.0024288531858474016\n",
            " - layer3.bias grad norm: 0.0005798365455120802\n",
            " - layer3_input.weight grad norm: 0.5011987090110779\n",
            " - layer3_input.bias grad norm: 0.0005798365455120802\n",
            " - layer4.weight grad norm: 0.0029255286790430546\n",
            " - layer4.bias grad norm: 0.0005553166847676039\n",
            " - layer4_input.weight grad norm: 0.4779495298862457\n",
            " - layer4_input.bias grad norm: 0.0005553166847676039\n",
            " - layer5.weight grad norm: 0.011840016581118107\n",
            " - layer5.bias grad norm: 0.005151577293872833\n",
            "Gradients at iteration 814:\n",
            " - layer1.weight grad norm: 0.5542150139808655\n",
            " - layer1.bias grad norm: 0.0006494607659988105\n",
            " - layer2.weight grad norm: 0.001766747678630054\n",
            " - layer2.bias grad norm: 0.0005636909627355635\n",
            " - layer2_input.weight grad norm: 0.4882047474384308\n",
            " - layer2_input.bias grad norm: 0.0005636909627355635\n",
            " - layer3.weight grad norm: 0.0024817518424242735\n",
            " - layer3.bias grad norm: 0.0005585586768575013\n",
            " - layer3_input.weight grad norm: 0.47908100485801697\n",
            " - layer3_input.bias grad norm: 0.0005585586768575013\n",
            " - layer4.weight grad norm: 0.002902776002883911\n",
            " - layer4.bias grad norm: 0.0005479055689647794\n",
            " - layer4_input.weight grad norm: 0.47414660453796387\n",
            " - layer4_input.bias grad norm: 0.0005479055689647794\n",
            " - layer5.weight grad norm: 0.011052167974412441\n",
            " - layer5.bias grad norm: 0.005106721539050341\n",
            "Gradients at iteration 815:\n",
            " - layer1.weight grad norm: 0.5388551354408264\n",
            " - layer1.bias grad norm: 0.0006251283339224756\n",
            " - layer2.weight grad norm: 0.0017876401543617249\n",
            " - layer2.bias grad norm: 0.0005760108469985425\n",
            " - layer2_input.weight grad norm: 0.5014493465423584\n",
            " - layer2_input.bias grad norm: 0.0005760108469985425\n",
            " - layer3.weight grad norm: 0.002437540329992771\n",
            " - layer3.bias grad norm: 0.0005493666394613683\n",
            " - layer3_input.weight grad norm: 0.47959664463996887\n",
            " - layer3_input.bias grad norm: 0.0005493666394613683\n",
            " - layer4.weight grad norm: 0.002837396925315261\n",
            " - layer4.bias grad norm: 0.0005544097512029111\n",
            " - layer4_input.weight grad norm: 0.47746986150741577\n",
            " - layer4_input.bias grad norm: 0.0005544097512029111\n",
            " - layer5.weight grad norm: 0.012168478220701218\n",
            " - layer5.bias grad norm: 0.0050780922174453735\n",
            "Gradients at iteration 816:\n",
            " - layer1.weight grad norm: 0.5497484803199768\n",
            " - layer1.bias grad norm: 0.0006458696443587542\n",
            " - layer2.weight grad norm: 0.0017581540159881115\n",
            " - layer2.bias grad norm: 0.000602040730882436\n",
            " - layer2_input.weight grad norm: 0.5102645754814148\n",
            " - layer2_input.bias grad norm: 0.000602040730882436\n",
            " - layer3.weight grad norm: 0.0023865860421210527\n",
            " - layer3.bias grad norm: 0.0005720675108022988\n",
            " - layer3_input.weight grad norm: 0.4876684546470642\n",
            " - layer3_input.bias grad norm: 0.0005720675108022988\n",
            " - layer4.weight grad norm: 0.002797606633976102\n",
            " - layer4.bias grad norm: 0.000518473272677511\n",
            " - layer4_input.weight grad norm: 0.44658127427101135\n",
            " - layer4_input.bias grad norm: 0.000518473272677511\n",
            " - layer5.weight grad norm: 0.010359026491641998\n",
            " - layer5.bias grad norm: 0.0049982499331235886\n",
            "Gradients at iteration 817:\n",
            " - layer1.weight grad norm: 0.523586630821228\n",
            " - layer1.bias grad norm: 0.0006108308443799615\n",
            " - layer2.weight grad norm: 0.0017985589802265167\n",
            " - layer2.bias grad norm: 0.0006137529853731394\n",
            " - layer2_input.weight grad norm: 0.5221731662750244\n",
            " - layer2_input.bias grad norm: 0.0006137529853731394\n",
            " - layer3.weight grad norm: 0.0024544536136090755\n",
            " - layer3.bias grad norm: 0.0005708914832212031\n",
            " - layer3_input.weight grad norm: 0.4873494505882263\n",
            " - layer3_input.bias grad norm: 0.0005708914832212031\n",
            " - layer4.weight grad norm: 0.0029337825253605843\n",
            " - layer4.bias grad norm: 0.0005378678906708956\n",
            " - layer4_input.weight grad norm: 0.46420061588287354\n",
            " - layer4_input.bias grad norm: 0.0005378678906708956\n",
            " - layer5.weight grad norm: 0.012387286871671677\n",
            " - layer5.bias grad norm: 0.00517225032672286\n",
            "Gradients at iteration 818:\n",
            " - layer1.weight grad norm: 0.5162816047668457\n",
            " - layer1.bias grad norm: 0.0005919550894759595\n",
            " - layer2.weight grad norm: 0.0018350286409258842\n",
            " - layer2.bias grad norm: 0.0006055789417587221\n",
            " - layer2_input.weight grad norm: 0.5194670557975769\n",
            " - layer2_input.bias grad norm: 0.0006055789417587221\n",
            " - layer3.weight grad norm: 0.002595891011878848\n",
            " - layer3.bias grad norm: 0.0005886733415536582\n",
            " - layer3_input.weight grad norm: 0.5095407366752625\n",
            " - layer3_input.bias grad norm: 0.0005886733415536582\n",
            " - layer4.weight grad norm: 0.0030099672731012106\n",
            " - layer4.bias grad norm: 0.0005141365109011531\n",
            " - layer4_input.weight grad norm: 0.45143213868141174\n",
            " - layer4_input.bias grad norm: 0.0005141365109011531\n",
            " - layer5.weight grad norm: 0.011575892567634583\n",
            " - layer5.bias grad norm: 0.005394694861024618\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tot \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 3\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal time:\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/vol/bitbucket/aan120/Final-Year-Project/Pytorch/FBSNNs.py:263\u001b[0m, in \u001b[0;36mFBSNN.train\u001b[0;34m(self, N_Iter, learning_rate)\u001b[0m\n\u001b[1;32m    260\u001b[0m t_batch, W_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_minibatch()  \u001b[38;5;66;03m# M x (N+1) x 1, M x (N+1) x D\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Compute the loss for the current batch\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m loss, X_pred, Y_pred, Y0_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Perform backpropagation\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients again to ensure correct gradient accumulation\u001b[39;00m\n",
            "File \u001b[0;32m/vol/bitbucket/aan120/Final-Year-Project/Pytorch/FBSNNs.py:149\u001b[0m, in \u001b[0;36mFBSNN.loss_function\u001b[0;34m(self, t, W, Xi)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Initial state for all trajectories\u001b[39;00m\n\u001b[1;32m    148\u001b[0m X0 \u001b[38;5;241m=\u001b[39m Xi\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD)  \u001b[38;5;66;03m# M x D\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m Y0, Z0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_u\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX0\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Obtain the network output and its gradient at the initial state\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Store the initial state and the network output\u001b[39;00m\n\u001b[1;32m    152\u001b[0m X_list\u001b[38;5;241m.\u001b[39mappend(X0)\n",
            "File \u001b[0;32m/vol/bitbucket/aan120/Final-Year-Project/Pytorch/FBSNNs.py:108\u001b[0m, in \u001b[0;36mFBSNN.net_u\u001b[0;34m(self, t, X)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((t, X), \u001b[38;5;241m1\u001b[39m)  \n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Pass the concatenated input through the neural network model\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# The output u is a tensor of dimensions M x 1, representing the value function at each input (t, X)\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# M x 1\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Compute the gradient of the output u with respect to the state variables X\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# The gradient is calculated for each input in the batch, resulting in a tensor of dimensions M x D\u001b[39;00m\n\u001b[1;32m    112\u001b[0m Du \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(outputs\u001b[38;5;241m=\u001b[39m[u], inputs\u001b[38;5;241m=\u001b[39m[X], grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u), \n\u001b[1;32m    113\u001b[0m                         allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m/vol/bitbucket/aan120/myvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m/vol/bitbucket/aan120/Final-Year-Project/Pytorch/Models.py:71\u001b[0m, in \u001b[0;36mResnet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m shortcut \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstable:\n\u001b[0;32m---> 71\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstable_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4_input(u)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/vol/bitbucket/aan120/Final-Year-Project/Pytorch/Models.py:43\u001b[0m, in \u001b[0;36mResnet.stable_forward\u001b[0;34m(self, layer, out)\u001b[0m\n\u001b[1;32m     40\u001b[0m     RtR \u001b[38;5;241m=\u001b[39m delta \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m RtR \u001b[38;5;241m/\u001b[39m (norm \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     41\u001b[0m A \u001b[38;5;241m=\u001b[39m RtR \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(RtR\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "tot = time.time()\n",
        "print(model.device)\n",
        "graph = model.train(n_iter, lr)\n",
        "print(\"total time:\", time.time() - tot, \"s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "t_test, W_test = model.fetch_minibatch()\n",
        "X_pred, Y_pred = model.predict(Xi, t_test, W_test)\n",
        "samples = 5\n",
        "\n",
        "if type(t_test).__module__ != 'numpy':\n",
        "    t_test = t_test.cpu().numpy()\n",
        "if type(X_pred).__module__ != 'numpy':\n",
        "    X_pred = X_pred.cpu().detach().numpy()\n",
        "if type(Y_pred).__module__ != 'numpy':\n",
        "    Y_pred = Y_pred.cpu().detach().numpy()\n",
        "\n",
        "Y_test = np.reshape(u_exact(T,np.reshape(t_test[0:M, :, :], [-1, 1]), np.reshape(X_pred[0:M, :, :], [-1, D])),\n",
        "                    [M, -1, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.plot(graph[0], graph[1])\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Value')\n",
        "plt.yscale(\"log\")\n",
        "plt.title('Evolution of the training loss')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(t_test[0:1, :, 0].T, Y_pred[0:1, :, 0].T, 'b', label='Learned $u(t,X_t)$')\n",
        "plt.plot(t_test[0:1, :, 0].T, Y_test[0:1, :, 0].T, 'r--', label='Exact $u(t,X_t)$')\n",
        "plt.plot(t_test[0:1, -1, 0], Y_test[0:1, -1, 0], 'ko', label='$Y_T = u(T,X_T)$')\n",
        "\n",
        "plt.plot(t_test[1:samples, :, 0].T, Y_pred[1:samples, :, 0].T, 'b')\n",
        "plt.plot(t_test[1:samples, :, 0].T, Y_test[1:samples, :, 0].T, 'r--')\n",
        "plt.plot(t_test[1:samples, -1, 0], Y_test[1:samples, -1, 0], 'ko')\n",
        "\n",
        "plt.plot([0], Y_test[0, 0, 0], 'ks', label='$Y_0 = u(0,X_0)$')\n",
        "\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$Y_t = u(t,X_t)$')\n",
        "plt.title(str(D) + '-dimensional Black-Scholes-Barenblatt, ' + model.mode + \"-\" + model.activation)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "errors = np.sqrt((Y_test - Y_pred) ** 2 / Y_test ** 2)\n",
        "mean_errors = np.mean(errors, 0)\n",
        "std_errors = np.std(errors, 0)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(t_test[0, :, 0], mean_errors, 'b', label='mean')\n",
        "plt.plot(t_test[0, :, 0], mean_errors + 2 * std_errors, 'r--', label='mean + two standard deviations')\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('relative error')\n",
        "plt.title(str(D) + '-dimensional Black-Scholes-Barenblatt, ' + model.mode + \"-\" + model.activation)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_model(\"models/BlackScholesBarenblattMLMC100D.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "db8df854c49da63d7a9a70fc04e89db6090f48201a05c37ee636be674f92602b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
